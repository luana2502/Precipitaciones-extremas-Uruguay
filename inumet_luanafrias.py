# -*- coding: utf-8 -*-
"""INUMET_LuanaFrias.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yVrIdcf1ZEH9lyANsqIZCaeQaq8B6jIT

# <center><b>ANÁLISIS EXPLORATORIO DE DATOS DE PRECIPITACIONES EN LAS ESTACIONES DE INUMET: PATRONES Y COMPORTAMIENTOS DE LAS LLUVIAS EN URUGUAY (1991-2021) </b></center>
## <center><b>Universidad Tecnológica de Uruguay - UTEC - ITR CS</b></center>
## <center> Luana Frias, Nelcy Atehortua</center>
### <center>2023</center>

# <center><b>PREPROCESAMIENTO DE DATOS</b></center>

Identificar datos nulos, seleccionar variables de interés, explorar relaciones entre variables y entre muestras.
"""

# Se importan funciones auxiliares desde GitHub requeridas para el desarrollo de
# este práctico
!git clone https://github.com/jsblandon/weather_data_uy_preprocessing.git
import sys
sys.path.append('/content/weather_data_uy_preprocessing')

# Librería Basemap
!pip install "basemap == 1.3.0b1" "basemap-data == 1.3.0b1" "basemap-data-hires == 1.3.0b1"

pip install statsmodels

pip install pymannkendall

# Commented out IPython magic to ensure Python compatibility.
# Se importan las librerías necesarias
import numpy as np # Generación y manejo de datos
import pandas as pd # Manipulación y análisis de datos
import matplotlib.pyplot as plt # Librería para realizar gráficos
import seaborn as sns # Crear gráficos estadísticos
from google.colab import files, drive # Importar datos de Google
from matplotlib.axes._axes import _log as matplotlib_axes_logger # Generación de mapas
from datetime import datetime
import matplotlib.patches as mpatches # Generador de parches para gráficos

from weather_data_preprocessing import null_report # Reporte de datos NAN
import matplotlib.dates as mdates # Manipulación de datos
from matplotlib.dates import DateFormatter # Manipulación en datos de formato fecha
from math import sqrt # Funciones matemáticas

import statsmodels.api as sm # Descomposición de serie
from statsmodels.tsa.stattools import adfuller # Análisis de estacionariedad
import pymannkendall as mk # Análisis de tendencia
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.arima.model import ARIMA

# Instalar skforecast y shap
!pip install skforecast shap

import skforecast
#from skforecast.ForecasterAutoreg import ForecasterAutoreg
#from skforecast.ForecasterAutoregCustom import ForecasterAutoregCustom
#from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect
from skforecast.model_selection import grid_search_forecaster
from skforecast.model_selection import backtesting_forecaster
from skforecast.utils import save_forecaster
from skforecast.utils import load_forecaster
import shap

from sklearn.experimental import enable_iterative_imputer # Proporciona acceso a versiones eficaces de muchos algoritmos comunes.
from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split # Test de entrenamiento
from sklearn.linear_model import LinearRegression # Algoritmo de regresión lineal
from sklearn.metrics import mean_squared_error # Error cuadrático medio
from sklearn.metrics import r2_score, mean_absolute_error # Errores

# %matplotlib inline
# %config InlineBackend.figure_formats = ['svg']

import warnings
print('Skforecast version: ', skforecast.__version__)
sns.set_style('whitegrid')
warnings.filterwarnings("ignore")

!pip install --upgrade matplotlib

#!pip install SciencePlots

#%%capture
# Para que las gráficas tengan la fuente parecida a la de LaTeX
#!sudo apt-get install dvipng texlive-latex-extra texlive-fonts-recommended cm-super

#import scienceplots
#import matplotlib as mpl  # Asegúrate de importar matplotlib
#from warnings import simplefilter  # Para ignorar advertencias
# Plot settings
#%matplotlib inline
#%config InlineBackend.figure_formats = ['svg']

#mpl.rcParams['figure.dpi'] = 600
#sns.set_style('whitegrid')
#simplefilter("ignore")
# plt.style.use(['science','ieee'])
#plt.style.use(['science','notebook'])

# Conectar con Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define una función para analizar las fechas en el formato adecuado
date_parser = lambda x: pd.to_datetime(x, format='%d/%m/%Y')
#Ruta Nelcy
#ruta_preci_inumet = "/content/drive/MyDrive/Investigacion/Tutoria_Tesis_TrabajosGrado/Analisis_Precipitaciones_Uruguay/DataSet/Inumet/IP079_23_Data_Promedios_codificado.csv"

ruta_preci_inumet = "/content/drive/MyDrive/datasCo/inumet/IP079_23_Data_Promedios_codificado.csv"

# Carga los datos usando el date_parser que definiste
data_preci_inumet = pd.read_csv(ruta_preci_inumet,
                                parse_dates=['Fecha'],
                                index_col=['Fecha'],
                                delimiter=';',
                                date_parser=date_parser)

#Recorte de datos
data_preci_inumet=data_preci_inumet.loc['1991-01-01':'2021-12-31']
data_preci_inumet

"""## **Gestión de Datos Faltantes**"""

#CONTEOS DE S/V (0 sin verificar) EN CADA ESTACIÓN METEOROLOGÍCA
# diccionario para almacenar los recuentos de "s/d"
sv = {}

# Total de filas en el DataFrame
total_rows = len(data_preci_inumet)

# Iterar a través de cada columna y contar "s/v"
for column in data_preci_inumet.columns:
    counts = data_preci_inumet[column].apply(lambda x: x.count("s/v")).sum()
    percentage = (counts / total_rows) * 100
    sv[column] = (counts, percentage)

# Imprimir los recuentos y porcentajes
print("Número y porcentaje de 's/v' en cada columna:")
for column, (count, percentage) in sv.items():
    print(f"{column}: {count} ({percentage:.2f}%)")

# Convertir el diccionario en un DataFrame para facilitar la graficación
sv_df = pd.DataFrame.from_dict(sv, orient='index', columns=['Count', 'Percentage'])

# Ordenar el DataFrame por recuento en orden descendente
sv_df = sv_df.sort_values(by='Count', ascending=False)

# Crear un gráfico de barras
plt.figure(figsize=(10, 6))
plt.bar(sv_df.index, sv_df['Count'])
plt.xlabel('Estación Meteorológica',fontsize=17)
plt.ylabel('Número de ceros sin verificar',fontsize=17)
plt.title('Recuento de ceros sin verificar en cada Estación Meteorológica',fontsize=17)
plt.xticks(rotation=90,fontsize=15)
plt.yticks(fontsize=15) # Rotar etiquetas del eje x para mayor legibilidad
plt.tight_layout()
plt.savefig("ceros.svg",bbox_inches="tight")
plt.show()

#CONTEO DE S/D (Sin datos) EN CADA ESTACIÓN METEOROLOGICA
# diccionario para almacenar los recuentos de "s/d"
sd = {}

# Total de filas en el DataFrame
total_rows = len(data_preci_inumet)

# Iterar a través de cada columna y contar "s/v"
for column in data_preci_inumet.columns:
    counts = data_preci_inumet[column].apply(lambda x: x.count("s/d")).sum()
    percentage = (counts / total_rows) * 100
    sd[column] = (counts, percentage)

# Imprimir los recuentos y porcentajes
print("Número y porcentaje de 's/d' en cada columna:")
for column, (count, percentage) in sd.items():
    print(f"{column}: {count} ({percentage:.2f}%)")

# Convertir el diccionario en un DataFrame para facilitar la graficación
sd_df = pd.DataFrame.from_dict(sd, orient='index', columns=['Count', 'Percentage'])

# Ordenar el DataFrame por recuento en orden descendente
sd_df = sd_df.sort_values(by='Count', ascending=False)

# Crear un gráfico de barras
plt.figure(figsize=(10, 6))
plt.bar(sd_df.index, sd_df['Count'])
plt.xlabel('Estación Meteorológica',fontsize=17)
plt.ylabel('Número de sin datos',fontsize=17)
plt.title('Recuento de sin datos en cada Estación Meteorológica',fontsize=17)
plt.xticks(rotation=90,fontsize=15)  # Rotar etiquetas del eje x para mayor legibilidad
plt.yticks(fontsize=15)
plt.tight_layout()
plt.savefig("sd.svg",bbox_inches="tight")
plt.show()

#ELIMINAR ESTACIONES QUE SUPEREN EL 10% DE DATOS FALTANTES, EN ESTE CASO MINAS Y SAN JOSÉ
#TAMBIEN SE ELIMINA TACUAREMBÓ

# Lista de nombres de las columnas a eliminar
columnas_a_eliminar = ['Tacuarembo_mm', 'SanJose_mm', 'Minas_mm']

# Eliminar las columnas
data_preci_inumet = data_preci_inumet.drop(columns=columnas_a_eliminar)

data_preci_inumet

"""## **Imputación de Datos Faltantes**"""

# Estadísticos datos Precipitación INIA
data_preci_inumet.describe()

"""### **Imputación univariada**"""

# Reemplazar "s/d" y "s/v" con NaN para que Pandas los reconozca como valores faltantes
data_preci_inumet.replace(["s/d", "s/v"], np.nan, inplace=True)

# Se crea el objeto de imputación simple utilizando la media como estadístico
imputador = SimpleImputer(# Bandera que representa los datos faltantes
                          missing_values = np.nan,
                          # Estrategia para rellenar los datos faltantes. Pruebe
                          # con un valor constante y con los estadísticos
                          # mencionados previamente
                          strategy = 'mean')

# Se rellenan los datos faltantes para los datos de Precipitación
data_imputado = pd.DataFrame(imputador.fit_transform(data_preci_inumet),
                              columns=['Artigas_mm',
                                       'Carrasco_mm',
                                       'Melo_mm',
                                       'Colonia_mm',
                                       'Durazno_mm',
                                       'Trinidad_mm',
                                       'Florida_mm',
                                       'PuntaEste_mm',
                                       'Prado_mm',
                                       'Melilla_mm',
                                       'Paysandu_mm',
                                       'Rivera_mm',
                                       'Rocha_mm',
                                       'Salto_mm',
                                       'Mercedes_mm',
                                       'TreintaTres_mm',
                                       'Young_mm'
                                       ])

#Verificar la imputación
# Contar los valores NaN en cada columna
nan_counts = data_imputado.isnull().sum()

print(nan_counts)

# Restablecer el índice del DataFrame a las fechas originales
data_imputado.index = data_preci_inumet.index

data_imputado

"""## **Analisis exploratorio de datos**

### **Resampleo de datos - Precipitación acumulada**
"""

# Resampleo para todos los años
# Datos acumulados por meses, años y semestral para todas las estaciones de INIA
#Por meses
data_meses= data_imputado.resample('MS').sum()
#data_meses
#Por años
data_anual= data_imputado.resample('YS').sum()
#data_años

#Por semestre
data_semestre= data_imputado.resample('6MS').sum()
#data_semestre

"""Este código agrupa los datos de los meses de diciembre, enero y febrero como "Verano", marzo, abril y mayo como "Otoño", junio, julio y agosto como "Invierno" y septiembre, octubre y noviembre como "Primavera"."""

data_estacional=data_imputado.resample('QS-DEC').sum()
data_estacional

#RESAMPLEO VERANO,INVIERNO,PRIMAVERA,OTOÑO
data_verano = data_estacional.loc[data_estacional.index.quarter == 4]
data_primavera=data_estacional.loc[data_estacional.index.quarter == 3]
data_invierno=data_estacional.loc[data_estacional.index.quarter == 2]
data_oto= data_estacional.loc[data_estacional.index.quarter == 1]

data_verano_describe = data_verano.describe()
nombre_archivo_csv = "descripcion_estadistica_verano.csv"
data_verano_describe.to_csv(nombre_archivo_csv)
data_verano_describe

#RESAMPLEO VERANO,INVIERNO,PRIMAVERA,OTOÑO POR REGIONES

#NORTE
Primavera_Norte = data_primavera[['Artigas_mm', 'Rivera_mm', 'Salto_mm']]
Verano_Norte = data_verano[['Artigas_mm', 'Rivera_mm', 'Salto_mm']]
Invierno_Norte = data_invierno[['Artigas_mm', 'Rivera_mm', 'Salto_mm']]
Oto_Norte = data_oto[['Artigas_mm', 'Rivera_mm', 'Salto_mm']]

#SUR
Primavera_Sur = data_primavera[['Carrasco_mm', 'Colonia_mm','Florida_mm','PuntaEste_mm','Prado_mm','Melilla_mm','Rocha_mm']]
Verano_Sur = data_verano[['Carrasco_mm', 'Colonia_mm','Florida_mm','PuntaEste_mm','Prado_mm','Melilla_mm','Rocha_mm']]
Invierno_Sur = data_invierno[['Carrasco_mm', 'Colonia_mm','Florida_mm','PuntaEste_mm','Prado_mm','Melilla_mm','Rocha_mm']]
Oto_Sur = data_oto[['Carrasco_mm', 'Colonia_mm','Florida_mm','PuntaEste_mm','Prado_mm','Melilla_mm','Rocha_mm']]

#CENTRO
Primavera_Centro = data_primavera[['Durazno_mm', 'Trinidad_mm','TreintaTres_mm','Melo_mm','Paysandu_mm', 'Young_mm','Mercedes_mm']]
Verano_Centro = data_verano[['Durazno_mm', 'Trinidad_mm','TreintaTres_mm','Melo_mm','Paysandu_mm', 'Young_mm','Mercedes_mm']]
Invierno_Centro = data_invierno[['Durazno_mm', 'Trinidad_mm','TreintaTres_mm','Melo_mm','Paysandu_mm', 'Young_mm','Mercedes_mm']]
Oto_Centro= data_oto[['Durazno_mm', 'Trinidad_mm','TreintaTres_mm','Melo_mm','Paysandu_mm', 'Young_mm','Mercedes_mm']]

#PROMEDIOS ANUALES POR ESTACIONES DEL AÑO PARA CADA REGION DEL PAIS
#def calcular_promedio_total(df, nombre_df):
    # Extrae el año de la columna 'Fecha'
   # df['Año'] = df.index.year

    # Calcula el promedio por año para todas las columnas
   # promedio_por_año = df.groupby('Año').mean()

    # Luego, calcula el promedio de los promedios de todas las columnas
   # promedio_total = promedio_por_año.mean(axis=1)

    # Crea un DataFrame para los resultados con 'Año' y 'Promedio'
   # resultado = pd.DataFrame({'Año': promedio_total.index, 'Promedio': promedio_total})

    # Agrega el nombre del DataFrame como una columna
   # resultado['DataFrame'] = nombre_df

  #  return resultado

# Lista de DataFrames a procesar con sus respectivos nombres
#dataframes = [(Primavera_Norte, 'Primavera_Norte'), (Verano_Norte, 'Verano_Norte'), (Invierno_Norte, 'Invierno_Norte'), (Oto_Norte, 'Oto_Norte'),
             # (Primavera_Sur, 'Primavera_Sur'), (Verano_Sur, 'Verano_Sur'), (Invierno_Sur, 'Invierno_Sur'), (Oto_Sur, 'Oto_Sur'),
             # (Primavera_Centro, 'Primavera_Centro'), (Verano_Centro, 'Verano_Centro'), (Oto_Centro, 'Oto_Centro'),(Invierno_Centro, 'Invierno_Centro')]

# Procesa cada DataFrame y guarda los resultados en archivos CSV
#for i, (df, nombre_df) in enumerate(dataframes):
    #resultado = calcular_promedio_total(df, nombre_df)
   # nombre_archivo = f'promedio_total_{nombre_df}.csv'

    # Guarda los resultados en el archivo CSV
   # resultado.to_csv(nombre_archivo, index=False)

# Obtener una lista de todas las estaciones en los DataFrames
estaciones_df1 = Verano_Norte.columns[0:]  # Suponiendo que la primera columna es 'fecha'
estaciones_df2 = Verano_Centro.columns[0:]
estaciones_df3 = Verano_Sur.columns[0:]

# Crear una figura con tres subplots en columnas
fig, axs = plt.subplots(1, 3, figsize=(20, 5))

# Iterar a través de las estaciones y crear un gráfico de líneas para cada una en cada DataFrame
for estacion in estaciones_df1:
    axs[0].plot(Verano_Norte.index, Verano_Norte[estacion], label=estacion)

for estacion in estaciones_df2:
    axs[1].plot(Verano_Centro.index, Verano_Centro[estacion], label=estacion)

for estacion in estaciones_df3:
    axs[2].plot(Verano_Sur.index, Verano_Sur[estacion], label=estacion)

# Configurar límites en el eje y
for ax in axs:
    ax.set_ylim(0, 1200)

# Configurar títulos, etiquetas y leyendas para los subplots
axs[0].set_title('Región Norte acumulado Verano')
axs[0].set_xlabel('Fecha')
axs[0].set_ylabel('Precipitación (mm)')
axs[0].legend(ncol=2, fontsize=8)

axs[1].set_title('Región Centro acumulado Verano')
axs[1].set_xlabel('Fecha')
axs[1].set_ylabel('Precipitación (mm)')
axs[1].legend(ncol=2, fontsize=8.5)

axs[2].set_title('Region Sur acumulado Verano')
axs[2].set_xlabel('Fecha')
axs[2].set_ylabel('Precipitación (mm)')
axs[2].legend(ncol=2, fontsize=8.5)

# Ajustar el espacio entre subplots
plt.savefig("Verano.pdf",bbox_inches="tight")
plt.tight_layout()

# Mostrar el gráfico
plt.show()

# Obtener una lista de todas las estaciones en los DataFrames
estaciones_df1 = Primavera_Norte.columns[0:]  # Suponiendo que la primera columna es 'fecha'
estaciones_df2 = Primavera_Centro.columns[0:]
estaciones_df3 = Primavera_Sur.columns[0:]

# Crear una figura con tres subplots en columnas
fig, axs = plt.subplots(1, 3, figsize=(20, 5))

# Iterar a través de las estaciones y crear un gráfico de líneas para cada una en cada DataFrame
for estacion in estaciones_df1:
    axs[0].plot(Primavera_Norte.index, Primavera_Norte[estacion], label=estacion)

for estacion in estaciones_df2:
    axs[1].plot(Primavera_Centro.index, Primavera_Centro[estacion], label=estacion)

for estacion in estaciones_df3:
    axs[2].plot(Primavera_Sur.index, Primavera_Sur[estacion], label=estacion)

# Configurar límites en el eje y
for ax in axs:
    ax.set_ylim(0, 1200)

# Configurar títulos, etiquetas y leyendas para los subplots
axs[0].set_title('Región Norte acumulado Primavera')
axs[0].set_xlabel('Fecha')
axs[0].set_ylabel('Precipitación (mm)')
axs[0].legend(ncol=2, fontsize=8)

axs[1].set_title('Región Centro acumulado Primavera')
axs[1].set_xlabel('Fecha')
axs[1].set_ylabel('Precipitación (mm)')
axs[1].legend(ncol=2, fontsize=8)

axs[2].set_title('Region Sur acumulado Primavera')
axs[2].set_xlabel('Fecha')
axs[2].set_ylabel('Precipitación (mm)')
axs[2].legend(ncol=2, fontsize=8.5)

# Ajustar el espacio entre subplots
plt.savefig("Prim.pdf",bbox_inches="tight")
plt.tight_layout()

# Mostrar el gráfico
plt.show()

# Obtener una lista de todas las estaciones en los DataFrames
estaciones_df1 = Oto_Norte.columns[0:]  # Suponiendo que la primera columna es 'fecha'
estaciones_df2 = Oto_Centro.columns[0:]
estaciones_df3 = Oto_Sur.columns[0:]

# Crear una figura con tres subplots en columnas
fig, axs = plt.subplots(1, 3, figsize=(20, 5))

# Iterar a través de las estaciones y crear un gráfico de líneas para cada una en cada DataFrame
for estacion in estaciones_df1:
    axs[0].plot(Oto_Norte.index, Oto_Norte[estacion], label=estacion)

for estacion in estaciones_df2:
    axs[1].plot(Oto_Centro.index, Oto_Centro[estacion], label=estacion)

for estacion in estaciones_df3:
    axs[2].plot(Oto_Sur.index, Oto_Sur[estacion], label=estacion)

# Configurar límites en el eje y
for ax in axs:
    ax.set_ylim(0, 1200)

# Configurar títulos, etiquetas y leyendas para los subplots
axs[0].set_title('Región Norte acumulado Otoño')
axs[0].set_xlabel('Fecha')
axs[0].set_ylabel('Precipitación (mm)')
axs[0].legend(ncol=2, fontsize=8.5)

axs[1].set_title('Región Centro acumulado Otoño')
axs[1].set_xlabel('Fecha')
axs[1].set_ylabel('Precipitación (mm)')
axs[1].legend(ncol=2, fontsize=8.5)

axs[2].set_title('Region Sur acumulado Otoño')
axs[2].set_xlabel('Fecha')
axs[2].set_ylabel('Precipitación (mm)')
axs[2].legend(ncol=2, fontsize=8.5)

# Ajustar el espacio entre subplots
plt.savefig("Oto.svg",bbox_inches="tight")
plt.tight_layout()

# Mostrar el gráfico
plt.show()

# Obtener una lista de todas las estaciones en los DataFrames
estaciones_df1 = Invierno_Norte.columns[0:]  # Suponiendo que la primera columna es 'fecha'
estaciones_df2 = Invierno_Centro.columns[0:]
estaciones_df3 = Invierno_Sur.columns[0:]

# Crear una figura con tres subplots en columnas
fig, axs = plt.subplots(1, 3, figsize=(20, 5))

# Iterar a través de las estaciones y crear un gráfico de líneas para cada una en cada DataFrame
for estacion in estaciones_df1:
    axs[0].plot(Invierno_Norte.index, Invierno_Norte[estacion], label=estacion)

for estacion in estaciones_df2:
    axs[1].plot(Invierno_Centro.index, Invierno_Centro[estacion], label=estacion)

for estacion in estaciones_df3:
    axs[2].plot(Invierno_Sur.index, Invierno_Sur[estacion], label=estacion)

# Configurar límites en el eje y
for ax in axs:
    ax.set_ylim(0, 1200)

# Configurar títulos, etiquetas y leyendas para los subplots
axs[0].set_title('Región Norte acumulado Invierno')
axs[0].set_xlabel('Fecha')
axs[0].set_ylabel('Precipitación (mm)')
axs[0].legend(ncol=2, fontsize=8)

axs[1].set_title('Región Centro acumulado Invierno')
axs[1].set_xlabel('Fecha')
axs[1].set_ylabel('Precipitación (mm)')
axs[1].legend(ncol=2, fontsize=8.5)

axs[2].set_title('Region Sur acumulado Invierno')
axs[2].set_xlabel('Fecha')
axs[2].set_ylabel('Precipitación (mm)')
axs[2].legend(ncol=2, fontsize=8.5)

# Ajustar el espacio entre subplots
plt.savefig("Invierno.pdf",bbox_inches="tight")
plt.tight_layout()

# Mostrar el gráfico
plt.show()

"""### **Estadísticos**"""

estadistico_diario=data_imputado.describe()
estadisticos_anual = data_anual.describe()
estadisticos_anual
estadistico_semestral= data_semestre.describe()
#estadistico_semestral
estadistico_mensual=data_meses.describe()
#estadistico_mensual

estadisticos_anual

"""### **Visualizaciones**"""

#GRAFICO ANUAL PARA TODAS LAS ESTACIONES
# Calcular el total de precipitaciones por año
totales_por_año = data_anual.sum(axis=1)

# Crear el gráfico de barras
plt.figure(figsize=(10,5))
colors = ['#FFC300' if year != 2002 and year != 2008 else ('blue' if year == 2002 else 'red') for year in totales_por_año.index.year]
plt.bar(totales_por_año.index.year, totales_por_año,color=colors,width=0.65)


# Agregar etiquetas y título
plt.xlabel('Año',fontsize=17)
#plt.xlabel('Year',fontsize=17)
plt.ylabel('Precipitación acumulada mm',fontsize=17)
#plt.ylabel('Accumulated precipitation mm',fontsize=17)
plt.title('Precipitación acumulada anual en todas las estaciones Inumet 1991-2021',fontsize=17)
#plt.title('Annual accumulated precipitation at all Inumet stations 1991-2021',fontsize=17)
# Mostrar todas las etiquetas de años en el eje x
plt.xticks(ticks=totales_por_año.index.year, labels=totales_por_año.index.year, rotation=90,fontsize=12)
# Ajustar los límites del eje x
min_year = totales_por_año.index.year.min() - 1
max_year = totales_por_año.index.year.max() + 1
plt.xlim(min_year, max_year)

plt.tick_params(axis='y', labelsize=12)

#Parche para etiquetas
dosmildos = mpatches.Patch(color='blue', label='2002 año mas lluvioso ')
#dosmildos = mpatches.Patch(color='blue', label='2002 the rainiest year')
dosmilocho = mpatches.Patch(color='red', label='2008 año menos lluvioso ')
#dosmilocho = mpatches.Patch(color='red', label='2008 the least rainy year')
plt.legend(handles=[dosmildos,dosmilocho], fontsize= 11, loc='upper left')

# Mostrar el gráfico
plt.savefig("AcumuladosInumetInglés.pdf",bbox_inches="tight")
plt.show()

#Grafico de año mas seco y luvioso por meses para todo el periodo de estudio
# Calcular el total de precipitaciones por mes
totales_por_mes= data_meses.sum(axis=1)
# Seleccionar los datos para los años 2002 y 2008
lluvia = totales_por_mes.loc["2001-12-31":"2002-12-31"]
sequia = totales_por_mes.loc["2007-12-31":"2008-12-31"]
# Convertir las Series en DataFrames
lluvia_df = lluvia.to_frame().reset_index()
sequia_df = sequia.to_frame().reset_index()

# Encontrar los valores máximos y mínimos de los datos para ambos años
y_min = min(lluvia.min(), sequia.min()) - 30
y_max = max(lluvia.max(), sequia.max()) + 30

# Crear los gráficos
fig, axes = plt.subplots(1, 2, figsize=(12, 7))
plt.xlabel("Fecha")
plt.ylabel("Precipitación Acumulada mm")
fig.suptitle('Año con más y menos acumulados de lluvia en Uruguay')

# Graficar el año 2002 más lluvioso
sns.lineplot(ax=axes[0], data=lluvia_df, x='Fecha', y=0,
             marker='o', dashes=False, color="blue").set_title("Año 2002 mas lluvioso")

# Graficar el año 2008 menos lluvioso
sns.lineplot(ax=axes[1], data=sequia_df, x='Fecha', y=0,
             marker='o', dashes=False, color="red").set_title("Año 2008 menos lluvioso")

# Establecer los límites del eje Y en ambos gráficos
axes[0].set_ylim(y_min, y_max)
axes[1].set_ylim(y_min, y_max)

# Ajustar el formato de las fechas en el eje x
fig.autofmt_xdate(rotation=90)

plt.savefig("mayorMenorPreci.pdf",bbox_inches="tight")
plt.show()

#GRAFICO PRECIPITACIONES ACUMULADAS ANUALES POR ESTACIONES
# Lista de colores para las estaciones
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#7b4173', '#4daf4a', '#e41a1c', '#377eb8', '#984ea3', '#ff9d00', '#a65628']

plt.figure(figsize=(10, 5))

# Itera sobre las estaciones y crea una línea para cada una
for i, estacion in enumerate(data_anual.columns):
    plt.plot(data_anual.index, data_anual[estacion], color=colors[i], label=estacion)

plt.xlabel('Fecha')
#plt.xlabel('Date')
plt.ylabel('Precipitaciones Acumuladas mm')
#plt.ylabel('Accumulated Precipitation mm')
plt.title('Precipitaciones acumuladas por estaciones 1991-2021')
#plt.title('Accumulated precipitation by meteorological stations 1991-2021')
# Ajusta la ubicación y orientación de la leyenda
plt.legend(loc='upper right', bbox_to_anchor=(1, 1), ncol=3, fontsize='small')

plt.grid(True)
plt.tight_layout()
plt.savefig("TodasEstacionesAnual.pdf",bbox_inches="tight")
plt.show()

# Estaciones de cada región
estaciones_norte = ['Artigas_mm', 'Rivera_mm', 'Salto_mm']
estaciones_centro = ['Durazno_mm', 'Trinidad_mm','TreintaTres_mm','Melo_mm','Paysandu_mm', 'Young_mm','Mercedes_mm']
estaciones_sur = ['Carrasco_mm', 'Colonia_mm','Florida_mm','PuntaEste_mm','Prado_mm','Melilla_mm','Rocha_mm']

# Lista de colores para las estaciones
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#7b4173', '#4daf4a', '#e41a1c', '#377eb8', '#984ea3', '#ff9d00', '#a65628']

# Crear una figura con subplots en una columna
fig, axs = plt.subplots(3, 1, figsize=(10, 10))
# Establecer los límites del eje Y en ambos gráficos
axs[0].set_ylim(400, 3000)
axs[1].set_ylim(400, 3000)
axs[2].set_ylim(400, 3000)
# Gráficos de las distintas regiones
regiones = [estaciones_norte, estaciones_centro, estaciones_sur]
#titulos = ['Norte', 'Centro', 'Sur']
titulos = ['Northern', 'Centeral', 'Southern']

for ax, estaciones, titulo in zip(axs, regiones, titulos):
    for i, estacion in enumerate(estaciones):
        ax.plot(data_anual.index, data_anual[estacion], color=colors[i], label=estacion)

    ax.set_xlabel('Año')
    #ax.set_xlabel('Year')
    ax.set_ylabel('Acumulados mm ')
    #ax.set_ylabel('Accumulated mm')
    ax.set_title(f'Precipitaciones acumuladas anuales en las estaciones de la región {titulo}')
    #ax.set_title(f'Annual accumulated rainfall at stations in the {titulo} region ')
    ax.legend(loc='upper right', bbox_to_anchor=(1, 1), ncol=3,fontsize=11)
    ax.grid(True)

    # Ajustar los límites del eje x
    ax.set_xlim(data_anual.index.min(), data_anual.index.max())

plt.tight_layout()
plt.savefig("AnualPorRegionInglés.pdf",bbox_inches="tight")
plt.show()

import geopandas as gpd
from shapely.geometry import Point

# -----------------------------------------------------------
# 1) Coordenadas de cada estación
# -----------------------------------------------------------
coords = {
    'Artigas_mm':    {'lat': -30.398215, 'lon': -56.511778},
    'Rivera_mm':     {'lat': -30.97031,  'lon': -55.4735},
    'Melo_mm':       {'lat': -32.366735, 'lon': -54.192783},
    'TreintaTres_mm':{'lat': -33.222860, 'lon': -54.387341},
    'Rocha_mm':      {'lat': -34.493570, 'lon': -54.312595},
    'PuntaEste_mm':  {'lat': -34.968940, 'lon': -54.951223},
    'Carrasco_mm':   {'lat': -34.832923, 'lon': -56.012876},
    'Prado_mm':      {'lat': -34.860676, 'lon': -56.207861},
    'Melilla_mm':    {'lat': -34.789114, 'lon': -56.263134},
    'Florida_mm':    {'lat': -34.088473, 'lon': -56.186948},
    'Trinidad_mm':   {'lat': -33.536566, 'lon': -56.915996},
    'Durazno_mm':    {'lat': -33.351075, 'lon': -56.502345},
    'Colonia_mm':    {'lat': -34.451874, 'lon': -57.767770},
    'Mercedes_mm':   {'lat': -33.250514, 'lon': -58.069076},
    'Young_mm':      {'lat': -32.686348, 'lon': -57.649920},
    'Paysandu_mm':   {'lat': -32.379524, 'lon': -58.027395},
    'Salto_mm':      {'lat': -31.438397, 'lon': -57.983003}
}

# -----------------------------------------------------------
# 2) Calcular el promedio multianual de precipitación para cada estación
# -----------------------------------------------------------
# Se asume que 'data_anual' es un DataFrame cuyas columnas son exactamente los nombres de estaciones:
# ['Artigas_mm','Rivera_mm', ... , 'Salto_mm'], y sus índices son años (int o datetime).
#
# Ejemplo de estructura de data_anual:
#             Artigas_mm  Rivera_mm   Melo_mm   ...   Salto_mm
# Año
# 1980        120.5       95.2       110.3     ...   80.1
# 1981         98.7       85.4        90.2    ...   70.3
# ...          ...        ...         ...     ...    ...
# 2020        105.2      100.1       115.3    ...   95.0
#
# Calculamos el promedio (media) de cada columna:
promedios = data_anual.mean(axis=0)  # Serie con índice = nombre de estación, valor = promedio anual

# Construimos un DataFrame para graficar con GeoPandas
df_plot = pd.DataFrame({
    'Estacion': list(coords.keys()),
    'Latitud' : [coords[est]['lat'] for est in coords.keys()],
    'Longitud': [coords[est]['lon'] for est in coords.keys()],
    'Promedio_Anual_mm': [promedios[est] for est in coords.keys()]
})

# -----------------------------------------------------------
# 3) Convertir a GeoDataFrame
# -----------------------------------------------------------
geometry = [Point(xy) for xy in zip(df_plot['Longitud'], df_plot['Latitud'])]
gdf_plot = gpd.GeoDataFrame(df_plot, geometry=geometry, crs="EPSG:4326")


# -----------------------------------------------------------
# 4) Cargar shapefile de países de Natural Earth y filtrar Uruguay
# -----------------------------------------------------------
# Asegurate de cambiar esta ruta a donde realmente esté tu shapefile descomprimido.
shapefile_path = "/content/drive/MyDrive/datasCo/ne_110m_admin_0_countries.shp"
world = gpd.read_file(shapefile_path)

# Filtrar solo Uruguay (revisar si el campo se llama 'NAME' o 'ADMIN' en tu shapefile)
if 'NAME' in world.columns:
    uruguay = world[world['NAME'] == 'Uruguay']
elif 'ADMIN' in world.columns:
    uruguay = world[world['ADMIN'] == 'Uruguay']
else:
    raise ValueError("No encontré la columna 'NAME' ni 'ADMIN' en el shapefile para filtrar Uruguay.")


# -----------------------------------------------------------
# 5) Graficar el contorno de Uruguay y las estaciones coloreadas
# -----------------------------------------------------------
fig, ax = plt.subplots(1, 1, figsize=(8, 8))

# 5.1) Dibujar únicamente el borde de Uruguay
uruguay.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=1)

# 5.2) Pintar los puntos de estación según el promedio multianual
gdf_plot.plot(
    ax=ax,
    column='Promedio_Anual_mm',
    cmap='viridis',
    markersize=100,
    edgecolor='k',
    legend=True
)

# 5.3) Etiquetas y personalización del mapa
ax.set_title('Promedio anual de precipitación (mm) \n Estaciones en Uruguay', fontsize=14)
ax.set_xlabel('Longitud (°)', fontsize=12)
ax.set_ylabel('Latitud (°)', fontsize=12)

# 5.4) Anotar cada estación junto a su punto
for _, row in gdf_plot.iterrows():
    ax.text(
        row['Longitud'] + 0.1,
        row['Latitud'] + 0.1,
        row['Estacion'].replace('_mm',''),
        fontsize=8
    )

ax.grid(True)
plt.tight_layout()
plt.show()

# Definir el color fluorescente
color_fluorescente = '#B9B99E'  # Color para la desviación estándar

# Configuración de la figura
ancho_columna = 10
aspect_ratio = 1.25    # Relación de aspecto
altura_figura = ancho_columna / aspect_ratio  # Altura de la figura

# Crear una figura con subplots en una columna
fig, axs = plt.subplots(3, 1, figsize=(ancho_columna, altura_figura))

# Configuración de los límites del eje Y
for ax in axs:
    ax.set_ylim(400, 3000)

# Gráficos de las distintas regiones
regiones = [estaciones_norte, estaciones_centro, estaciones_sur]
titulos = ['Norte', 'Centro', 'Sur']

for ax, estaciones, titulo in zip(axs, regiones, titulos):
    for i, estacion in enumerate(estaciones):
        ax.plot(data_anual.index, data_anual[estacion], color=colors[i], label=estacion)

    # Calcular la media y la desviación estándar para la región
    data_region = data_anual[estaciones].mean(axis=1)
    media_region = data_region.mean()
    desviacion_region = data_region.std()

    # Línea de media
    ax.plot(data_region.index, data_region, color='black', linestyle='--', linewidth=2, label=f'{titulo} mean')

    # Zona de desviación estándar
    ax.fill_between(data_region.index,
                    data_region - desviacion_region,
                    data_region + desviacion_region,
                    color=color_fluorescente, alpha=0.5, label=f'{titulo} std dev')

    ax.set_xlabel('Año', fontsize=16)
    ax.set_ylabel('Acumulado mm', fontsize=16)
    ax.set_title(f'Precipitación acumulada anual en las estaciones de la región {titulo}', fontsize=18)
    ax.legend(loc='upper right', bbox_to_anchor=(1, 1), ncol=3, fontsize=10)
    ax.grid(True)

    # Ajustar los límites del eje X
    ax.set_xlim(data_anual.index.min(), data_anual.index.max())

    # Ajustar el tamaño de las etiquetas de los ejes
    ax.tick_params(axis='both', which='major', labelsize=14)

# Ajustar diseño y guardar el gráfico
plt.tight_layout()
plt.savefig("AnualPorRegionInglés.pdf", bbox_inches="tight")
plt.show()

"""**ACUMULADAS POR ESTACIONES CLIMÁTICAS:**"""

#Pecipitacion acumulada por estaciones en VERANO

# Lista de colores para las estaciones
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#7b4173', '#4daf4a', '#e41a1c', '#377eb8', '#984ea3', '#ff9d00', '#a65628']

ax=plt.figure(figsize=(10, 5))


# Itera sobre las estaciones y crea una línea para cada una
for i, estacion in enumerate(data_verano.columns):
    plt.plot(data_verano.index, data_verano[estacion], color=colors[i], label=estacion)

plt.xlabel('Fecha')
plt.ylabel('Precipitaciones Acumuladas mm')
plt.title('Precipitaciones acumuladas en Verano por estaciones 1991-2021')

# Ajusta la ubicación y orientación de la leyenda
plt.legend(loc='upper right', bbox_to_anchor=(1, 1), ncol=3,fontsize=8)

plt.grid(True)
plt.tight_layout()
plt.show()

#Pecipitacion acumulada por estaciones en PRIMAVERA

# Lista de colores para las estaciones
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#7b4173', '#4daf4a', '#e41a1c', '#377eb8', '#984ea3', '#ff9d00', '#a65628']

plt.figure(figsize=(10, 5))

# Itera sobre las estaciones y crea una línea para cada una
for i, estacion in enumerate(data_primavera.columns):
    plt.plot(data_primavera.index, data_primavera[estacion], color=colors[i], label=estacion)

plt.xlabel('Fecha')
plt.ylabel('Precipitaciones Acumuladas mm')
plt.title('Precipitaciones acumuladas en Primavera por estaciones 1991-2021')

# Ajusta la ubicación y orientación de la leyenda
plt.legend(loc='upper right', bbox_to_anchor=(1, 1), ncol=3,fontsize=8)

# Establece los límites del eje y
plt.ylim(0, 1200)

plt.grid(True)
plt.tight_layout()
plt.show()

#Pecipitacion acumulada por estaciones en OTOÑO

# Lista de colores para las estaciones
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#7b4173', '#4daf4a', '#e41a1c', '#377eb8', '#984ea3', '#ff9d00', '#a65628']

plt.figure(figsize=(10, 5))

# Itera sobre las estaciones y crea una línea para cada una
for i, estacion in enumerate(data_oto.columns):
    plt.plot(data_oto.index, data_oto[estacion], color=colors[i], label=estacion)

plt.xlabel('Fecha')
plt.ylabel('Precipitaciones Acumuladas mm')
plt.title('Precipitaciones acumuladas en Otoño por estaciones 1991-2021')

# Ajusta la ubicación y orientación de la leyenda
plt.legend(loc='upper right', bbox_to_anchor=(1, 1), ncol=3,fontsize=8)

# Establece los límites del eje y
plt.ylim(0, 1200)

plt.grid(True)
plt.tight_layout()
plt.show()

#Pecipitacion acumulada por estaciones en INVIERNO

# Lista de colores para las estaciones
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#7b4173', '#4daf4a', '#e41a1c', '#377eb8', '#984ea3', '#ff9d00', '#a65628']

plt.figure(figsize=(10, 5))

# Itera sobre las estaciones y crea una línea para cada una
for i, estacion in enumerate(data_invierno.columns):
    plt.plot(data_invierno.index, data_invierno[estacion], color=colors[i], label=estacion)

plt.xlabel('Fecha')
plt.ylabel('Precipitaciones Acumuladas mm')
plt.title('Precipitaciones acumuladas en INVIERNO por estaciones 1991-2021')

# Ajusta la ubicación y orientación de la leyenda
plt.legend(loc='upper right', bbox_to_anchor=(1, 1), ncol=3,fontsize=8)
# Establece los límites del eje y
plt.ylim(0, 1200)

plt.grid(True)
plt.tight_layout()
plt.show()

mean_precipitation = data_anual.mean()
std_deviation = data_anual.std()

# Crea una figura para el gráfico
plt.figure(figsize=(10, 6))

# Crea un gráfico de barras para la media de precipitaciones
plt.bar(mean_precipitation.index, mean_precipitation, color='blue', label='Media')

# Agrega las barras de desviación estándar como error bars
plt.errorbar(mean_precipitation.index, mean_precipitation, yerr=std_deviation, fmt='o', color='red', label='Desviación Estándar')

# Configura etiquetas y título
plt.xlabel('Estaciones')
plt.ylabel('Precipitación')
plt.title('Media y Desviación Estándar de Precipitaciones Anuales por Estación')
plt.xticks(rotation=45)
plt.legend(fontsize=9)

# Muestra el gráfico
plt.tight_layout()
plt.show()

# Crea una figura para el gráfico
plt.figure(figsize=(10, 6))

# Crea las líneas de media y desviación estándar
plt.plot(mean_precipitation.index, mean_precipitation, marker='o', label='Media')
plt.fill_between(mean_precipitation.index, mean_precipitation - std_deviation, mean_precipitation + std_deviation, alpha=0.2, label='Desviación Estándar')

# Configura etiquetas y título
plt.xlabel('Estaciones')
plt.ylabel('Precipitación')
plt.title('Media y Desviación Estándar de Precipitaciones Anuales por Estación')
plt.xticks(rotation=45)
plt.legend()

# Muestra el gráfico
plt.tight_layout()
plt.show()

#MEDIAS Y DESVIACIÓN ESTANDAR P

# Calcula las medias y desviaciones estándar de las precipitaciones anuales para cada región
mean_precipitations_norte = data_anual[estaciones_norte].mean(axis=1)
std_deviations_norte = data_anual[estaciones_norte].std(axis=1)

mean_precipitations_centro = data_anual[estaciones_centro].mean(axis=1)
std_deviations_centro = data_anual[estaciones_centro].std(axis=1)

mean_precipitations_sur = data_anual[estaciones_sur].mean(axis=1)
std_deviations_sur = data_anual[estaciones_sur].std(axis=1)

# Crea una figura para el gráfico
plt.figure(figsize=(10, 6))

# Crea las líneas de media y desviación estándar para las estaciones en cada región
plt.plot(data_anual.index, mean_precipitations_norte, marker='o', label='Norte - Media')
plt.fill_between(data_anual.index, mean_precipitations_norte - std_deviations_norte, mean_precipitations_norte + std_deviations_norte, alpha=0.2, label='Norte - Desviación Estándar')

plt.plot(data_anual.index, mean_precipitations_centro, marker='o', label='Centro - Media')
plt.fill_between(data_anual.index, mean_precipitations_centro - std_deviations_centro, mean_precipitations_centro + std_deviations_centro, alpha=0.2, label='Centro - Desviación Estándar')

plt.plot(data_anual.index, mean_precipitations_sur, marker='o', label='Sur - Media')
plt.fill_between(data_anual.index, mean_precipitations_sur - std_deviations_sur, mean_precipitations_sur + std_deviations_sur, alpha=0.2, label='Sur - Desviación Estándar')

# Configura etiquetas y título
plt.xlabel('Año',fontsize=15)
plt.ylabel('Precipitación Acumulada mm',fontsize=15)
plt.title('Media y Desviación Estándar de Precipitaciones Anuales por Región Inumet',fontsize=15)
plt.xticks(rotation=45,fontsize=12)
plt.yticks(fontsize=12)
plt.legend()

# Ajusta los límites del eje x
plt.xlim(data_anual.index[0], data_anual.index[-1])

# Muestra el gráfico
plt.tight_layout()
plt.savefig("MediaDesviacionPorRegion.pdf",bbox_inches="tight")
plt.show()

"""#**ANÁLISIS PARA SERIES TEMPORALES**
- Descomposición de la serie temporal
- Análisis de estacionalidad
- Prueba de estacionariedad
- Modelo ARIMA
- Modelo ETS

https://towardsdatascience.com/rainfall-time-series-analysis-and-forecasting-87a29316494e

https://www.kaggle.com/code/kanncaa1/time-series-prediction-tutorial-with-eda

## **Descomposición de la serie**

### **ADDITIVA ESTACIONES NORTE**
"""

# Traza un componente de serie temporal en un eje matplotlib especificado.
def plot_component(ax, data, color, label):
    ax.plot(data.index, data, color=color, label=label)
    # ax.legend(loc='best')  # Comentado para eliminar la leyenda

# Descompone en sus componentes (original, tendencia, estacionalidad y residuos)
def plot_station_decomposition(station, data, ax_original, ax_trend, ax_seasonal, ax_residual):
    decomposition = sm.tsa.seasonal_decompose(data, model='additive')

    plot_component(ax_original, data, '#5B68E7', 'Original')
    plot_component(ax_trend, decomposition.trend, '#FF335B', 'Tendencia')
    plot_component(ax_seasonal, decomposition.seasonal, '#056010', 'Estacionalidad')
    plot_component(ax_residual, decomposition.resid, '#D75800', 'Residuos')

stations = ['Rivera_mm', 'Salto_mm', 'Artigas_mm']
fig, axes = plt.subplots(4, len(stations), figsize=(18, 10))

for i, station in enumerate(stations):
    data = data_meses[station]
    plot_station_decomposition(station, data, axes[0, i], axes[1, i], axes[2, i], axes[3, i])

# Configura los títulos de las columnas
for i, station in enumerate(stations):
    axes[0, i].set_title(f'Estación {station}')
    axes[1, i].set_title('')
    axes[2, i].set_title('')
    axes[3, i].set_title('')

# Configura los títulos de las filas
axes[0, 0].set_ylabel('Original')
axes[1, 0].set_ylabel('Tendencia')
axes[2, 0].set_ylabel('Estacionalidad')
axes[3, 0].set_ylabel('Residuos')

# Ajusta los espacios entre los gráficos
plt.tight_layout()

# Muestra los gráficos
plt.show()

"""### **ADDITIVA ESTACIONES CENTRO**"""

#Traza un componente de serie temporal en un eje matplotlib especificado.
def plot_component(ax, data, color, label):
    ax.plot(data.index, data, color=color, label=label)

# Descompone en sus componentes (original, tendencia, estacionalidad y residuos)
def plot_station_decomposition(station, data, ax_original, ax_trend, ax_seasonal, ax_residual):
    decomposition = sm.tsa.seasonal_decompose(data, model='additive')

    plot_component(ax_original, data, '#5B68E7', 'Original')
    plot_component(ax_trend, decomposition.trend, '#FF335B', 'Tendencia')
    plot_component(ax_seasonal, decomposition.seasonal, '#056010', 'Estacionalidad')
    plot_component(ax_residual, decomposition.resid, '#D75800', 'Residuos')

stations = ['Durazno_mm', 'Trinidad_mm','TreintaTres_mm','Melo_mm','Paysandu_mm', 'Young_mm','Mercedes_mm']
fig, axes = plt.subplots(4, len(stations), figsize=(40, 7))
for i, station in enumerate(stations):
    data = data_meses[station]
    plot_station_decomposition(station, data, axes[0, i], axes[1, i], axes[2, i], axes[3, i])

# Configura los títulos de las columnas
for i, station in enumerate(stations):
    axes[0, i].set_title(f'Estación {station}')
    axes[1, i].set_title('')
    axes[2, i].set_title('')
    axes[3, i].set_title('')

# Configura los títulos de las filas
axes[0, 0].set_ylabel('Original')
axes[1, 0].set_ylabel('Tendencia')
axes[2, 0].set_ylabel('Estacionalidad')
axes[3, 0].set_ylabel('Residuos')

# Ajusta los espacios entre los gráficos
plt.tight_layout()

# Muestra los gráficos
plt.show()

"""### **ADDITIVA ESTACIONES SUR**"""

#Traza un componente de serie temporal en un eje matplotlib especificado.
def plot_component(ax, data, color, label):
    ax.plot(data.index, data, color=color, label=label)

# Descompone en sus componentes (original, tendencia, estacionalidad y residuos)
def plot_station_decomposition(station, data, ax_original, ax_trend, ax_seasonal, ax_residual):
    decomposition = sm.tsa.seasonal_decompose(data, model='additive')

    plot_component(ax_original, data,  '#5B68E7', 'Original')
    plot_component(ax_trend, decomposition.trend, '#FF335B', 'Tendencia')
    plot_component(ax_seasonal, decomposition.seasonal, '#056010', 'Estacionalidad')
    plot_component(ax_residual, decomposition.resid, '#D75800', 'Residuos')

estaciones_sur = ['Carrasco_mm', 'Colonia_mm','Florida_mm','PuntaEste_mm','Prado_mm','Melilla_mm','Rocha_mm']
fig, axes = plt.subplots(4, len(stations), figsize=(40, 7))
for i, station in enumerate(stations):
    data = data_meses[station]
    plot_station_decomposition(station, data, axes[0, i], axes[1, i], axes[2, i], axes[3, i])

# Configura los títulos de las columnas
for i, station in enumerate(stations):
    axes[0, i].set_title(f'Estación {station}')
    axes[1, i].set_title('')
    axes[2, i].set_title('')
    axes[3, i].set_title('')

# Configura los títulos de las filas
axes[0, 0].set_ylabel('Original')
axes[1, 0].set_ylabel('Tendencia')
axes[2, 0].set_ylabel('Estacionalidad')
axes[3, 0].set_ylabel('Residuos')

# Ajusta los espacios entre los gráficos
plt.tight_layout()

# Muestra los gráficos
plt.show()

# Define las estaciones por región
estaciones_norte = ['Artigas_mm', 'Rivera_mm', 'Salto_mm']
estaciones_centro = ['Durazno_mm', 'Trinidad_mm','TreintaTres_mm','Melo_mm','Paysandu_mm', 'Young_mm','Mercedes_mm']
estaciones_sur = ['Carrasco_mm', 'Colonia_mm','Florida_mm','PuntaEste_mm','Prado_mm','Melilla_mm','Rocha_mm']

# Función para calcular el promedio de las estaciones de cada región
def promedio_region(data, estaciones):
    return data[estaciones].mean(axis=1)

# Calcular el promedio para cada región
promedio_norte = promedio_region(data_meses, estaciones_norte)
promedio_centro = promedio_region(data_meses, estaciones_centro)
promedio_sur = promedio_region(data_meses, estaciones_sur)

# Función para descomponer y graficar
def plot_decomposition(data, title, axs):
    decomposition = sm.tsa.seasonal_decompose(data, model='additive')
    axs[0].plot(decomposition.observed, color='#5B68E7')
    axs[0].set_title(f'Original- {title}')
    axs[1].plot(decomposition.trend, color='#FF335B')
    axs[1].set_title(f'Trend - {title}')
    axs[2].plot(decomposition.seasonal, color='#056010')
    axs[2].set_title(f'Seasonal - {title}')
    axs[3].plot(decomposition.resid, color='#D75800')
    axs[3].set_title(f'Residual - {title}')

# Crear la figura y los ejes
fig, axs = plt.subplots(4, 3, figsize=(15, 10))  # 4 filas para Observed, Trend, Seasonal, y Residual y 3 columnas para cada región

# Graficar la descomposición para cada región
plot_decomposition(promedio_norte, 'Norte', axs[:, 0])
plot_decomposition(promedio_centro, 'Centro', axs[:, 1])
plot_decomposition(promedio_sur, 'Sur', axs[:, 2])

plt.tight_layout()
plt.savefig("DescomposicionRegionAd.png",bbox_inches="tight")
plt.show()

# Define las estaciones por región
estaciones_norte = ['Artigas_mm', 'Rivera_mm', 'Salto_mm']
estaciones_centro = ['Durazno_mm', 'Trinidad_mm','TreintaTres_mm','Melo_mm','Paysandu_mm', 'Young_mm','Mercedes_mm']
estaciones_sur = ['Carrasco_mm', 'Colonia_mm','Florida_mm','PuntaEste_mm','Prado_mm','Melilla_mm','Rocha_mm']

# Función para descomponer y obtener los límites de los ejes y
def get_decomposition_limits(data):
    decomposition = sm.tsa.seasonal_decompose(data, model='additive')
    observed_limits = (np.min(decomposition.observed), np.max(decomposition.observed))
    trend_limits = (np.min(decomposition.trend), np.max(decomposition.trend))
    seasonal_limits = (np.min(decomposition.seasonal), np.max(decomposition.seasonal))
    resid_limits = (np.min(decomposition.resid), np.max(decomposition.resid))
    return observed_limits, trend_limits, seasonal_limits, resid_limits

# Obtener los límites para cada componente en cada región
obs_limits_n, tr_limits_n, seas_limits_n, res_limits_n = get_decomposition_limits(promedio_norte)
obs_limits_c, tr_limits_c, seas_limits_c, res_limits_c = get_decomposition_limits(promedio_centro)
obs_limits_s, tr_limits_s, seas_limits_s, res_limits_s = get_decomposition_limits(promedio_sur)

# Encontrar los límites globales para cada componente
obs_limits = (min(obs_limits_n[0], obs_limits_c[0], obs_limits_s[0]), max(obs_limits_n[1], obs_limits_c[1], obs_limits_s[1]))
tr_limits = (min(tr_limits_n[0], tr_limits_c[0], tr_limits_s[0]), max(tr_limits_n[1], tr_limits_c[1], tr_limits_s[1]))
seas_limits = (min(seas_limits_n[0], seas_limits_c[0], seas_limits_s[0]), max(seas_limits_n[1], seas_limits_c[1], seas_limits_s[1]))
res_limits = (min(res_limits_n[0], res_limits_c[0], res_limits_s[0]), max(res_limits_n[1], res_limits_c[1], res_limits_s[1]))

# Función para descomponer y graficar con límites y sin etiquetas de y en columnas no iniciales
def plot_decomposition(data, title, axs, obs_limits, tr_limits, seas_limits, res_limits, show_ylabel=False):
    decomposition = sm.tsa.seasonal_decompose(data, model='additive')
    axs[0].plot(decomposition.observed, color='#5B68E7')
    axs[0].set_title(f'Original - {title}', fontsize=22)
    axs[0].set_ylim(obs_limits)
    axs[1].plot(decomposition.trend, color='#FF335B')
    axs[1].set_title(f'Trend - {title}', fontsize=22)
    axs[1].set_ylim(tr_limits)
    axs[2].plot(decomposition.seasonal, color='#056010')
    axs[2].set_title(f'Seasonal - {title}', fontsize=22)
    axs[2].set_ylim(seas_limits)
    axs[3].plot(decomposition.resid, color='#D75800')
    axs[3].set_title(f'Residual - {title}', fontsize=22)
    axs[3].set_ylim(res_limits)

    # Ajustar el tamaño de las etiquetas de los ejes
    for ax in axs:
        ax.tick_params(axis='both', which='major', labelsize=13)
        ax.tick_params(axis='both', which='minor', labelsize=13)

    if not show_ylabel:
        for ax in axs:
            ax.set_yticklabels([])

# Crear la figura y los ejes
fig, axs = plt.subplots(4, 3, figsize=(15, 10))  # 4 filas para Observed, Trend, Seasonal, y Residual y 3 columnas para cada región

# Graficar la descomposición para cada región
plot_decomposition(promedio_norte, 'North', axs[:, 0], obs_limits, tr_limits, seas_limits, res_limits, show_ylabel=True)
plot_decomposition(promedio_centro, 'Center', axs[:, 1], obs_limits, tr_limits, seas_limits, res_limits)
plot_decomposition(promedio_sur, 'South', axs[:, 2], obs_limits, tr_limits, seas_limits, res_limits)

# Ocultar los ejes x de todas las filas excepto la última
for ax in axs[:-1, :].flatten():
    ax.set_xticklabels([])
    ax.set_xlabel('')

# Ajustar el tamaño de las etiquetas de los ejes x
for ax in axs[-1, :]:
    ax.set_xlabel('Year', fontsize=16)

plt.tight_layout()
plt.savefig("DescomposicionRegionAd.pdf", bbox_inches="tight")
plt.show()

"""### Aditiva por estaciones 33, Artigas y Durazno"""

#ADDITIVO 33

decomposition = sm.tsa.seasonal_decompose(data_meses['TreintaTres_mm'], model='additive')
trend = decomposition.trend  # Trend Decomposition
seasonal = decomposition.seasonal  # Seasonal Decomposition
residual = decomposition.resid  # Residual Decomposition

plt.figure(figsize=(8, 8))

plt.subplot(411)
plt.plot(data_meses['TreintaTres_mm'], color='#5B68E7', label='Original')
plt.title('Descomposición Aditiva de la serie estación Treinta y Tres Inumet')
plt.legend(loc='best')

plt.subplot(412)
plt.plot(trend, color='#FF335B', label='Tendencia')
#plt.title('Trend')
plt.legend(loc='best')

plt.subplot(413)
plt.plot(seasonal, color='#056010', label='Variacion Estacional')
#plt.title('Seasonal Variations')
plt.legend(loc='best')

plt.subplot(414)
plt.plot(residual, color='#D75800', label='Residuos')
#plt.title('Irregular Variations')
plt.legend(loc='best')

plt.tight_layout()
plt.savefig("DescomposicionTT.pdf", bbox_inches="tight")
plt.show()

#ADDITIVO 33

decomposition = sm.tsa.seasonal_decompose(data_meses['TreintaTres_mm'], model='multiplicative')
trend = decomposition.trend  # Trend Decomposition
seasonal = decomposition.seasonal  # Seasonal Decomposition
residual = decomposition.resid  # Residual Decomposition

plt.figure(figsize=(8, 8))

plt.subplot(411)
plt.plot(data_meses['TreintaTres_mm'], color='#5B68E7', label='Original')
plt.title('Descomposición Multiplicativa de la serie estación Treinta y Tres Inumet')
plt.legend(loc='best')

plt.subplot(412)
plt.plot(trend, color='#FF335B', label='Tendencia')
#plt.title('Trend')
plt.legend(loc='best')

plt.subplot(413)
plt.plot(seasonal, color='#056010', label='Variacion Estacional')
#plt.title('Seasonal Variations')
plt.legend(loc='best')

plt.subplot(414)
plt.plot(residual, color='#D75800', label='Residuos')
#plt.title('Irregular Variations')
plt.legend(loc='best')

plt.tight_layout()
plt.savefig("DescomposicionTTM.pdf", bbox_inches="tight")
plt.show()

#ADDITIVO Artigas

decomposition = sm.tsa.seasonal_decompose(data_meses['Artigas_mm'], model='additive')
trend = decomposition.trend  # Trend Decomposition
seasonal = decomposition.seasonal  # Seasonal Decomposition
residual = decomposition.resid  # Residual Decomposition

plt.figure(figsize=(10, 10))

plt.subplot(411)
plt.plot(data_meses['Salto_mm'], color='#5B68E7', label='Original')
plt.title('Descomposición Aditiva de la serie estación Artigas Inumet')
plt.legend(loc='best')

plt.subplot(412)
plt.plot(trend, color='#FF335B', label='Tendencia')
plt.legend(loc='best')

plt.subplot(413)
plt.plot(seasonal, color='#056010', label='Variacion Estacional')
#plt.title('Seasonal Variations')
plt.legend(loc='best')

plt.subplot(414)
plt.plot(residual, color='#D75800', label='Residuos')
#plt.title('Irregular Variations')
plt.legend(loc='best')

plt.tight_layout()
plt.savefig("Artigas.svg", bbox_inches="tight")
plt.show()

#ADDITIVO Durazno

decomposition = sm.tsa.seasonal_decompose(data_meses['Durazno_mm'], model='additive')
trend = decomposition.trend  # Trend Decomposition
seasonal = decomposition.seasonal  # Seasonal Decomposition
residual = decomposition.resid  # Residual Decomposition

plt.figure(figsize=(10, 10))

plt.subplot(411)
plt.plot(data_meses['Salto_mm'], color='#5B68E7', label='Original')
plt.title('Descomposición Aditiva de la serie estación Durazno Inumet')
plt.legend(loc='best')

plt.subplot(412)
plt.plot(trend, color='#FF335B', label='Tendencia')
plt.legend(loc='best')

plt.subplot(413)
plt.plot(seasonal, color='#056010', label='Variacion Estacional')
#plt.title('Seasonal Variations')
plt.legend(loc='best')

plt.subplot(414)
plt.plot(residual, color='#D75800', label='Residuos')
#plt.title('Irregular Variations')
plt.legend(loc='best')

plt.tight_layout()
plt.savefig("Durazno.svg", bbox_inches="tight")
plt.show()

"""## **Prueba de estacionariedad Dickey-Fuller aumentada**

**Estacionariedad:** Un proceso es estacionario si su media y varianza son constantes en el tiempo, y si el valor de la covarianza entre dos periodos solo depende   de   la   distancia   entre   estos   dos   periodos   y   no   del momento en que se mida. Si la media depende del tiempo (no estacionariedad   en   la   media),   se   dice   que   la   serie   tiene   unatendencia determinista o que la serie es estacionaria alrededor dela   tendencia.

Prueba de estacionariedad Dickey-Fuller, es un procedimiento para contrastar la estacionariedad de una serie a partir del contrste de raices unitarias.

Para interpretar los resultados del test de Dickey-Fuller aumentado (ADF), es importante entender el significado de los valores obtenidos:

**ADF Statistic (Estadístico ADF):** Este valor es el estadístico de prueba calculado. Cuanto más negativo sea, más fuerte es la evidencia contra la hipótesis nula de que hay una raíz unitaria (la serie no es estacionaria).

**p-value (Valor p):** Este valor indica la probabilidad de obtener un resultado al menos tan extremo como el observado, suponiendo que la hipótesis nula es cierta. Un valor p bajo (típicamente menor que 0.05) indica que podemos rechazar la hipótesis nula y concluir que la serie es estacionaria.

**Critical Values (Valores críticos):** Estos valores son umbrales para el estadístico ADF a diferentes niveles de significancia (1%, 5%, y 10%). Si el estadístico ADF es menor (más negativo) que el valor crítico correspondiente, podemos rechazar la hipótesis nula a ese nivel de significancia.

p-value <= VC o ADF <= VC --- Es estacionaria
"""

# Itera sobre cada columna (estacion) del DataFrame
for column in data_meses.columns:
    print(f"Resultados del test ADF para la estación {column}:")

    # Selecciona la serie temporal de la estación
    precipitation_data = data_meses[column].dropna()  # Elimina valores NaN si los hay

    # Aplica el test de Dickey-Fuller aumentado
    result = adfuller(precipitation_data)

    # Extrae los resultados
    adf_statistic = result[0]
    p_value = result[1]
    critical_values = result[4]

    # Imprime los resultados
    print('   ADF Statistic:', adf_statistic)
    print('   p-value:', p_value)
    print('   Critical Values:')
    for key, value in critical_values.items():
        print(f'      {key}: {value}')
    print('\n')

"""# **INTERPRETACIÓN de resultados**
 Todas las estaciones mencionadas, los resultados indican que las series temporales de precipitaciones son estacionarias, dado que en todos los casos el valor p es extremadamente bajo y el estadístico ADF es menor que los valores críticos a todos los niveles de significancia.
"""

#Test Dickey-Fuller por regiones
# Función para aplicar el test de Dickey-Fuller aumentado y mostrar los resultados
def adf_test(serie, region):
    result = adfuller(serie.dropna())  # Elimina valores NaN si los hay
    adf_statistic = result[0]
    p_value = result[1]
    critical_values = result[4]

    print(f"Resultados del test ADF para la región {region}:")
    print('   ADF Statistic:', adf_statistic)
    print('   p-value:', p_value)
    print('   Critical Values:')
    for key, value in critical_values.items():
        print(f'      {key}: {value}')
    print('\n')

# Aplicar el test de Dickey-Fuller aumentado para cada región
adf_test(promedio_norte, 'Norte')
adf_test(promedio_centro, 'Centro')
adf_test(promedio_sur, 'Sur')

"""## **Análisis de tendencia**

## **Prueba de tendencia Mann Kendall:**
**Trend (Tendencia):** Indica si se detectó una tendencia ascendente, descendente o si no se detectó ninguna tendencia ("no trend").

**h (hipótesis nula):** Un valor de False indica que no se rechaza la hipótesis nula de que no hay tendencia. Si fuera True, indicaría que se rechaza la hipótesis nula, es decir, que se detecta una tendencia significativa.

**p-value:** El valor p asociado a la prueba. Un valor p alto (generalmente > 0.05) indica que no se puede rechazar la hipótesis nula de que no hay tendencia.

**z:** Estadístico de prueba normalizado. Valores cercanos a 0 indican que no hay tendencia.

**Tau:** Coeficiente de Kendall Tau, que mide la correlación entre la variable de tiempo y las precipitaciones. Valores cercanos a 0 indican poca o ninguna tendencia.

**s:** Estadístico S de Mann-Kendall. Un valor S positivo indica una tendencia ascendente, mientras que un valor S negativo indica una tendencia descendente. En tu caso, los valores S son bastante bajos en magnitud.

**var_s:** Varianza del estadístico S. Este valor se usa para calcular el estadístico z.

**slope (pendiente):** Estimación de la pendiente de la tendencia. Una pendiente negativa indica una tendencia descendente y una pendiente positiva indica una tendencia ascendente. En tu caso, las pendientes son muy pequeñas.

**intercept (intercepto):** Intercepto de la línea de tendencia estimada
"""

# Función para aplicar la prueba de Mann-Kendall y mostrar los resultados
def mann_kendall_test(serie, estacion):
    result = mk.original_test(serie.dropna())  # Elimina valores NaN si los hay
    print(f"Resultados del test de Mann-Kendall para la estación {estacion}:")
    print(result)
    print('\n')

# Itera sobre cada columna del DataFrame
for column in data_meses.columns:
    # Selecciona la serie temporal de la estación
    precipitation_data = data_meses[column]

    # Aplica la prueba de Mann-Kendall
    mann_kendall_test(precipitation_data, column)

"""# **Interpretación resultados:**
Los resultados de la prueba de Mann-Kendall para todas las estaciones indican que no se encontró evidencia de una tendencia significativa en las precipitaciones registradas.
"""

# Test de Mann-Kendall por regiones

# Función para aplicar el test de Mann-Kendall y mostrar los resultados
def mk_test(serie, region):
    result = mk.original_test(serie.dropna())  # Elimina valores NaN si los hay
    trend = result.trend
    h = result.h
    p_value = result.p
    z = result.z
    Tau = result.Tau
    s = result.s
    var_s = result.var_s
    slope = result.slope
    intercept = result.intercept

    print(f"Resultados del test de Mann-Kendall para la región {region}:")
    print('   Trend:', trend)
    print('   h:', h)
    print('   p-value:', p_value)
    print('   z:', z)
    print('   Tau:', Tau)
    print('   s:', s)
    print('   var_s:', var_s)
    print('   slope:', slope)
    print('   intercept:', intercept)
    print('\n')

# Aplicar el test de Mann-Kendall para cada región
mk_test(promedio_norte, 'Norte')
mk_test(promedio_centro, 'Centro')
mk_test(promedio_sur, 'Sur')

"""## **Autocovarianza y autocorrelación**"""

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Función para generar y mostrar los gráficos de ACF y PACF en subplots
def plot_acf_pacf_subplot(series_list, titles):
    with plt.style.context(['science', 'notebook']):
        # Configuración de la figura
        ancho_columna = 12  # Ajustar el ancho de la figura
        aspect_ratio = 2   # Relación de aspecto
        altura_figura = ancho_columna / aspect_ratio  # Altura de la figura

        num_plots = len(series_list)
        fig, axes = plt.subplots(num_plots, 2, figsize=(ancho_columna, altura_figura))

        # Asegurarse de que los ejes tengan la forma correcta incluso si hay solo un subplot
        if num_plots == 1:
            axes = axes.reshape(1, -1)

        for i, (series, title) in enumerate(zip(series_list, titles)):
            # Gráfico de autocorrelación simple (ACF)
            plot_acf(series, lags=10, ax=axes[i, 0], zero=True)
            axes[i, 0].set_title(f'Sample autocorrelation (ACF) - {title}', fontsize=18)
            axes[i, 0].set_xlabel('Lag', fontsize=16)
            axes[i, 0].set_ylabel('ACF', fontsize=16)
            axes[i, 0].grid(True)
            axes[i, 0].set_ylim([-0.5, 1.5])  # Ajuste del límite del eje y
            axes[i, 0].tick_params(axis='both', which='major', labelsize=14)

            # Gráfico de autocorrelación parcial (PACF)
            plot_pacf(series, lags=10, ax=axes[i, 1], zero=True)
            axes[i, 1].set_title(f'Partial autocorrelation (PACF) - {title}', fontsize=18)
            axes[i, 1].set_xlabel('Lag', fontsize=16)
            axes[i, 1].set_ylabel('PACF', fontsize=16)
            axes[i, 1].grid(True)
            axes[i, 1].set_ylim([-0.5, 1.5])  # Ajuste del límite del eje y
            axes[i, 1].tick_params(axis='both', which='major', labelsize=16)

        # Ajustar diseño y guardar el gráfico
        plt.tight_layout()
        plt.savefig("ACFPACF_SciencePlots.pdf", bbox_inches="tight")
        plt.show()

# Generar y mostrar los gráficos de ACF y PACF para cada región en subplots
plot_acf_pacf_subplot([promedio_norte, promedio_centro, promedio_sur],
                      ['North', 'Center', 'South'])

"""## **Modelos Autorregresivos**"""

# Define el conjunto de test y de train para Artigas
steps = 12
datos_train = data_meses['Artigas_mm'][:-steps]
datos_test  = data_meses['Artigas_mm'][-steps:]
print(f"Fechas train : {datos_train.index.min()} --- {datos_train.index.max()}  (n={len(datos_train)})")
print(f"Fechas test  : {datos_test.index.min()} --- {datos_test.index.max()}  (n={len(datos_test)})")

fig, ax = plt.subplots(figsize=(6, 2.5))
datos_train.plot(ax=ax, label='train')  # Modificado para usar 'Artigas_mm'
datos_test.plot(ax=ax, label='test')    # Modificado para usar 'Artigas_mm'

# Define los títulos de los ejes y del gráfico
ax.set_xlabel('Year',fontsize=18)
ax.set_ylabel('Accumulated mm',fontsize=18)
ax.set_title('Training and testing data for Artigas station',fontsize=18)

# Muestra la leyenda
ax.legend()

promedio_norte_train = promedio_norte.loc['1991-01-01':'2020-12-31']
promedio_norte_val = promedio_norte.loc['2021-01-01':'2021-12-31']

promedio_centro_train = promedio_centro.loc['1991-01-01':'2020-12-31']
promedio_centro_val = promedio_centro.loc['2021-01-01':'2021-12-31']

promedio_sur_train = promedio_sur.loc['1991-01-01':'2020-12-31']
promedio_sur_val = promedio_sur.loc['2021-01-01':'2021-12-31']

# Datos de entrenamiento y prueba para el promedio de cada región
steps = 12  # Ajusta según necesites el número de pasos para los datos de prueba


# Para la región Norte
#datos_train_norte = promedio_norte_train[:-steps]
#datos_test_norte = promedio_norte_train[-steps:]
datos_train_norte = promedio_norte[:-steps]
datos_test_norte = promedio_norte[-steps:]


# Para la región Centro
#datos_train_centro = promedio_centro_train[:-steps]
#datos_test_centro = promedio_centro_train[-steps:]
datos_train_centro = promedio_centro[:-steps]
datos_test_centro = promedio_centro[-steps:]


# Para la región Sur
#datos_train_sur = promedio_sur_train[:-steps]
#datos_test_sur = promedio_sur_train[-steps:]
datos_train_sur = promedio_sur[:-steps]
datos_test_sur = promedio_sur[-steps:]


# Graficar los datos de entrenamiento y prueba para cada región promedio
fig, axs = plt.subplots(3, 1, figsize=(8, 10), sharex=True)


# Región Norte
axs[0].plot(datos_train_norte.index, datos_train_norte.values, label='train', color='blue')
axs[0].plot(datos_test_norte.index, datos_test_norte.values, label='test', linestyle='--', color='red')
axs[0].set_title('Training and Testing Data - Northern Region',fontsize=18)
axs[0].set_xlabel('Year',fontsize=18)
axs[0].set_ylabel('Accumulated mm',fontsize=18)
axs[0].legend()


# Región Centro
axs[1].plot(datos_train_centro.index, datos_train_centro.values, label='train', color='green')
axs[1].plot(datos_test_centro.index, datos_test_centro.values, label='test', linestyle='--', color='orange')
axs[1].set_title('Training and testing data - Central Region',fontsize=18)
axs[1].set_xlabel('Year',fontsize=18)
axs[1].set_ylabel('Accumulated mm',fontsize=18)
axs[1].legend()

# Región Sur
axs[2].plot(datos_train_sur.index, datos_train_sur.values, label='train', color='purple')
axs[2].plot(datos_test_sur.index, datos_test_sur.values, label='test', linestyle='--', color='#f513bb')
axs[2].set_title('Training and Testing Data - Southern Region',fontsize=18)
axs[2].set_xlabel('Year',fontsize=18)
axs[2].set_ylabel('Accumulated mm',fontsize=18)
axs[2].legend()


# Ajustar diseño y mostrar gráficos
plt.tight_layout()
plt.savefig("Data_test_train.svg", bbox_inches="tight")
plt.show()

"""### ARIMA"""

# Modelos ARIMA para cada región
# Para la región Norte
model_norte = ARIMA(datos_train_norte, order=(0, 0, 1))
result_norte = model_norte.fit()
arima_forecast_norte = result_norte.forecast(steps=len(datos_test_norte))
print("Resumen del modelo para la Región Norte")
print(result_norte.summary())

# Para la región Centro
model_centro = ARIMA(datos_train_centro, order=(0, 0, 1))
result_centro = model_centro.fit()
arima_forecast_centro = result_centro.forecast(steps=len(datos_test_centro))
print("\nResumen del modelo para la Región Centro")
print(result_centro.summary())

# Para la región Sur
model_sur = ARIMA(datos_train_sur, order=(0, 0, 1))
result_sur = model_sur.fit()
arima_forecast_sur = result_sur.forecast(steps=len(datos_test_sur))
print("\nResumen del modelo para la Región Sur")
print(result_sur.summary())

# Función para calcular los errores
def calculate_errors(test, predictions):
    mse = mean_squared_error(test, predictions)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(test, predictions)
    r2 = r2_score(test, predictions)
    mre = np.mean(np.abs((test - predictions) / test)) * 100
    return {'R^2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MRE (%)': mre}

# Calculo de errores
arima_errores_norte = calculate_errors(datos_test_norte, arima_forecast_norte)
arima_errores_centro = calculate_errors(datos_test_centro, arima_forecast_centro)
arima_errores_sur = calculate_errors(datos_test_sur, arima_forecast_sur)

print("ARIMA Errors Norte: \n", arima_errores_norte)
print("ARIMA Errors Centro: \n", arima_errores_centro)
print("ARIMA Errors Sur: \n", arima_errores_sur)

"""### SARIMA"""

# Modelos SARIMA para cada región
# Para la región Norte
model_norte_sa = SARIMAX(datos_train_norte, order=(0, 0, 1), seasonal_order=(0, 1, 1, 12))
result_norte_sa = model_norte_sa.fit(disp=False)
sarima_forecast_norte = result_norte_sa.forecast(steps=len(datos_test_norte))
print("Resumen del modelo para la Región Norte")
print(result_norte_sa.summary())

# Para la región Centro
model_centro_sa = SARIMAX(datos_train_centro, order=(0, 0, 1), seasonal_order=(0, 1, 1, 12))
result_centro_sa = model_centro_sa.fit(disp=False)
sarima_forecast_centro = result_centro_sa.forecast(steps=len(datos_test_centro))
print("\nResumen del modelo para la Región Centro")
print(result_centro_sa.summary())

# Para la región Sur
model_sur_sa = SARIMAX(datos_train_sur, order=(0, 0, 1), seasonal_order=(0, 1, 1, 12))
result_sur_sa = model_sur_sa.fit(disp=False)
sarima_forecast_sur = result_sur_sa.forecast(steps=len(datos_test_sur))
print("\nResumen del modelo para la Región Sur")
print(result_sur_sa.summary())

# Función para calcular los errores
def calculate_errors(test, predictions):
    mse = mean_squared_error(test, predictions)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(test, predictions)
    r2 = r2_score(test, predictions)
    mre = np.mean(np.abs((test - predictions) / test)) * 100
    return {'R^2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MRE (%)': mre}

# Calculo de errores
sarima_errores_norte = calculate_errors(datos_test_norte, sarima_forecast_norte)
sarima_errores_centro = calculate_errors(datos_test_centro, sarima_forecast_centro)
sarima_errores_sur = calculate_errors(datos_test_sur, sarima_forecast_sur)

print("SARIMA Errors Norte: \n", sarima_errores_norte)
print("SARIMA Errors Centro: \n", sarima_errores_centro)
print("SARIMA Errors Sur: \n", sarima_errores_sur)

# Graficar los resultados NORTE
plt.figure(figsize=(10, 6))
plt.plot(datos_train_norte.index, datos_train_norte, label='Entrenamiento')
plt.plot(datos_test_norte.index, datos_test_norte, label='Datos Reales')
plt.plot(datos_test_norte.index, arima_forecast_norte, label='ARIMA Forecast')
plt.plot(datos_test_norte.index, sarima_forecast_norte, label='SARIMA Forecast')
plt.legend(fontsize=10)
plt.title('Forecasting ARIMA vs SARIMA for 2021 - Northern Region',fontsize=18)
plt.xlabel('Year',fontsize=18)
plt.ylabel('Accumulated mm',fontsize=18)
plt.show()

with plt.style.context(['science', 'notebook']):
    # Configuración de la figura
    ancho_columna = 8  # Ancho en pulgadas
    aspect_ratio = 2     # Relación de aspecto
    altura_figura = ancho_columna / aspect_ratio  # Altura de la figura

    # Crear la figura
    plt.figure(figsize=(ancho_columna, altura_figura))

    # Graficar las series de datos
    plt.plot(datos_train_centro.index, datos_train_centro, label='Training', linewidth=2)
    plt.plot(datos_test_centro.index, datos_test_centro, label='Actual Data', linewidth=2)
    plt.plot(datos_test_centro.index, arima_forecast_centro, label='ARIMA Forecast', linestyle='--', linewidth=2)
    plt.plot(datos_test_centro.index, sarima_forecast_centro, label='SARIMA Forecast', linestyle=':', linewidth=2)

    # Configurar el gráfico
    plt.legend(fontsize=10)
    plt.title('Forecasting ARIMA vs SARIMA for 2021 - Central Region', fontsize=18)
    plt.xlabel('Year', fontsize=18)
    plt.ylabel('Accumulated mm', fontsize=18)
    plt.grid(True)

    # Ajustar diseño y guardar el gráfico
    plt.tight_layout()
    plt.savefig("forecasting_SciencePlots.pdf", bbox_inches="tight")
    plt.show()

# Graficar los resultados SUR
plt.figure(figsize=(10, 6))
plt.plot(datos_train_sur.index, datos_train_sur, label='Entrenamiento')
plt.plot(datos_test_sur.index, datos_test_sur, label='Datos Reales')
plt.plot(datos_test_sur.index, arima_forecast_sur, label='ARIMA Forecast')
plt.plot(datos_test_sur.index, sarima_forecast_sur, label='SARIMA Forecast')
plt.legend()
plt.title('Forecasting ARIMA vs SARIMA for 2021 - Southern Region',fontsize=18)
plt.xlabel('Year',fontsize=18)
plt.ylabel('Accumulated mm',fontsize=18)
plt.show()

"""# **EVENTOS EXTREMOS**"""

#Df filtrado de ceros
# Filtrar las filas que no contienen ceros
df_sin_cerosPrueba= data_imputado[(data_imputado != 0.0).all(axis=1)]
df_sin_cerosPrueba

#DF donde los dias sin lluvia son reemplazados por NaN
df_sin_ceros = data_imputado[(data_imputado > 0.0)]
df_sin_ceros

df_EvenExtremosInumet = df_sin_ceros[df_sin_ceros>= 50]

#Se eliminaron las filas completamente llenas de NaN y
#reemplazaron los NaN restantes por -999 si la fila tiene al menos un valor numérico.

# Eliminamos filas completamente llenas de NaN
df_sin_lluvia_sin_nan_filas = df_sin_ceros.dropna(how='all')

# Reemplazamos NaN por -999 en las filas restantes
df_diasSinLLuvia = df_sin_lluvia_sin_nan_filas.fillna(-999)
df_diasSinLLuvia

"""## Eventos extremos inumet:
# https://www.inumet.gub.uy/clima/recursos-hidricos/eventos-extremos-de-precipitacion
"""

# Crear un nuevo DataFrame donde solo se mantengan las precipitaciones mayores a 50mm (Eventos extremos Inumet)

# Filtrar las filas con precipitaciones >= 50 mm
df_EvenExtremos = df_diasSinLLuvia[df_diasSinLLuvia>= 50]
df_EvenExtremos

#Boxplots eventos extremos Inumet
# Convertir el DataFrame de formato ancho a formato largo para facilitar el uso de seaborn
df_melted = df_EvenExtremos.reset_index().melt(id_vars='Fecha', var_name='Estacion', value_name='Precipitacion')

# Eliminar filas con NaN en la columna 'Precipitacion'
df_melted = df_melted.dropna(subset=['Precipitacion'])

# Establecer el estilo de los gráficos
sns.set(style="whitegrid")

# Crear el gráfico de cajas
plt.figure(figsize=(14, 8))
ax = sns.boxplot(x='Estacion', y='Precipitacion', data=df_melted)

# Agregar título y etiquetas
plt.title('Distribución de Eventos Extremos de Lluvia (>= 50 mm)')
plt.xlabel('Estación Meteorológica')
plt.ylabel('Precipitación (mm)')

# Rotar las etiquetas del eje x para mejor legibilidad
plt.xticks(rotation=45)

# Mostrar el gráfico
plt.tight_layout()
plt.show()

df_melted

# Contar el número de eventos extremos (no NaN) por estación
event_counts = df_EvenExtremos.notna().sum()

# Ordenar los eventos de mayor a menor
event_counts = event_counts.sort_values(ascending=False)

# Crear el gráfico de barras
plt.figure(figsize=(14, 8))
sns.barplot(x=event_counts.index, y=event_counts.values, palette="viridis")

# Agregar título y etiquetas
plt.title('Número de Eventos Extremos de Lluvia (>= 50 mm) por Estación')
plt.xlabel('Estación Meteorológica')
plt.ylabel('Número de Eventos Extremos')

# Rotar las etiquetas del eje x para mejor legibilidad
plt.xticks(rotation=45)

# Mostrar el gráfico
plt.tight_layout()
plt.show()

# Estaciones para cada subplot
stations_group1 = ['Artigas_mm', 'Rivera_mm', 'Salto_mm']
stations_group2 = ['Durazno_mm', 'Trinidad_mm','TreintaTres_mm','Melo_mm','Paysandu_mm', 'Young_mm','Mercedes_mm']
stations_group3 = ['Carrasco_mm', 'Colonia_mm','Florida_mm','PuntaEste_mm','Prado_mm','Melilla_mm','Rocha_mm']

# Crear la figura y los subplots
fig, axs = plt.subplots(3, 1, figsize=(14, 18), sharex=True)

# Configuración del primer gráfico
for station in stations_group1:
    axs[0].plot(df_EvenExtremos.index, df_EvenExtremos[station], label=station, marker='o')
axs[0].set_title('Artigas, Rivera, Salto')
axs[0].set_ylabel('Precipitación (mm)')
axs[0].legend()
axs[0].grid(True)

# Configuración del segundo gráfico
for station in stations_group2:
    axs[1].plot(df_EvenExtremos.index, df_EvenExtremos[station], label=station, marker='o')
axs[1].set_title('Durazno, Trinidad, Treinta y Tres, Melo, Paysandú, Young, Mercedes')
axs[1].set_ylabel('Precipitación (mm)')
axs[1].legend()
axs[1].grid(True)

# Configuración del tercer gráfico
for station in stations_group3:
    axs[2].plot(df_EvenExtremos.index, df_EvenExtremos[station], label=station, marker='o')
axs[2].set_title('Carrasco, Colonia, Florida, Punta Este, Prado, Melilla, Rocha')
axs[2].set_xlabel('Fecha')
axs[2].set_ylabel('Precipitación (mm)')
axs[2].legend()
axs[2].grid(True)

# Ajustar el diseño
plt.tight_layout()
plt.show()

g = sns.FacetGrid(df_melted, col="Estacion", col_wrap=4, height=4, aspect=1.5)
g.map(sns.histplot, "Precipitacion", bins=10)
g.set_titles("{col_name}")
g.set_axis_labels("Precipitación (mm)", "Frecuencia")
plt.tight_layout()
plt.show()

"""## Eventos extremos Percentil 90

Cualquier día en el que la precipitación diaria supere el valor del percentil 90 para esa estación específica podría considerarse como un evento extremo de precipitación en esa ubicación.
"""

df_sin_ceros

"""**Interpretación:**
Ej: Artigas: El percentil 90 es 36.37 mm, lo que significa que en el 90 % de los días con lluvia, la precipitación fue menor o igual a 36.37 mm. Si un día llovió más de ese valor, se clasifica como un evento extremo.

"""

# Calcular el percentil 90 para cada columna (cada pluviómetro)
percentiles_90 = df_sin_ceros.quantile(0.90)

# Filtrar eventos extremos para cada pluviómetro
df_eventos_extremos = df_sin_ceros.apply(lambda col: col[col > percentiles_90[col.name]])

# Mostrar los percentiles calculados
print("Percentiles 90 por pluviómetro:")
print(percentiles_90)

# Mostrar los primeros valores de eventos extremos
print(df_eventos_extremos.head())

df_eventos_extremos

def filtrar_eventos_extremos(df_eventos, estacion):
    """
    Filtra los eventos extremos de una estación específica y devuelve un DataFrame con sus datos.

    Parámetros:
        df_eventos (pd.DataFrame): DataFrame con los eventos extremos de todas las estaciones.
        estacion (str): Nombre de la estación (debe coincidir con una de las columnas del DataFrame).

    Retorna:
        pd.DataFrame: DataFrame con las fechas y los eventos extremos de la estación seleccionada.
    """
    if estacion not in df_eventos.columns:
        raise ValueError(f"La estación '{estacion}' no se encuentra en el DataFrame.")

    # Filtrar la estación seleccionada y eliminar filas con NaN
    df_filtrado = df_eventos[[estacion]].dropna()

    # Renombrar la columna para facilitar la interpretación
    df_filtrado = df_filtrado.rename(columns={estacion: "Precipitacion_mm"})

    return df_filtrado

df_artigas90 = filtrar_eventos_extremos(df_eventos_extremos, "Artigas_mm")
df_carrasco90 = filtrar_eventos_extremos(df_eventos_extremos, "Carrasco_mm")
df_melo90 = filtrar_eventos_extremos(df_eventos_extremos, "Melo_mm")
df_colonia90 = filtrar_eventos_extremos(df_eventos_extremos, "Colonia_mm")
df_durazno90 = filtrar_eventos_extremos(df_eventos_extremos, "Durazno_mm")
df_trinidad90 = filtrar_eventos_extremos(df_eventos_extremos, "Trinidad_mm")
df_florida90 = filtrar_eventos_extremos(df_eventos_extremos, "Florida_mm")
df_puntaeste90 = filtrar_eventos_extremos(df_eventos_extremos, "PuntaEste_mm")
df_prado90 = filtrar_eventos_extremos(df_eventos_extremos, "Prado_mm")
df_melilla90 = filtrar_eventos_extremos(df_eventos_extremos, "Melilla_mm")
df_paysandu90 = filtrar_eventos_extremos(df_eventos_extremos, "Paysandu_mm")
df_rivera90 = filtrar_eventos_extremos(df_eventos_extremos, "Rivera_mm")
df_rocha90 = filtrar_eventos_extremos(df_eventos_extremos, "Rocha_mm")
df_salto90 = filtrar_eventos_extremos(df_eventos_extremos, "Salto_mm")
df_mercedes90 = filtrar_eventos_extremos(df_eventos_extremos, "Mercedes_mm")
df_treintatres90 = filtrar_eventos_extremos(df_eventos_extremos, "TreintaTres_mm")
df_young90 = filtrar_eventos_extremos(df_eventos_extremos, "Young_mm")

df_artigas90

# Convertir el DataFrame a formato "largo" para facilitar la visualización
df_melted = df_eventos_extremos.melt(var_name="Pluviómetro", value_name="Precipitación")

# Crear una figura
plt.figure(figsize=(12, 6))

# Graficar boxplot
df_melted.boxplot(by="Pluviómetro", column="Precipitación", rot=90, grid=True, figsize=(12, 6))

# Configuración del gráfico
plt.xlabel("Pluviómetro")
plt.ylabel("Precipitación (mm)")
plt.title("Distribución de Eventos Extremos de Precipitación por Pluviómetro")
plt.suptitle("")  # Eliminar título automático del boxplot
plt.grid(True)

# Mostrar el gráfico
plt.show()

def graficar_eventos_extremos(df_eventos, estaciones, titulo="Eventos Extremos de Precipitación"):
    """
    Genera subplots con los eventos extremos de precipitación para cada estación individualmente.
    Incluye la media y la desviación estándar de la precipitación para cada estación.

    Parámetros:
        df_eventos (pd.DataFrame): DataFrame con eventos extremos de las estaciones.
        estaciones (list): Lista con los nombres de las estaciones a graficar.
        titulo (str): Título general de la figura.

    Retorna:
        None (muestra el gráfico).
    """
    num_estaciones = len(estaciones)
    fig, axs = plt.subplots(num_estaciones, 1, figsize=(10, 3 * num_estaciones), sharex=True)

    if num_estaciones == 1:  # Si hay solo una estación, axs no es una lista
        axs = [axs]

    color_eventos = "#4D4D4D"  # Un gris oscuro para los eventos extremos  # Paleta de colores

    for i, (ax, estacion) in enumerate(zip(axs, estaciones)):
        # Extraer datos de la estación
        datos = df_eventos[estacion].dropna()
        fechas = datos.index
        valores = datos.values

        # Calcular estadísticas
        media = datos.mean()
        std = datos.std()

        # Graficar eventos extremos
        ax.plot(fechas, valores, color=color_eventos, label=f"{estacion}", marker="o", markersize=4, linestyle="None")

        # Graficar la media y desviación estándar
        ax.axhline(media, color="black", linestyle="--", linewidth=2, label="Media")
        ax.fill_between(fechas, media - std, media + std, color="#00CED1", alpha=0.2, label="±1 Desv. Est.")

        # Configuración del subplot
        ax.set_ylabel("Precipitación (mm)")
        ax.set_title(f"Eventos extremos en {estacion}")
        ax.legend()
        ax.grid(True)

    axs[-1].set_xlabel("Fecha")  # Etiqueta de eje X en el último subplot
    fig.suptitle(titulo, fontsize=14)
    plt.tight_layout()
    plt.show()

# 📌 Uso de la función con todas las estaciones
estaciones_lista = [
    "Artigas_mm", "Carrasco_mm", "Melo_mm", "Colonia_mm", "Durazno_mm",
    "Trinidad_mm", "Florida_mm", "PuntaEste_mm", "Prado_mm", "Melilla_mm",
    "Paysandu_mm", "Rivera_mm", "Rocha_mm", "Salto_mm", "Mercedes_mm",
    "TreintaTres_mm", "Young_mm"
]

graficar_eventos_extremos(df_eventos_extremos, estaciones_lista)

# Estadísticos básicos para cada pluviómetro
estadisticosNoventa = df_eventos_extremos.describe().T  # describe() devuelve estadísticos como media, std, min, 25%, 50%, 75%, max
# Mostrar el DataFrame con los estadísticos
estadisticosNoventa

"""## Eventos extremos Percentil 95"""

# Calcular el percentil 95 para cada columna (cada pluviómetro)
percentiles_95 = df_sin_ceros.quantile(0.95)

# Filtrar eventos extremos para cada pluviómetro
df_eventos_extremos_NueveCinco = df_sin_ceros.apply(lambda col: col[col > percentiles_95[col.name]])

# Mostrar los percentiles calculados
print("Percentiles 95 por pluviómetro:")
print(percentiles_95)

# Mostrar los primeros valores de eventos extremos
print(df_eventos_extremos_NueveCinco.head())

df_eventos_extremos_NueveCinco

df_artigas95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Artigas_mm")
df_carrasco95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Carrasco_mm")
df_melo95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Melo_mm")
df_colonia95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Colonia_mm")
df_durazno95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Durazno_mm")
df_trinidad95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Trinidad_mm")
df_florida95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Florida_mm")
df_puntaeste95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "PuntaEste_mm")
df_prado95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Prado_mm")
df_melilla95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Melilla_mm")
df_paysandu95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Paysandu_mm")
df_rivera95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Rivera_mm")
df_rocha95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Rocha_mm")
df_salto95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Salto_mm")
df_mercedes95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Mercedes_mm")
df_treintatres95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "TreintaTres_mm")
df_young95 = filtrar_eventos_extremos(df_eventos_extremos_NueveCinco, "Young_mm")

#Recorte de datos
df_mercedes95_a=df_mercedes95.loc['1990-01-01':'1998-12-31']
df_mercedes95_a

# Asegurar que el índice es de tipo datetime
df_salto95.index = pd.to_datetime(df_salto95.index)

# Filtrar por enero de 2001
df_filtrado = df_salto95.loc[(df_salto95.index.year == 1991) & (df_salto95.index.month == 8)]

# Obtener el valor máximo de precipitación
max_precip = df_filtrado.max()

# Obtener la fecha en que ocurrió el máximo
fecha_max_precip = df_filtrado.idxmax()

print(f"Máximo de precipitación: {max_precip} mm")
print(f"Ocurrió el día: {fecha_max_precip}")

df_artigas95

# Convertir el DataFrame a formato "largo" para facilitar la visualización
df_melted = df_eventos_extremos_NueveCinco.melt(var_name="Pluviómetro", value_name="Precipitación")

# Crear una figura
plt.figure(figsize=(12, 6))

# Graficar boxplot
df_melted.boxplot(by="Pluviómetro", column="Precipitación", rot=90, grid=True, figsize=(12, 6))

# Configuración del gráfico
plt.xlabel("Pluviómetro")
plt.ylabel("Precipitación (mm)")
plt.title("Distribución de Eventos Extremos de Precipitación por Pluviómetro (Percentil 95)")
plt.suptitle("")  # Eliminar título automático del boxplot
plt.grid(True)

# Mostrar el gráfico
plt.show()

graficar_eventos_extremos(df_eventos_extremos_NueveCinco, estaciones_lista)

# Estadísticos básicos para cada pluviómetro
estadisticosNovenCinco = df_eventos_extremos_NueveCinco.describe().T  # describe() devuelve estadísticos como media, std, min, 25%, 50%, 75%, max
# Mostrar el DataFrame con los estadísticos
estadisticosNovenCinco

# Asegurarse de que el índice sea de tipo datetime
df_eventos_extremos_NueveCinco.index = pd.to_datetime(df_eventos_extremos_NueveCinco.index)

# Agrupar por año y calcular el valor máximo de cada estación
maximos_anuales_95 = df_eventos_extremos_NueveCinco.groupby(df_eventos_extremos_NueveCinco.index.year).max()

# Opcional: renombrar el índice a "Año"
maximos_anuales_95.index.name = 'Año'

# Mostrar el resultado
maximos_anuales_95

# Asegurar que el índice es de tipo datetime
df_salto95.index = pd.to_datetime(df_salto95.index)

# Filtrar solo por el año 1991
df_filtrado = df_salto95[df_salto95.index.year == 2005]

# Obtener el valor máximo de precipitación
max_precip = df_filtrado.max()

# Obtener la fecha en que ocurrió el máximo
fecha_max_precip = pd.to_datetime(df_filtrado.idxmax().values[0])

print(f"Máximo de precipitación en 1991: {max_precip.values[0]} mm")
print(f"Ocurrió el día: {fecha_max_precip.date()}")

# --- 1. Configuración de la figura -------------------------------------------------
fig, axs = plt.subplots(3, 1, figsize=(10, 12), sharex=True, sharey=True)

# Límites fijos del eje Y solicitados
y_min = 45
y_max = 280

# --- 2. Función auxiliar para cada región ------------------------------------------
def plot_region(ax, estaciones, titulo):
    for est in estaciones:
        ax.plot(maximos_anuales_95.index, maximos_anuales_95[est],
                marker='o', label=est, linewidth=1)

    datos_region = maximos_anuales_95[estaciones]
    media_anual = datos_region.mean(axis=1)
    std_anual = datos_region.std(axis=1)

    ax.fill_between(maximos_anuales_95.index,
                    media_anual - std_anual,
                    media_anual + std_anual,
                    color='#75e0ff', alpha=0.3,
                    label='±1 σ' if titulo == 'Región Norte' else None)

    ax.plot(maximos_anuales_95.index, media_anual,
            color='black', linestyle=':', linewidth=1.6,
            label='Media región' if titulo == 'Región Norte' else None)

    ax.set_title(titulo)
    ax.set_ylabel('Precipitación (mm)')
    ax.set_ylim(y_min, y_max)
    ax.grid(True, linestyle='--', alpha=0.5)
    ax.legend(ncol=2, fontsize=8)

# --- 3. Dibujar cada región ---------------------------------------------------------
plot_region(axs[0], estaciones_norte,  'Región Norte')
plot_region(axs[1], estaciones_centro, 'Región Centro')
plot_region(axs[2], estaciones_sur,    'Región Sur')

axs[2].set_xlabel('Año')

# --- 4. Título general de la figura -------------------------------------------------
fig.suptitle("Eventos extremos máximos por año (Percentil 95)", fontsize=12, y=1)

plt.tight_layout()
plt.savefig("eventos_extremos.pdf", dpi=300)
plt.show()

"""## Eventos extremos Percentil 99"""

# Calcular el percentil 99 para cada columna (cada pluviómetro)
percentiles_99 = df_sin_ceros.quantile(0.99)

# Filtrar eventos extremos para cada pluviómetro
df_eventos_extremos_NueveNueve = df_sin_ceros.apply(lambda col: col[col > percentiles_99[col.name]])

# Mostrar los percentiles calculados
print("Percentiles 99 por pluviómetro:")
print(percentiles_99)

# Mostrar los primeros valores de eventos extremos
print(df_eventos_extremos_NueveNueve.head())

# Convertir el DataFrame a formato "largo" para facilitar la visualización
df_melted = df_eventos_extremos_NueveNueve.melt(var_name="Pluviómetro", value_name="Precipitación")

# Crear una figura
plt.figure(figsize=(12, 6))

# Graficar boxplot
df_melted.boxplot(by="Pluviómetro", column="Precipitación", rot=90, grid=True, figsize=(12, 6))

# Configuración del gráfico
plt.xlabel("Pluviómetro")
plt.ylabel("Precipitación (mm)")
plt.title("Distribución de Eventos Extremos de Precipitación por Pluviómetro (Percentil 99)")
plt.suptitle("")  # Eliminar título automático del boxplot
plt.grid(True)

# Mostrar el gráfico
plt.show()

graficar_eventos_extremos(df_eventos_extremos_NueveNueve, estaciones_lista)

# Estadísticos básicos para cada pluviómetro
estadisticosNueveNueve = df_eventos_extremos_NueveNueve.describe().T  # describe() devuelve estadísticos como media, std, min, 25%, 50%, 75%, max
# Mostrar el DataFrame con los estadísticos
estadisticosNueveNueve

"""## Selección de eventos extremos"""

# === Datos de Mercedes para cada percentil ===
fechas_90 = df_eventos_extremos["Mercedes_mm"].dropna().index
valores_90 = df_eventos_extremos["Mercedes_mm"].dropna().values

fechas_95 = df_eventos_extremos_NueveCinco["Mercedes_mm"].dropna().index
valores_95 = df_eventos_extremos_NueveCinco["Mercedes_mm"].dropna().values

fechas_99 = df_eventos_extremos_NueveNueve["Mercedes_mm"].dropna().index
valores_99 = df_eventos_extremos_NueveNueve["Mercedes_mm"].dropna().values

# =====================================================
#  Gráfico 1: Todos los percentiles en un solo gráfico
# =====================================================
plt.figure(figsize=(12, 6))

# Percentil 90
plt.scatter(fechas_90, valores_90, marker='o', label='Percentil 90',
            color='#1f77b4', s=100, edgecolor='black', alpha=0.7)

# Percentil 95
plt.scatter(fechas_95, valores_95, marker='s', label='Percentil 95',
            color='#65ff18', s=100, edgecolor='black', alpha=1)

# Percentil 99
plt.scatter(fechas_99, valores_99, marker='^', label='Percentil 99',
            color='red', s=100, edgecolor='black', alpha=0.7)

plt.title("Eventos extremos en Mercedes (Percentiles 90, 95 y 99)", fontsize=14)
plt.ylabel("Precipitación (mm)", fontsize=12)
plt.xlabel("Fecha", fontsize=12)
plt.legend(frameon=True, framealpha=0.9)
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.savefig("eventos_extremos_mercedes_todos_juntos.svg", dpi=300)
plt.show()

# =====================================================
# Gráfico 2: Subplots separados por percentil
# =====================================================
dfs = [valores_90, valores_95, valores_99]
fechas = [fechas_90, fechas_95, fechas_99]
labels = ["Percentil 90", "Percentil 95", "Percentil 99"]
colors = ['#4D4D4D'] * 3

fig, axs = plt.subplots(3, 1, figsize=(12, 9), sharex=True)
color_rango = "#00CED1"

for i in range(3):
    datos = dfs[i]
    fechas_i = fechas[i]
    media = datos.mean()
    std = datos.std()

    axs[i].plot(fechas_i, datos, color=colors[i], label=labels[i], marker='o', markersize=4, linestyle="None")
    axs[i].axhline(media, color="black", linestyle="--", linewidth=2, label="Media")
    axs[i].fill_between(fechas_i, media - std, media + std, color=color_rango, alpha=0.2, label="±1 Desv. Est.")
    axs[i].set_ylabel("Precipitación (mm)")
    axs[i].set_title(f"{labels[i]} - Mercedes")
    axs[i].grid(True)
    axs[i].legend()

axs[-1].set_xlabel("Fecha")
fig.suptitle("Eventos extremos en Mercedes - Percentiles 90, 95 y 99", fontsize=14)
plt.tight_layout()
plt.savefig("eventos_extremos_mercedes_subplots.png", dpi=300)
plt.show()

"""## **Periodo de retorno**"""

from scipy import stats
tabulaciones = np.arange(-10,100,0.1)
distnormal = stats.norm.pdf(tabulaciones,
                            loc=estadistico_diario.iloc[1][0], scale=estadistico_diario.iloc[2][0])
distlognormal = stats.pearson3.pdf(tabulaciones,skew=1,
                                   loc= estadistico_diario.iloc[1][0], scale=estadistico_diario.iloc[2][0])
distweibull = stats.dweibull.pdf(tabulaciones,c=1,
                                   loc= estadistico_diario.iloc[1][0], scale=estadistico_diario.iloc[2][0])
distchi2 = stats.chi2.pdf(tabulaciones,df=2,
                                   loc= estadistico_diario.iloc[1][0], scale=estadistico_diario.iloc[2][0])
distgumbel = stats.gumbel_r.pdf(tabulaciones,
                                loc=estadistico_diario.iloc[1][0],
                                scale=estadistico_diario.iloc[2][0])

data_preci_inumet.columns

def plot_distributions(data_col, estadistico_diario, nombre_estacion,guardar=False):
    """
    Plotea las distribuciones ajustadas a una serie de precipitaciones diarias.

    Parámetros:
    - data_col: Serie de pandas con las precipitaciones (ej. data["Mercedes_mm"])
    - estadistico_diario: DataFrame con estadísticas. Se espera:
        [1][0] = media, [2][0] = desviación estándar
    - nombre_estacion: Nombre de la estación para el título del gráfico
    """
    # Convertir a valores numéricos y filtrar positivos
    data_col = pd.to_numeric(data_col, errors='coerce')
    datos_filtrados = data_col[data_col > 0]

    # Tabulación para calcular PDFs
    tabulaciones = np.arange(-10, 100, 0.1)
    media = estadistico_diario.iloc[1][0]
    std = estadistico_diario.iloc[2][0]

    # Cálculo de distribuciones
    dist_normal = stats.norm.pdf(tabulaciones, loc=media, scale=std)
    dist_lognormal = stats.pearson3.pdf(tabulaciones, skew=1, loc=media, scale=std)
    dist_weibull = stats.dweibull.pdf(tabulaciones, c=1, loc=media, scale=std)
    dist_chi2 = stats.chi2.pdf(tabulaciones, df=2, loc=media, scale=std)
    dist_gumbel = stats.gumbel_r.pdf(tabulaciones, loc=media, scale=std)

    # Gráfico
    plt.figure(figsize=(10, 5))
    datos_filtrados.hist(bins=50, edgecolor='#4aaaaa', color='#80BCD8', density=True)

    plt.plot(tabulaciones, dist_normal, color='orange', linestyle='--', linewidth=2, label='Normal')
    plt.plot(tabulaciones, dist_lognormal, color='lightcoral', linestyle='--', linewidth=2, label='Lognormal (Pearson3)')
    plt.plot(tabulaciones, dist_weibull, color='#7B7C7E', linestyle='-.', linewidth=2, label='Weibull')
    plt.plot(tabulaciones, dist_chi2, color='#3F83B7', linestyle=':', linewidth=2, label='Chi2')
    plt.plot(tabulaciones, dist_gumbel, color='purple', linestyle='-', linewidth=2, label='Gumbel')

    plt.title(f'Distribuciones ajustadas - Estación {nombre_estacion}')
    plt.xlabel('Precipitación (mm)')
    plt.ylabel('Densidad')
    plt.xlim(-10, 100)
    plt.legend(loc='upper right')
    plt.grid(True)
    plt.tight_layout()

    # Guardar si se indica
    if guardar:
        nombre_archivo = f"distribuciones_{nombre_estacion}.svg"
        plt.savefig(nombre_archivo, format='pdf')
        print(f"Gráfico guardado como {nombre_archivo}")

    plt.show()

plot_distributions(data_preci_inumet["Mercedes_mm"], estadistico_diario, "Mercedes",guardar=True)

plot_distributions(data_preci_inumet["Artigas_mm"], estadistico_diario, "Artigas",guardar=True)

plot_distributions(data_preci_inumet["Prado_mm"], estadistico_diario, "Prado",guardar=True)

plot_distributions(data_preci_inumet["Durazno_mm"], estadistico_diario, "Durazno")

from scipy.stats import norm, lognorm, weibull_min, chi2, gumbel_r

def plot_distributions_extremos95(serie_eventos_extremos, nombre_estacion,guardar=False):
    """
    Plotea el histograma de eventos extremos (percentil 95) y ajusta varias distribuciones teóricas.

    Parámetros:
    - serie_eventos_extremos: Serie de pandas con los valores de precipitación de eventos extremos (P95)
    - nombre_estacion: string con el nombre de la estación (ej. 'Mercedes')
    """
    # --- 1. Filtrar y preparar datos ---
    datos = pd.to_numeric(serie_eventos_extremos, errors='coerce').dropna()
    datos = datos[datos > 0]  # asegurarse de eliminar ceros o negativos si los hubiera

    x = np.linspace(datos.min(), datos.max(), 500)

    # --- 2. Ajuste de distribuciones ---
    param_norm = norm.fit(datos)
    distnormal = norm.pdf(x, *param_norm)

    param_lognorm = lognorm.fit(datos, floc=0)
    distlognormal = lognorm.pdf(x, *param_lognorm)

    param_weibull = weibull_min.fit(datos, floc=0)
    distweibull = weibull_min.pdf(x, *param_weibull)

    param_chi2 = chi2.fit(datos, floc=0)
    distchi2 = chi2.pdf(x, *param_chi2)

    param_gumbel = gumbel_r.fit(datos)
    distgumbel = gumbel_r.pdf(x, *param_gumbel)

    # --- 3. Graficar ---
    plt.figure(figsize=(10, 5))
    datos.hist(bins=30, edgecolor='#4aaaaa', color='#80BCD8', density=True, label='Datos observados')

    plt.plot(x, distnormal, color='orange', linewidth=2, linestyle='--', label='Dist Normal')
    plt.plot(x, distlognormal, color='lightcoral', linewidth=2, linestyle='--', label='Dist Lognormal')
    plt.plot(x, distweibull, color='#7B7C7E', linewidth=2, linestyle='-.', label='Dist Weibull')
    plt.plot(x, distchi2, color='#3F83B7', linewidth=2, linestyle=':', label='Dist Chi2')
    plt.plot(x, distgumbel, color='purple', linewidth=2, linestyle='-', label='Dist Gumbel')

    plt.xlim(0, datos.max() + 10)
    plt.title(f'Ajuste de distribuciones a eventos extremos ({nombre_estacion} - P95)')
    plt.xlabel('Precipitación (mm)')
    plt.ylabel('Densidad')
    plt.legend(loc='upper right')
    plt.grid(alpha=0.3)
    plt.tight_layout()
     # Guardar si se indica
    if guardar:
        nombre_archivo = f"distribucionesMaximos_{nombre_estacion}.svg"
        plt.savefig(nombre_archivo, format='svg')
        print(f"Gráfico guardado como {nombre_archivo}")
    plt.show()

plot_distributions_extremos95(df_eventos_extremos_NueveCinco["Young_mm"], "Young",guardar=True)

plot_distributions_extremos95(df_eventos_extremos_NueveCinco["Mercedes_mm"], "Mercedes",guardar=True)

plot_distributions_extremos95(df_eventos_extremos_NueveCinco["Artigas_mm"], "Artigas",guardar=True)

plot_distributions_extremos95(df_eventos_extremos_NueveCinco["Prado_mm"], "Prado",guardar=True)

plot_distributions_extremos95(df_eventos_extremos_NueveCinco["Durazno_mm"], "Durazno")

"""✅ Selecciona los máximos anuales de cada estación.

✅ Ordena los valores y calcula la probabilidad de excedencia (método de Gringorten).

✅ Calcula el periodo de retorno a partir de la probabilidad.

✅ Ajusta los datos usando la distribución de Gumbel.

✅ Consolida todos los resultados en un solo DataFrame.
"""

from scipy import stats
import numpy as np
import pandas as pd

estaciones = ['Artigas_mm','Carrasco_mm','Melo_mm','Colonia_mm','Durazno_mm',
              'Trinidad_mm','Florida_mm','PuntaEste_mm','Prado_mm','Melilla_mm',
              'Paysandu_mm','Rivera_mm','Young_mm','Rocha_mm','Salto_mm',
              'Mercedes_mm','TreintaTres_mm']

resultados = {}
parametros = []  # resumen por estación

for estacion in estaciones:
    datos = pd.to_numeric(data_imputado[estacion], errors='coerce').dropna()

    # Máximos anuales (si usás año hidrológico, cambia 'Y' por 'A-JUN' o el que corresponda)
    maximos_anuales = datos.resample('Y').max().dropna()

    # Ordenar de mayor a menor
    x = maximos_anuales.sort_values(ascending=False).values
    n = len(x)
    i = np.arange(1, n+1)                 # 1 = mayor, n = menor
    q_exc = (i - 0.44) / (n + 0.12)       # Gringorten (excedencia)
    T_emp = 1.0 / q_exc                   # período de retorno empírico

    # Ajuste Gumbel (MLE) — parámetros usados
    loc, scale = stats.gumbel_r.fit(x)    # μ, β
    F = stats.gumbel_r.cdf(x, loc=loc, scale=scale)
    T_gumbel = 1.0 / (1.0 - F)            # período de retorno paramétrico (modelo)

    # Curva teórica (mismo q_empírico)
    x_teo = stats.gumbel_r.ppf(1 - q_exc, loc=loc, scale=scale)

    df = pd.DataFrame({
        'Precipitación (mm)': x,
        'Probabilidad excedencia (emp.)': q_exc,
        'Periodo retorno (emp., años)': T_emp,
        'Periodo retorno (Gumbel, años)': T_gumbel,
        'Ajuste Gumbel (mm)': x_teo
    })
    resultados[estacion] = df

    parametros.append({'estacion': estacion, 'mu_loc': loc, 'beta_scale': scale, 'n_anios': n})

# DataFrame consolidado (MultiIndex) con todas las estaciones
df_resultados = pd.concat(resultados, names=['Estacion', 'Ranking'])

# Resumen de parámetros por estación
df_parametros = pd.DataFrame(parametros).sort_values('estacion')

df_resultados

df_parametros

# Lista de estaciones
estaciones = ['Artigas_mm', 'Carrasco_mm', 'Melo_mm', 'Colonia_mm', 'Durazno_mm',
              'Trinidad_mm', 'Florida_mm', 'PuntaEste_mm', 'Prado_mm', 'Melilla_mm',
              'Paysandu_mm', 'Rivera_mm', 'Young_mm', 'Rocha_mm', 'Salto_mm',
              'Mercedes_mm', 'TreintaTres_mm']

# Diccionario para almacenar resultados
resultados = {}

for estacion in estaciones:
    # Seleccionar los datos de la estación y eliminar valores nulos
    datos = data_imputado[estacion].dropna()

    # Obtener el valor máximo de precipitación de cada año
    maximos_anuales = datos.resample('Y').max().dropna()

    # Ordenar de mayor a menor
    serie_ordenada = maximos_anuales.sort_values(ascending=False).reset_index(drop=True)

    # Número total de datos
    n = len(serie_ordenada)

    # Probabilidad de excedencia empírica usando el método de Gringorten
    rank = serie_ordenada.rank(method='max', ascending=False)
    prob_excedencia = (rank - 0.44) / (n + 0.12)

    # Periodo de retorno (T)
    periodo_retorno = 1 / prob_excedencia

    # Ajuste de la distribución de Gumbel
    loc, scale = stats.gumbel_r.fit(serie_ordenada)

    # Crear DataFrame con resultados
    resultados[estacion] = pd.DataFrame({
        'Precipitación (mm)': serie_ordenada.values,
        'Probabilidad de excedencia': prob_excedencia.values,
        'Periodo de retorno (años)': periodo_retorno.values,
        'Ajuste Gumbel': stats.gumbel_r.ppf(1 - prob_excedencia, loc=loc, scale=scale)
    })

# Convertir el diccionario de resultados en un DataFrame consolidado
df_resultados = pd.concat(resultados, names=['Estación', 'Ranking'])

# Mostrar las primeras filas como ejemplo
df_resultados

"""**PERIODO DE RETORNO PARA LOS MAXIMOS POR AÑO DEL PERCENTIL 95**"""

maximos_anuales_95

from scipy.stats import gumbel_r

# Diccionario para guardar los resultados
retornos_gumbel = {}

for estacion in maximos_anuales_95.columns:
    # Extraer la serie de máximos anuales para la estación (sin NaNs)
    serie = maximos_anuales_95[estacion].dropna().sort_values(ascending=False).reset_index(drop=True)
    n = len(serie)

    # Cálculo de probabilidad de excedencia con método de Gringorten
    rank = serie.rank(method='max', ascending=False)
    prob_excedencia = (rank - 0.44) / (n + 0.12)
    periodo_retorno_emp = 1 / prob_excedencia

    # Ajustar distribución Gumbel
    loc, scale = gumbel_r.fit(serie)

    # Calcular valor teórico Gumbel para cada probabilidad
    valor_gumbel = gumbel_r.ppf(1 - prob_excedencia, loc=loc, scale=scale)

    # Crear DataFrame de resultados
    df_resultado = pd.DataFrame({
        'Precipitación (mm)': serie.values,
        'Probabilidad de excedencia': prob_excedencia.values,
        'Periodo de retorno empírico (años)': periodo_retorno_emp.values,
        'Valor estimado Gumbel (mm)': valor_gumbel
    })

    retornos_gumbel[estacion] = df_resultado

# Ejemplo: mostrar resultados de una estación específica
retornos_gumbel['Artigas_mm']

retornos_gumbel['Mercedes_mm']

# Crear lista para almacenar filas del resumen
resumen_eventos = []

for estacion, df in retornos_gumbel.items():
    # Ordenar por precipitación (menor a mayor)
    df_ordenado = df.sort_values('Precipitación (mm)').reset_index(drop=True)

    # Evento mínimo, mediano y máximo
    evento_min = df_ordenado.iloc[0]
    evento_med = df_ordenado.iloc[len(df_ordenado) // 2]
    evento_max = df_ordenado.iloc[-1]

    # Agregar al resumen
    resumen_eventos.extend([
        {'Estación': estacion, 'Tipo de evento': 'Mínimo', **evento_min.to_dict()},
        {'Estación': estacion, 'Tipo de evento': 'Mediano', **evento_med.to_dict()},
        {'Estación': estacion, 'Tipo de evento': 'Máximo', **evento_max.to_dict()},
    ])

# Convertir a DataFrame final
df_resumen = pd.DataFrame(resumen_eventos)

# Mostrar primeras filas por consola
print(df_resumen.head(10))

# Guardar como archivo Excel
df_resumen.to_excel("resumen_eventos_extremos.xlsx", index=False)

df_resumen

df_resumen["Error_abs"] = (df_resumen["Precipitación (mm)"] - df_resumen["Valor estimado Gumbel (mm)"]).abs()

# También podés calcular el error relativo (%)
df_resumen["Error_rel_%"] = (df_resumen["Error_abs"] / df_resumen["Precipitación (mm)"]) * 100

# Mostrar primeras filas
df_resumen

df_resumen_str = df_resumen.copy()

# Formatear floats a string con 3 decimales, usando coma decimal
df_resumen_str = df_resumen_str.applymap(
    lambda x: f"{x:,.3f}".replace(",", "X").replace(".", ",").replace("X", ".") if isinstance(x, float) else x
)

df_resumen_str

# Periodo de retorno de interés
T = 10

# Recopilar valores para cada estación
estaciones = []
valores_T10 = []

for est, df in retornos_gumbel.items():
    idx = (np.abs(df['Periodo de retorno empírico (años)'] - T)).idxmin()
    estaciones.append(est)
    valores_T10.append(df.loc[idx, 'Valor estimado Gumbel (mm)'])

# Ordenar de menor a mayor el valor de precipitación
orden = np.argsort(valores_T10)
estaciones_ordenadas = [estaciones[i] for i in orden]
valores_ordenados = [valores_T10[i] for i in orden]

# Gráfico de barras
plt.figure(figsize=(12, 6))
plt.bar(estaciones_ordenadas, valores_ordenados, color='skyblue')
plt.xticks(rotation=90, fontsize=14)  # tamaño de letra de los nombres de estaciones
plt.yticks(fontsize=14)               # tamaño de letra en eje Y
plt.ylabel('Precipitación estimada (mm)', fontsize=16)
plt.title(f'Precipitación estimada para T = {T} años (de menor a mayor)', fontsize=16)
plt.grid(axis='y')
plt.tight_layout()
# Guardar como PDF
plt.savefig("precipitacion_T10.pdf", format='pdf')
plt.show()

import matplotlib.ticker as mticker
def plot_curva_periodo_retorno_con_margen(retornos_gumbel, estacion,
                                           margen_relativo=0.1,
                                           guardar_pdf=False,
                                           nombre_archivo=None):
    """
    Grafica la curva de período de retorno (empírica + ajuste Gumbel)
    con banda de ajuste y eje X limitado a [1, 60] años en escala log.
    """
    if estacion not in retornos_gumbel:
        print(f"⚠️ La estación '{estacion}' no se encuentra en el diccionario.")
        return

    df = retornos_gumbel[estacion].copy()

    # Crear intervalo de confianza si no existe
    if 'IC superior (mm)' not in df.columns or 'IC inferior (mm)' not in df.columns:
        df['IC superior (mm)'] = df['Valor estimado Gumbel (mm)'] * (1 + margen_relativo)
        df['IC inferior (mm)'] = df['Valor estimado Gumbel (mm)'] * (1 - margen_relativo)

    # Filtrar solo periodos entre 1 y 60 años
    df = df[(df['Periodo de retorno empírico (años)'] >= 1) &
            (df['Periodo de retorno empírico (años)'] <= 60)]

    fig, ax = plt.subplots(figsize=(8, 5))

    # Datos empíricos (ahora con línea sólida que conecta los puntos)
    ax.plot(df['Periodo de retorno empírico (años)'],
            df['Precipitación (mm)'],
            marker='o', linestyle='-',  # <-- aquí cambiamos a línea
            label='Datos empíricos',
            markersize=6, color='C0', linewidth=1)

    # Ajuste de Gumbel
    ax.plot(df['Periodo de retorno empírico (años)'],
            df['Valor estimado Gumbel (mm)'],
            linestyle='--', color='C1',
            label='Ajuste Gumbel', linewidth=2)

    # Banda de confianza
    ax.fill_between(df['Periodo de retorno empírico (años)'],
                    df['IC inferior (mm)'],
                    df['IC superior (mm)'],
                    color='C1', alpha=0.3,
                    label=f'Margen ±{int(margen_relativo*100)}%')

    # Escala y límites
    ax.set_xscale('log')
    ax.set_xlim(1, 60)

    # Ticks personalizados en log
    ticks = [1, 2, 5, 10, 20, 50]
    ax.set_xticks(ticks)
    ax.get_xaxis().set_major_formatter(mticker.ScalarFormatter())
    ax.tick_params(axis='x', which='minor', bottom=False)

    # Etiquetas y estilo
    ax.set_xlabel('Período de retorno (años)', fontsize=14)
    ax.set_ylabel('Precipitación (mm)', fontsize=14)
    ax.set_title(f'Curva de período de retorno - {estacion}', fontsize=16)
    ax.grid(True, which='both', ls='--', lw=0.5, alpha=0.7)
    ax.legend(loc='upper left', framealpha=0.9)

    plt.tight_layout()

    if guardar_pdf:
        if not nombre_archivo:
            nombre_archivo = f"curva_periodo_retorno_{estacion}.svg"
        fig.savefig(nombre_archivo, format='svg')
        print(f"✅ Gráfico guardado como '{nombre_archivo}'")

    plt.show()

plot_curva_periodo_retorno_con_margen(retornos_gumbel, 'Artigas_mm',guardar_pdf=True)

plot_curva_periodo_retorno_con_margen(retornos_gumbel, 'Young_mm',guardar_pdf=True)

plot_curva_periodo_retorno_con_margen(retornos_gumbel, 'Mercedes_mm',guardar_pdf=True)

plot_curva_periodo_retorno_con_margen(retornos_gumbel, 'Prado_mm',guardar_pdf=True)

plot_curva_periodo_retorno_con_margen(retornos_gumbel, 'Rivera_mm',guardar_pdf=True)

plot_curva_periodo_retorno_con_margen(retornos_gumbel, 'Paysandu_mm',guardar_pdf=True)

"""## Prueba"""

data_preci_inumet.columns

# 1) Asegurar índice datetime
data_preci_inumet = data_preci_inumet.copy()
if not np.issubdtype(data_preci_inumet.index.dtype, np.datetime64):
    data_preci_inumet.index = pd.to_datetime(data_preci_inumet.index, errors='coerce')
    data_preci_inumet = data_preci_inumet[~data_preci_inumet.index.isna()]  # quita filas con índice inválido

# 2) Detectar columnas con dtype 'object' o con strings
cols_obj = [c for c in data_preci_inumet.columns if data_preci_inumet[c].dtype == 'O']
print("Columnas con dtype object:", cols_obj)

# 3) Ver ejemplos de valores no numéricos por columna
def ejemplos_no_numericos(s, n=8):
    mask = ~pd.to_numeric(s, errors='coerce').notna() & s.notna()
    return s[mask].drop_duplicates().head(n).tolist()

for c in data_preci_inumet.columns:
    try:
        bad = ejemplos_no_numericos(data_preci_inumet[c])
        if bad:
            print(f"[{c}] ejemplos no numéricos ->", bad)
    except Exception:
        pass

import re

def limpiar_df_precipitacion(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # 1) Normalizar strings frecuentes
    def normalizar_valor(x):
        if isinstance(x, str):
            x = x.strip()
            # guiones unicode → ASCII
            x = x.replace("\u2212", "-").replace("−", "-")
            # coma decimal → punto
            x = x.replace(",", ".")
            # valores típicos de "missing" a vacío
            if x in {"", ".", "-", "NA", "N/A", "null", "None"}:
                return np.nan
            # quitar separadores o símbolos extraños, dejar dígitos/.,+,-,e,E
            x = re.sub(r"[^\d\.\-\+eE]", "", x)
        return x

    df = df.applymap(normalizar_valor)

    # 2) Convertir todo a numérico (lo no convertible → NaN)
    for c in df.columns:
        df[c] = pd.to_numeric(df[c], errors='coerce')

    # 3) Precipitaciones negativas imposibles → NaN
    for c in df.columns:
        df.loc[df[c] < 0, c] = np.nan

    # 4) Asegurar índice datetime
    if not np.issubdtype(df.index.dtype, np.datetime64):
        df.index = pd.to_datetime(df.index, errors='coerce')
        df = df[~df.index.isna()]

    return df

data_preci_inumet_clean = limpiar_df_precipitacion(data_preci_inumet)

# Chequeo post-limpieza
print(data_preci_inumet_clean.dtypes)

import numpy as np
import pandas as pd
from scipy.stats import gumbel_r, genextreme, kstest
from sklearn.utils import resample

# ============== UTILIDADES BÁSICAS ==============

def ensure_datetime_index(df: pd.DataFrame) -> pd.DataFrame:
    if not np.issubdtype(df.index.dtype, np.datetime64):
        df = df.copy()
        df.index = pd.to_datetime(df.index, errors='coerce')
        df = df[~df.index.isna()]
    return df

def annual_maxima(df: pd.DataFrame) -> pd.DataFrame:
    """Máximo por día -> Máximo anual (cierra en dic)."""
    df = ensure_datetime_index(df.copy())
    dfD = df.resample("D").max()
    ann = dfD.resample("A-DEC").max()
    ann.index = ann.index.year
    return ann

def fit_gumbel(x: pd.Series):
    x = pd.Series(x).dropna().values
    loc, scale = gumbel_r.fit(x)  # MLE
    return {"loc": float(loc), "scale": float(scale)}

def fit_gev(x: pd.Series):
    x = pd.Series(x).dropna().values
    c, loc, scale = genextreme.fit(x)  # MLE (SciPy usa 'c' como shape)
    return {"shape": float(c), "loc": float(loc), "scale": float(scale)}

def ks_test(x: pd.Series, dist: str, params: dict):
    x = pd.Series(x).dropna().values
    if dist == "gumbel":
        cdf = lambda v: gumbel_r.cdf(v, loc=params["loc"], scale=params["scale"])
    else:
        cdf = lambda v: genextreme.cdf(v, c=params["shape"], loc=params["loc"], scale=params["scale"])
    stat, p = kstest(x, cdf)
    return float(stat), float(p)

def return_levels(params: dict, dist: str, T=(10,25,50,100)) -> dict:
    """Cuantiles para periodos T usando F = 1 - 1/T."""
    T = np.asarray(T, dtype=float)
    F = 1.0 - 1.0 / T
    if dist == "gumbel":
        q = gumbel_r.ppf(F, loc=params["loc"], scale=params["scale"])
    else:
        q = genextreme.ppf(F, c=params["shape"], loc=params["loc"], scale=params["scale"])
    return {f"T{int(t)}": float(v) for t, v in zip(T, q)}

# ============== BOOTSTRAP ROBUSTO (OPCIONAL) ==============

def bootstrap_param_ci_robusto(x: pd.Series, dist="gev", n_boot=300, alpha=0.05, trim=0.01, seed=42):
    """
    Bootstrap percentil con recorte de colas (trim) para IC más estables.
    Devuelve median y low/high para cada parámetro. Si falla, ok=False.
    """
    rng = np.random.default_rng(seed)
    x = pd.Series(x).dropna().values
    if len(x) < 8:
        return {"ok": False, "error": "Muy pocos años para bootstrap", "n_ok": 0}

    ests = []
    for _ in range(n_boot):
        xb = resample(x, replace=True, random_state=rng.integers(0, 1_000_000_000))
        try:
            if dist == "gumbel":
                loc, scale = gumbel_r.fit(xb)
                ests.append({"loc": loc, "scale": scale})
            else:
                c, loc, scale = genextreme.fit(xb)
                ests.append({"shape": c, "loc": loc, "scale": scale})
        except Exception:
            continue

    if not ests:
        return {"ok": False, "error": "Sin réplicas válidas", "n_ok": 0}

    dfb = pd.DataFrame(ests)
    med = dfb.median()
    lo  = dfb.quantile(alpha/2 + trim)
    hi  = dfb.quantile(1 - alpha/2 - trim)

    out = {"ok": True, "n_ok": int(len(dfb))}
    for k in dfb.columns:
        out[f"{k}_median"] = float(med[k])
        out[f"{k}_low"]    = float(lo[k])
        out[f"{k}_high"]   = float(hi[k])
    return out

# ============== EJECUCIÓN POR ESTACIÓN (RÁPIDO) ==============

def resumen_sin_bootstrap_por_estacion(ann: pd.DataFrame, incluir_retornos=False, T=(10,25,50,100)) -> pd.DataFrame:
    """
    Corre Gumbel y GEV + K–S para TODAS las estaciones (sin bootstrap).
    ann: DF de máximos anuales (salida de annual_maxima).
    """
    filas = []
    for est in ann.columns:
        serie = ann[est].dropna()
        n = len(serie)
        if n < 8:
            filas += [
                {"estacion": est, "dist":"Gumbel","n_anios":n,"warning":"<8 años"},
                {"estacion": est, "dist":"GEV","n_anios":n,"warning":"<8 años"},
            ]
            continue

        # Gumbel
        try:
            g = fit_gumbel(serie)
            s, p = ks_test(serie, "gumbel", g)
            fila = {"estacion": est,"dist":"Gumbel","n_anios":n,
                    "loc":g["loc"],"scale":g["scale"],"shape":np.nan,
                    "ks_stat":s,"ks_pvalue":p,"warning":""}
            if incluir_retornos:
                fila.update({f"ret_{k}": v for k,v in return_levels(g, "gumbel", T=T).items()})
            filas.append(fila)
        except Exception as e:
            filas.append({"estacion": est,"dist":"Gumbel","n_anios":n,"warning":f"ajuste falló: {e}"})

        # GEV
        try:
            gev = fit_gev(serie)
            s, p = ks_test(serie, "gev", gev)
            fila = {"estacion": est,"dist":"GEV","n_anios":n,
                    "loc":gev["loc"],"scale":gev["scale"],"shape":gev["shape"],
                    "ks_stat":s,"ks_pvalue":p,"warning":""}
            if incluir_retornos:
                fila.update({f"ret_{k}": v for k,v in return_levels(gev, "gev", T=T).items()})
            filas.append(fila)
        except Exception as e:
            filas.append({"estacion": est,"dist":"GEV","n_anios":n,"warning":f"ajuste falló: {e}"})

    out = pd.DataFrame(filas).sort_values(["estacion","dist"]).reset_index(drop=True)
    return out

# ============== BOOTSTRAP FOCALIZADO (lista de estaciones) ==============

def bootstrap_por_estacion(ann: pd.DataFrame, estaciones, dist="gev", n_boot=300, alpha=0.05, trim=0.01, seed=42):
    """
    Corre bootstrap robusto SOLO para las estaciones indicadas.
    Devuelve DF con IC para parámetros (median/low/high).
    """
    filas = []
    for est in estaciones:
        serie = ann[est].dropna()
        n = len(serie)
        if n < 8:
            filas.append({"estacion": est, "dist": dist, "n_anios": n, "ok": False, "warning": "<8 años"})
            continue
        ci = bootstrap_param_ci_robusto(serie, dist=dist, n_boot=n_boot, alpha=alpha, trim=trim, seed=seed)
        fila = {"estacion": est, "dist": dist, "n_anios": n, "ok": ci.get("ok", False), "n_boot_ok": ci.get("n_ok", 0)}
        for k,v in ci.items():
            if k in {"ok","n_ok"}: continue
            fila[k] = v
        filas.append(fila)
    return pd.DataFrame(filas).sort_values("estacion").reset_index(drop=True)

# ============== PARALELIZACIÓN (opcional con joblib) ==============

def resumen_sin_bootstrap_parallel(ann: pd.DataFrame, incluir_retornos=False, T=(10,25,50,100), n_jobs=-1):
    """
    Igual que resumen_sin_bootstrap_por_estacion pero en paralelo si joblib está disponible.
    """
    try:
        from joblib import Parallel, delayed
    except ImportError:
        # sin joblib -> modo secuencial
        return resumen_sin_bootstrap_por_estacion(ann, incluir_retornos=incluir_retornos, T=T)

    def _proc(est):
        filas = []
        serie = ann[est].dropna()
        n = len(serie)
        if n < 8:
            filas += [
                {"estacion": est, "dist":"Gumbel","n_anios":n,"warning":"<8 años"},
                {"estacion": est, "dist":"GEV","n_anios":n,"warning":"<8 años"},
            ]
            return filas
        # Gumbel
        try:
            g = fit_gumbel(serie); s, p = ks_test(serie, "gumbel", g)
            fila = {"estacion": est,"dist":"Gumbel","n_anios":n,
                    "loc":g["loc"],"scale":g["scale"],"shape":np.nan,
                    "ks_stat":s,"ks_pvalue":p,"warning":""}
            if incluir_retornos: fila.update({f"ret_{k}": v for k,v in return_levels(g, "gumbel", T=T).items()})
            filas.append(fila)
        except Exception as e:
            filas.append({"estacion": est,"dist":"Gumbel","n_anios":n,"warning":f"ajuste falló: {e}"})
        # GEV
        try:
            gev = fit_gev(serie); s, p = ks_test(serie, "gev", gev)
            fila = {"estacion": est,"dist":"GEV","n_anios":n,
                    "loc":gev["loc"],"scale":gev["scale"],"shape":gev["shape"],
                    "ks_stat":s,"ks_pvalue":p,"warning":""}
            if incluir_retornos: fila.update({f"ret_{k}": v for k,v in return_levels(gev, "gev", T=T).items()})
            filas.append(fila)
        except Exception as e:
            filas.append({"estacion": est,"dist":"GEV","n_anios":n,"warning":f"ajuste falló: {e}"})
        return filas

    resultados = Parallel(n_jobs=n_jobs, prefer="threads")(delayed(_proc)(est) for est in ann.columns)
    out = pd.DataFrame([r for sub in resultados for r in sub]).sort_values(["estacion","dist"]).reset_index(drop=True)
    return out

# ============== EJEMPLOS DE USO ==============

# 1) Calcular UNA VEZ los máximos anuales
ann = annual_maxima(data_preci_inumet_clean)

# 2) Resumen rápido para TODAS las estaciones (en paralelo si tenés joblib)
resumen_rapido = resumen_sin_bootstrap_parallel(ann, incluir_retornos=True, T=(10,25,50,100), n_jobs=-1)
# Si no tenés joblib: usar resumen_sin_bootstrap_por_estacion(ann, incluir_retornos=True)

# 3) Elegir estaciones para bootstrap (ejemplo: las 4 más importantes)
estaciones_para_boot = ["Salto_mm", "Artigas_mm", "Carrasco_mm", "Rivera_mm"]

# 4) Bootstrap robusto SOLO para esas (GEV, 300 réplicas)
ci_gev = bootstrap_por_estacion(ann, estaciones_para_boot, dist="gev", n_boot=300, alpha=0.05, trim=0.01, seed=42)

# 5) (Opcional) Bootstrap de Gumbel para comparar
# ci_gum = bootstrap_por_estacion(ann, estaciones_para_boot, dist="gumbel", n_boot=300, alpha=0.05, trim=0.01, seed=42)

resumen_rapido

ci_gev

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde

# --- helpers: tomar valor de posición desde el resumen ya calculado ---
def _loc_from_resumen(resumen: pd.DataFrame, estacion: str, dist: str) -> float:
    fila = resumen[(resumen["estacion"] == estacion) & (resumen["dist"] == dist)]
    if fila.empty:
        raise ValueError(f"No encuentro fila en 'resumen' para {estacion} / {dist}")
    return float(fila["loc"].iloc[0])

def _centrar_serie(serie: pd.Series, metodo: str, resumen: pd.DataFrame | None, estacion: str):
    x = serie.dropna()
    label = "sin centrado"
    ref = None
    if metodo == "none":
        return x, ref, label
    if metodo == "media":
        ref = x.mean();   label = "centrado en media"
        return x - ref, ref, label
    if metodo == "mediana":
        ref = x.median(); label = "centrado en mediana"
        return x - ref, ref, label
    if metodo == "loc_gumbel":
        assert resumen is not None, "Pasá 'resumen' para usar loc_gumbel"
        ref = _loc_from_resumen(resumen, estacion, "Gumbel")
        label = "centrado en loc (Gumbel)"
        return x - ref, ref, label
    if metodo == "loc_gev":
        assert resumen is not None, "Pasá 'resumen' para usar loc_gev"
        ref = _loc_from_resumen(resumen, estacion, "GEV")
        label = "centrado en loc (GEV)"
        return x - ref, ref, label
    raise ValueError("metodo debe ser: 'none','media','mediana','loc_gumbel','loc_gev'")

# ===================== FACET (dos paneles) =====================
def plot_densidades_facet(ann: pd.DataFrame,
                          resumen: pd.DataFrame | None,
                          estaciones=("Artigas_mm","Mercedes_mm"),
                          centrar_por="none",
                          bandwidth=None,       # None = auto; también podés pasar un float
                          fill_alpha=0.35):
    e1, e2 = estaciones
    s1 = ann[e1].dropna()
    s2 = ann[e2].dropna()

    s1c, ref1, lab = _centrar_serie(s1, centrar_por, resumen, e1)
    s2c, ref2, _   = _centrar_serie(s2, centrar_por, resumen, e2)

    kde1 = gaussian_kde(s1c, bw_method=bandwidth)
    kde2 = gaussian_kde(s2c, bw_method=bandwidth)

    xmin = float(min(s1c.min(), s2c.min()))
    xmax = float(max(s1c.max(), s2c.max()))
    xr = np.linspace(xmin - 0.05*(xmax-xmin), xmax + 0.05*(xmax-xmin), 400)
    y1 = kde1(xr); y2 = kde2(xr)

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6), sharex=True)

    ax1.plot(xr, y1); ax1.fill_between(xr, 0, y1, alpha=fill_alpha)
    ax1.set_ylabel(e1)
    if centrar_por != "none":
        ax1.axvline(0, ls="--", lw=1)

    ax2.plot(xr, y2); ax2.fill_between(xr, 0, y2, alpha=fill_alpha)
    ax2.set_ylabel(e2)
    if centrar_por != "none":
        ax2.axvline(0, ls="--", lw=1)

    ax2.set_xlabel("Precipitación (mm)" + (" (centrada)" if centrar_por!="none" else ""))
    fig.suptitle(f"Comparación de densidades empíricas — {lab}")
    fig.tight_layout(rect=[0,0,1,0.95])
    plt.show()

# ===================== OVERLAY (un panel) =====================
def plot_densidades_overlay(ann: pd.DataFrame,
                            resumen: pd.DataFrame | None,
                            estaciones=("Artigas_mm","Mercedes_mm"),
                            centrar_por="none",
                            bandwidth=None,
                            fill_alpha=0.35):
    e1, e2 = estaciones
    s1c, _, lab = _centrar_serie(ann[e1], centrar_por, resumen, e1)
    s2c, _, _   = _centrar_serie(ann[e2], centrar_por, resumen, e2)

    kde1 = gaussian_kde(s1c.dropna(), bw_method=bandwidth)
    kde2 = gaussian_kde(s2c.dropna(), bw_method=bandwidth)

    xmin = float(min(s1c.min(), s2c.min()))
    xmax = float(max(s1c.max(), s2c.max()))
    xr = np.linspace(xmin - 0.05*(xmax-xmin), xmax + 0.05*(xmax-xmin), 400)
    y1 = kde1(xr); y2 = kde2(xr)

    plt.figure(figsize=(8,4.5))
    plt.plot(xr, y1, label=e1)
    plt.fill_between(xr, 0, y1, alpha=fill_alpha)
    plt.plot(xr, y2, label=e2)
    plt.fill_between(xr, 0, y2, alpha=fill_alpha)
    if centrar_por != "none":
        plt.axvline(0, ls="--", lw=1)
    plt.xlabel("Precipitación (mm)" + (" (centrada)" if centrar_por!="none" else ""))
    plt.ylabel("Densidad")
    plt.title(f"Densidades empíricas superpuestas — {lab}")
    plt.legend(frameon=False)
    plt.tight_layout()
    plt.show()

# ann  -> tus máximos anuales por estación (ya calculados)
# resumen -> tu tabla con parámetros y KS (como la de la captura)

# Facet sin centrar (muestra en mm reales)
plot_densidades_facet(ann, resumen_rapido, estaciones=("Artigas_mm","Mercedes_mm"), centrar_por="none")

# Facet centrando por loc de GEV (usa 'resumen'; NO refitea)
plot_densidades_facet(ann, resumen_rapido, estaciones=("Artigas_mm","Mercedes_mm"), centrar_por="loc_gev")

# Overlay centrando por mediana (comparás solo forma/escala)
plot_densidades_overlay(ann, resumen_rapido, estaciones=("Artigas_mm","Mercedes_mm"), centrar_por="mediana")

# ann  -> tus máximos anuales por estación (ya calculados)
# resumen -> tu tabla con parámetros y KS (como la de la captura)

# Facet sin centrar (muestra en mm reales)
plot_densidades_facet(ann, resumen_rapido, estaciones=("Artigas_mm","Young_mm"), centrar_por="none")

# Facet centrando por loc de GEV (usa 'resumen'; NO refitea)
plot_densidades_facet(ann, resumen_rapido, estaciones=("Artigas_mm","Young_mm"), centrar_por="loc_gev")

# Overlay centrando por mediana (comparás solo forma/escala)
plot_densidades_overlay(ann, resumen_rapido, estaciones=("Artigas_mm","Young_mm"), centrar_por="mediana")

# ann  -> tus máximos anuales por estación (ya calculados)
# resumen -> tu tabla con parámetros y KS (como la de la captura)

# Facet sin centrar (muestra en mm reales)
plot_densidades_facet(ann, resumen_rapido, estaciones=("Colonia_mm","Rivera_mm"), centrar_por="none")

# Facet centrando por loc de GEV (usa 'resumen'; NO refitea)
plot_densidades_facet(ann, resumen_rapido, estaciones=("Colonia_mm","Rivera_mm"), centrar_por="loc_gev")

# Overlay centrando por mediana (comparás solo forma/escala)
plot_densidades_overlay(ann, resumen_rapido, estaciones=("Colonia_mm","Rivera_mm"), centrar_por="mediana")

# ---------- (a) P–P plot ----------
def plot_pp(ann: pd.DataFrame, resumen: pd.DataFrame, estacion: str, dist: str="GEV",
            savepath: str=None):
    x = ann[estacion].dropna().values
    x_sorted = np.sort(x)
    n = len(x_sorted)
    # Gringorten (suave en colas)
    p_emp = (np.arange(1, n+1) - 0.44) / (n + 0.12)
    params = _get_params(resumen, estacion, dist)
    p_mod = _cdf(x_sorted, dist, params)

    plt.figure(figsize=(5,4))
    plt.plot(p_emp, p_mod, 'o', ms=4, alpha=0.8)
    plt.plot([0,1],[0,1], '-', lw=1)
    plt.xlabel("Probabilidades empíricas")
    plt.ylabel("Probabilidades del modelo")
    plt.title(f"(a) P–P plot — {estacion} ({dist})")
    plt.tight_layout()

    if savepath:   # <<< guardar si se pasa ruta
        plt.savefig(savepath, dpi=300, bbox_inches="tight")
    plt.show()

# ---------- (b) Q–Q plot ----------
def plot_qq(ann: pd.DataFrame, resumen: pd.DataFrame, estacion: str, dist: str="GEV",
            savepath: str=None):
    x = ann[estacion].dropna().values
    x_sorted = np.sort(x)
    n = len(x_sorted)
    p = (np.arange(1, n+1) - 0.44) / (n + 0.12)
    params = _get_params(resumen, estacion, dist)
    q_mod = _ppf(p, dist, params)

    plt.figure(figsize=(5,4))
    plt.plot(x_sorted, q_mod, 'o', ms=4, alpha=0.8, label="Datos vs modelo")
    lims = [min(x_sorted.min(), q_mod.min()), max(x_sorted.max(), q_mod.max())]
    plt.plot(lims, lims, '-', lw=1, label="Línea 1–1")
    plt.xlabel("Cuantiles empíricos (mm)")
    plt.ylabel("Cuantiles del modelo (mm)")
    plt.title(f"(b) Q–Q plot — {estacion} ({dist})")
    plt.legend(frameon=False)
    plt.tight_layout()

    if savepath:
        plt.savefig(savepath, dpi=300, bbox_inches="tight")
    plt.show()

# ---------- (c) Densidad empírica vs modelada ----------
def plot_density(ann: pd.DataFrame, resumen: pd.DataFrame, estacion: str, dist: str="GEV",
                 savepath: str=None):
    x = ann[estacion].dropna().values
    params = _get_params(resumen, estacion, dist)

    kde = gaussian_kde(x)
    xx = np.linspace(x.min()*0.9, x.max()*1.1, 400)
    y_emp = kde(xx)
    y_mod = _pdf(xx, dist, params)

    plt.figure(figsize=(6,4))
    plt.plot(xx, y_emp, label="Empírica", lw=1.6)
    plt.plot(xx, y_mod, linestyle="--", label="Modelada", lw=1.6)
    plt.xlabel("Precipitación máxima diaria (mm)")
    plt.ylabel("Densidad")
    plt.title(f"(c) Densidad empírica vs modelada — {estacion} ({dist})\nN={len(x)}  Bandwidth (auto KDE)")
    plt.legend(frameon=False)
    plt.tight_layout()

    if savepath:
        plt.savefig(savepath, dpi=300, bbox_inches="tight")
    plt.show()

# ---------- (d) Return Level plot ----------
def plot_return_level(ann: pd.DataFrame, resumen: pd.DataFrame, estacion: str, dist: str="GEV",
                      T_grid=(2,3,5,10,20,50,100,200,500,1000),
                      savepath: str=None):
    x = ann[estacion].dropna().values
    n = len(x)
    x_sorted = np.sort(x)
    # posiciones y periodos de retorno empíricos
    p_emp = (np.arange(1, n+1) - 0.44) / (n + 0.12)
    T_emp = 1.0 / (1.0 - p_emp)

    params = _get_params(resumen, estacion, dist)
    T_grid = np.asarray(T_grid, dtype=float)
    F = 1.0 - 1.0 / T_grid
    q_mod = _ppf(F, dist, params)

    plt.figure(figsize=(6,4))
    plt.plot(T_emp, x_sorted, 'o', ms=4, alpha=0.8, label="Empírico")
    plt.plot(T_grid, q_mod, '-', lw=1.8, label=f"Modelo {dist}")
    plt.xscale("log")
    plt.xlabel("Periodo de retorno (años)")
    plt.ylabel("Nivel de retorno (mm)")
    plt.title(f"(d) Niveles de retorno — {estacion} ({dist})")
    plt.grid(True, which="both", axis="x", alpha=0.2)
    plt.legend(frameon=False)
    plt.tight_layout()

    if savepath:
        plt.savefig(savepath, dpi=300, bbox_inches="tight")
    plt.show()

plot_qq(ann, resumen_rapido, "Artigas_mm", "Gumbel",
        savepath="QQplot_Artigas_Gumbel.svg")

plot_qq(ann, resumen_rapido, "Prado_mm", "Gumbel",
        savepath="QQplot_Prado_Gumbel.svg")

# ---------- Ejemplo de uso (ajustá estación/dist a gusto) ----------
# P–P, Q–Q, Densidad, Return level para Artigas con GEV:
plot_pp(ann, resumen_rapido, "Artigas_mm", "GEV")
plot_qq(ann, resumen_rapido, "Artigas_mm", "GEV")
plot_density(ann, resumen_rapido, "Artigas_mm", "GEV")
plot_return_level(ann, resumen_rapido, "Artigas_mm", "GEV")

# Idem Mercedes con Gumbel:
plot_pp(ann, resumen_rapido, "Mercedes_mm", "Gumbel")
plot_qq(ann, resumen_rapido, "Mercedes_mm", "Gumbel")
plot_density(ann, resumen_rapido, "Mercedes_mm", "Gumbel")
plot_return_level(ann, resumen_rapido, "Mercedes_mm", "Gumbel")

# Idem Mercedes con Gumbel:
plot_pp(ann, resumen_rapido, "Melo_mm", "Gumbel")
plot_qq(ann, resumen_rapido, "Melo_mm", "Gumbel")
plot_density(ann, resumen_rapido, "Melo_mm", "Gumbel")
plot_return_level(ann, resumen_rapido, "Melo_mm", "Gumbel")

# Idem Mercedes con Gumbel:
plot_pp(ann, resumen_rapido, "Rivera_mm", "Gumbel")
plot_qq(ann, resumen_rapido, "Rivera_mm", "Gumbel")
plot_density(ann, resumen_rapido, "Rivera_mm", "Gumbel")
plot_return_level(ann, resumen_rapido, "Rivera_mm", "Gumbel")

plot_qq(ann, resumen_rapido, "Artigas_mm", "Gumbel")
fig = plt.gcf()
fig.savefig("QQplot_Artigas_Gumbel.svg", dpi=300, bbox_inches="tight")
plt.close(fig)

# Idem Mercedes con Gumbel:
plot_pp(ann, resumen_rapido, "Artigas_mm", "Gumbel")
plt.savefig("PPplot_Artigas_Gumbel.svg", dpi=300, bbox_inches="tight")
plt.close()
plot_qq(ann, resumen_rapido, "Artigas_mm", "Gumbel")
plt.savefig("QQplot_Artigas_Gumbel.svg", dpi=300, bbox_inches="tight")
plt.close()
plot_density(ann, resumen_rapido, "Artigas_mm", "Gumbel")
plot_return_level(ann, resumen_rapido, "Artigas_mm", "Gumbel")

# Idem Mercedes con Gumbel:
plot_pp(ann, resumen_rapido, "Young_mm", "Gumbel")
plot_qq(ann, resumen_rapido, "Young_mm", "Gumbel")
plot_density(ann, resumen_rapido, "Young_mm", "Gumbel")
plot_return_level(ann, resumen_rapido, "Young_mm", "Gumbel")

"""##Anomalías de precipitación mensuales

"Las anomalias son indicadores que permiten interpretar si en un ano ha llovido por encima o por debajo de lo normal o lo esperado, convirtiéndose estas en herramientas útiles para la determinación de años Niña y Niño" http://www.scielo.org.co/scielo.php?pid=S0012-73532022000200168&script=sci_arttext

**Se interpreta de la siguiente manera:**

**Valores negativos** (Ejemplo: -72.25 en Artigas_mm en enero de 1991) Ese mes llovió un 72.25% menos que el promedio histórico del mismo mes.

**Valores positivos** (Ejemplo: 196.21 en Artigas_mm en abril de 1991) Ese mes llovió un 196.21% más que el promedio histórico del mismo mes.
"""

# Copiar el DataFrame de entrada
df = data_meses.copy()

# Asegurar que el índice es tipo fecha
df.index = pd.to_datetime(df.index)

# Calcular la precipitación promedio mensual para cada estación usando todos los datos disponibles
media_mensual = df.groupby(df.index.month).mean()

# Función para calcular la anomalía mensual
def calcular_anomalias(df, media_mensual):
    df_anomalias = df.copy()
    for mes in range(1, 13):  # Iterar sobre cada mes
        indices_mes = df.index.month == mes
        df_anomalias.loc[indices_mes] = ((df.loc[indices_mes] - media_mensual.loc[mes]) / media_mensual.loc[mes]) * 100
    return df_anomalias

# Calcular las anomalías de precipitación
df_anomalias = calcular_anomalias(df, media_mensual)

# Mostrar una vista previa de los datos
print(df_anomalias.head())

df_anomalias

def graficar_anomalias_por_estacion(df_anomalias, estaciones):
    for estacion in estaciones:
        plt.figure(figsize=(12, 5))
        plt.axhline(0, color='black', linewidth=1)  # Línea base en 0
        plt.plot(df_anomalias.index, df_anomalias[estacion], color='gray', alpha=0.6)  # Línea base

        # Resaltar anomalías positivas (azul) y negativas (rojo)
        plt.fill_between(df_anomalias.index, df_anomalias[estacion], where=(df_anomalias[estacion] > 0),
                         color='blue', alpha=0.5, label="Anomalía positiva")
        plt.fill_between(df_anomalias.index, df_anomalias[estacion], where=(df_anomalias[estacion] < 0),
                         color='red', alpha=0.5, label="Anomalía negativa")

        plt.title(f"Anomalías de precipitación en {estacion}")
        plt.xlabel("Fecha")
        plt.ylabel("Anomalía (%)")
        plt.legend()
        plt.grid()

        # Ajustar límites del eje X para eliminar los espacios en blanco
        plt.xlim(df_anomalias.index.min(), df_anomalias.index.max())

        plt.show()

# Lista de estaciones
estaciones = ["Artigas_mm", "Carrasco_mm", "Melo_mm", "Colonia_mm", "Durazno_mm",
              "Trinidad_mm", "Florida_mm", "PuntaEste_mm", "Prado_mm", "Melilla_mm",
              "Paysandu_mm", "Rivera_mm", "Rocha_mm", "Salto_mm", "Mercedes_mm",
              "TreintaTres_mm", "Young_mm"]

# Llamar a la función para graficar todas las estaciones
graficar_anomalias_por_estacion(df_anomalias, estaciones)

print(df_anomalias.head())

# Estadísticos básicos para cada pluviómetro
estadisticosAnomalias =df_anomalias.describe().T  # describe() devuelve estadísticos como media, std, min, 25%, 50%, 75%, max
# Mostrar el DataFrame con los estadísticos
estadisticosAnomalias

df

# Asegurarse de que el índice es datetime
df.index = pd.to_datetime(df.index)

# Calcular la media mensual por estación
media_mensual = df.groupby(df.index.month).mean()

# Renombrar el índice numérico del mes a nombre del mes (opcional)
import calendar
media_mensual.index = [calendar.month_name[m] for m in media_mensual.index]

# Mostrar el resultado
print(media_mensual)

"""###ANOMALÍAS + CONTEO

En el gráfico se presenta la evolución temporal de las anomalías mensuales de precipitación para la estación de Mercedes, expresadas en porcentaje respecto al promedio climatológico mensual. Sobre cada barra se indican, mediante puntos rojos y etiquetas numéricas, la cantidad de eventos extremos diarios ocurridos en ese mes, definidos como aquellos que superan el percentil 95 de la precipitación diaria.
"""

# Asegurar que ambos índices estén al final de cada mes
df_anomalias.index = df_anomalias.index.to_period('M').to_timestamp('M')
eventos_mensuales = df_eventos_extremos_NueveCinco.resample('M').count()
eventos_mensuales.index = eventos_mensuales.index.to_period('M').to_timestamp('M')

def graficar_anomalias_eventos(df_anomalias, eventos_mensuales, estacion, mostrar_tabla=False, guardar=False, nombre_archivo=None):
    """
    Genera un gráfico de barras con anomalías mensuales de precipitación y
    puntos etiquetados con la cantidad de eventos extremos por mes.

    Parámetros:
    - df_anomalias: DataFrame con anomalías mensuales (%)
    - eventos_mensuales: DataFrame con conteo de eventos extremos mensuales
    - estacion: string con el nombre de la estación (debe coincidir con las columnas)
    - mostrar_tabla: bool, si True imprime las primeras 20 filas combinadas
    - guardar: bool, si True guarda el gráfico como PNG
    - nombre_archivo: string, nombre del archivo si se guarda (sin extensión)
    """
    # Concatenar por columna para comparar (opcional)
    comparacion = pd.concat([df_anomalias, eventos_mensuales], axis=1, keys=['Anomalias (%)', 'Eventos_extremos'])

    if mostrar_tabla:
        try:
            print(comparacion.loc[:, [('Anomalias (%)', estacion), ('Eventos_extremos', estacion)]].dropna().head(20))
        except KeyError:
            print(f"⚠️ La estación '{estacion}' no se encuentra en los DataFrames.")
            return

    # Extraer series
    try:
        anom = df_anomalias[estacion]
        evt = eventos_mensuales[estacion]
    except KeyError:
        print(f"⚠️ La estación '{estacion}' no se encuentra en los DataFrames.")
        return

    # Asegurar fechas con mismo formato mensual
    anom.index = anom.index.to_period('M').to_timestamp('M')
    evt.index = evt.index.to_period('M').to_timestamp('M')

    # Fechas válidas (ambos tienen datos)
    fechas_validas = anom.dropna().index.intersection(evt.dropna().index)
    anom = anom.loc[fechas_validas]
    evt = evt.loc[fechas_validas]

    # Crear figura
    fig, ax1 = plt.subplots(figsize=(14, 6))

    # Barras de anomalía
    ax1.bar(anom.index, anom, width=50, color='lightblue', alpha=1)
    ax1.axhline(0, color='gray', linestyle='--', linewidth=1)
    ax1.set_ylabel('Anomalía de precipitación (%)', color='steelblue')
    ax1.tick_params(axis='y', labelcolor='steelblue')

    # Puntos y etiquetas para eventos
    for fecha in fechas_validas:
        n_eventos = int(evt[fecha])
        valor_anom = anom[fecha]
        if n_eventos > 0:
            ax1.plot(fecha, valor_anom, 'o', color='crimson', markersize=6, zorder=5)
            ax1.text(fecha, valor_anom + 10, f'{n_eventos}', color='crimson', fontsize=9, ha='center', weight='bold')

    # Título
    plt.title(f"Anomalía mensual y eventos extremos - {estacion}")
    plt.grid(True, axis='x', linestyle=':', alpha=0.5)
    fig.tight_layout()

    # Guardar si se desea
    if guardar:
        if not nombre_archivo:
            nombre_archivo = f"anomalias_eventos_{estacion}"
        plt.savefig(f"{nombre_archivo}.png", dpi=300)
        print(f"✅ Gráfico guardado como {nombre_archivo}.png")

    # Mostrar
    plt.show()

graficar_anomalias_eventos(df_anomalias, eventos_mensuales, 'Mercedes_mm', mostrar_tabla=True,guardar=True, nombre_archivo='AnomMenMercedes')

graficar_anomalias_eventos(df_anomalias, eventos_mensuales, 'Artigas_mm', mostrar_tabla=True,guardar=True, nombre_archivo='AnomMenArtigas')

graficar_anomalias_eventos(df_anomalias, eventos_mensuales, 'Young_mm', mostrar_tabla=True,guardar=True, nombre_archivo='AnomMenYoung')

graficar_anomalias_eventos(df_anomalias, eventos_mensuales, 'Salto_mm', mostrar_tabla=True,guardar=True, nombre_archivo='AnomMenSalto')

graficar_anomalias_eventos(df_anomalias, eventos_mensuales, 'Prado_mm', mostrar_tabla=True,guardar=True, nombre_archivo='AnomMenPrado')

"""###ANOMALÍAS + PRECIPITACIÓN EXTREMA"""

def graficar_anomalias_y_eventos_individuales(df_anomalias, df_eventos_extremos, estacion, mostrar_tabla=False, guardar=False, nombre_archivo=None):
    """
    Grafica anomalías mensuales (%) y eventos extremos diarios (mm), con ejes perfectamente alineados:
    0% de anomalía coincide con 50 mm de precipitación. Ejes fijos.
    """
    import matplotlib.pyplot as plt

    try:
        anom = df_anomalias[estacion]
        eventos = df_eventos_extremos[estacion]
    except KeyError:
        print(f"⚠️ Estación '{estacion}' no encontrada.")
        return

    anom.index = anom.index.to_period('M').to_timestamp('M')
    eventos.index = pd.to_datetime(eventos.index)
    eventos_validos = eventos.dropna()
    anom = anom.dropna()

    if mostrar_tabla:
        print(eventos_validos.head(15))

    # Parámetros fijos
    anom_min, anom_max = -100, 400
    prec_ref = 50  # mm donde queremos que cruce
    prec_max = 300
    prec_min = 0

    # Fórmula lineal para transformar anomalía <-> precipitación
    def anom_to_mm(x):
        return (x - 0) * (prec_max - prec_ref) / (anom_max - 0) + prec_ref

    def mm_to_anom(y):
        return (y - prec_ref) * (anom_max - 0) / (prec_max - prec_ref) + 0

    # Crear figura
    fig, ax1 = plt.subplots(figsize=(14, 6))
    ax1.bar(anom.index, anom, width=50, color='lightblue', alpha=1, label='Anomalía mensual (%)')
    ax1.axhline(0, color='gray', linestyle='--')
    ax1.set_ylabel('Anomalía de precipitación (%)', color='steelblue')
    ax1.tick_params(axis='y', labelcolor='steelblue')
    ax1.set_ylim(anom_min, anom_max)

    # Eje secundario perfectamente alineado
    ax2 = ax1.secondary_yaxis('right', functions=(anom_to_mm, mm_to_anom))
    ax2.set_ylabel('Precipitación diaria en eventos extremos (mm)', color='crimson')
    ax2.tick_params(axis='y', labelcolor='crimson')
    ax2.set_ylim(prec_min, prec_max)

    # Eventos extremos como puntos
    eventos_convertidos = mm_to_anom(eventos_validos.values)
    ax1.plot(eventos_validos.index, eventos_convertidos, 'o', color='crimson', label='Eventos extremos (mm)', zorder=5)

    plt.title(f"Anomalía mensual y eventos extremos diarios - {estacion}")
    ax1.grid(True, axis='x', linestyle=':', alpha=0.5)
    fig.tight_layout()

    if guardar:
        if not nombre_archivo:
            nombre_archivo = f"anomalias_eventos_individuales_{estacion}"
        plt.savefig(f"{nombre_archivo}.png", dpi=300)
        print(f"✅ Gráfico guardado como {nombre_archivo}.png")

    plt.show()

graficar_anomalias_y_eventos_individuales(df_anomalias, df_eventos_extremos_NueveCinco, 'Young_mm',guardar=True, nombre_archivo='AnomMenPreyoung')

graficar_anomalias_y_eventos_individuales(df_anomalias, df_eventos_extremos_NueveCinco, 'Mercedes_mm',guardar=True, nombre_archivo='AnomMenPreMercedes')

graficar_anomalias_y_eventos_individuales(df_anomalias, df_eventos_extremos_NueveCinco, 'Artigas_mm',guardar=True, nombre_archivo='AnomMenPreArtigas')

graficar_anomalias_y_eventos_individuales(df_anomalias, df_eventos_extremos_NueveCinco, 'Prado_mm',guardar=True, nombre_archivo='AnomMenPrePrado')

graficar_anomalias_y_eventos_individuales(df_anomalias, df_eventos_extremos_NueveCinco, 'Carrasco_mm',guardar=True, nombre_archivo='AnomMenPreCarrasco')

def resumen_eventos_en_anomalias_negativas(df_anomalias, df_eventos_extremos):
    """
    Devuelve un DataFrame resumen con total, máximo, mínimo y promedio de eventos extremos
    ocurridos en meses con anomalía negativa para todas las estaciones.

    Retorna:
    - df_resumen: DataFrame con filas ['Total', 'Máximo', 'Mínimo', 'Promedio'] y columnas por estación.
    """
    resumen = {}

    for estacion in df_anomalias.columns:
        try:
            # Índices en datetime
            df_anomalias.index = pd.to_datetime(df_anomalias.index)
            df_eventos_extremos.index = pd.to_datetime(df_eventos_extremos.index)

            # Anomalías negativas
            anom_neg = df_anomalias[df_anomalias[estacion] < 0].copy()
            meses_negativos = anom_neg.index.to_period('M')

            # Eventos extremos válidos
            eventos_est = df_eventos_extremos[[estacion]].dropna().copy()
            eventos_est['Mes'] = eventos_est.index.to_period('M')

            # Filtrar eventos dentro de meses con anomalía negativa
            eventos_filtrados = eventos_est[eventos_est['Mes'].isin(meses_negativos)][estacion]

            # Métricas
            total = len(eventos_filtrados)
            minimo = eventos_filtrados.min() if total > 0 else None
            maximo = eventos_filtrados.max() if total > 0 else None
            promedio = eventos_filtrados.mean() if total > 0 else None

            # Agregar al resumen
            resumen[estacion] = [total, maximo, minimo, promedio]

        except Exception as e:
            print(f"⚠️ Error procesando estación {estacion}: {e}")
            resumen[estacion] = [None, None, None, None]

    # Crear DataFrame final
    df_resumen = pd.DataFrame(resumen, index=['Total', 'Máximo', 'Mínimo', 'Promedio'])
    return df_resumen

df_resultado = resumen_eventos_en_anomalias_negativas(df_anomalias, df_eventos_extremos_NueveCinco)
display(df_resultado)

def analizar_eventos_en_anomalias_negativas(df_anomalias, df_eventos_extremos, estacion):
    """
    Devuelve resumen y DataFrame de eventos extremos que ocurrieron en meses con anomalías negativas.

    Parámetros:
    - df_anomalias: DataFrame mensual de anomalías (%), índice datetime mensual
    - df_eventos_extremos: DataFrame diario con eventos extremos (mm), índice datetime diario
    - estacion: nombre de la estación a analizar

    Retorna:
    - df_resultado: DataFrame con fechas y mm de eventos extremos en meses con anomalía negativa
    """
    # Asegurar índices datetime
    df_anomalias.index = pd.to_datetime(df_anomalias.index)
    df_eventos_extremos.index = pd.to_datetime(df_eventos_extremos.index)

    # Filtrar anomalías negativas
    anom_neg = df_anomalias[df_anomalias[estacion] < 0].copy()
    meses_negativos = anom_neg.index.to_period('M')

    # Filtrar eventos extremos válidos
    eventos_est = df_eventos_extremos[[estacion]].dropna().copy()
    eventos_est['Mes'] = eventos_est.index.to_period('M')

    # Filtrar los eventos que cayeron en meses con anomalía negativa
    eventos_en_negativos = eventos_est[eventos_est['Mes'].isin(meses_negativos)].copy()
    eventos_en_negativos = eventos_en_negativos.drop(columns='Mes')

    # Resumen
    valores = eventos_en_negativos[estacion].values
    cantidad = len(valores)
    minimo = valores.min() if cantidad > 0 else None
    maximo = valores.max() if cantidad > 0 else None
    media = valores.mean() if cantidad > 0 else None

    # Mostrar
    print(f"📉 Eventos extremos en meses con anomalía negativa - {estacion}")
    print(f"➡️  Total: {cantidad}")
    if cantidad > 0:
        print(f"🌧️  Mínimo: {minimo:.1f} mm")
        print(f"🌦️  Máximo: {maximo:.1f} mm")
        print(f"📊  Promedio: {media:.1f} mm")
    else:
        print("⚠️  No se encontraron eventos extremos en meses con anomalía negativa.")

    return eventos_en_negativos

eventos_negativos = analizar_eventos_en_anomalias_negativas(df_anomalias, df_eventos_extremos_NueveCinco, 'Mercedes_mm')
eventos_negativos

eventos_negativos = analizar_eventos_en_anomalias_negativas(df_anomalias, df_eventos_extremos_NueveCinco, 'Artigas_mm')
eventos_negativos

eventos_negativos = analizar_eventos_en_anomalias_negativas(df_anomalias, df_eventos_extremos_NueveCinco, 'Prado_mm')
eventos_negativos

"""### Anomalías precipitacion estacional"""

# Crear columnas auxiliares
data_imputado['Año'] = data_imputado.index.year
data_imputado['Mes'] = data_imputado.index.month

# Clasificación de estación
def clasificar_estacion(mes):
    if mes in [12, 1, 2]:
        return 'Verano'
    elif mes in [3, 4, 5]:
        return 'Otoño'
    elif mes in [6, 7, 8]:
        return 'Invierno'
    else:
        return 'Primavera'

data_imputado['Estacion'] = data_imputado['Mes'].apply(clasificar_estacion)

# Ajustar año para diciembre
data_imputado.loc[data_imputado['Mes'] == 12, 'Año'] += 1

# Validar que haya datos para los 3 meses de cada verano
veranos_validos = data_imputado[data_imputado['Estacion'] == 'Verano'].groupby('Año')['Mes'].nunique()
años_verano_completo = veranos_validos[veranos_validos == 3].index

# Filtrar solo veranos completos
data_filtrada = data_imputado[~((data_imputado['Estacion'] == 'Verano') & (~data_imputado['Año'].isin(años_verano_completo)))]

# Agrupar por año y estación
precip_estacional = data_filtrada.drop(columns=['Mes']).groupby(['Año', 'Estacion']).sum()

# Calcular climatología y anomalías (%)
clima_media = precip_estacional.groupby('Estacion').mean()
anomalias_estacionales = (precip_estacional - clima_media) / clima_media * 100

# Mostrar resultado final
anomalias_estacionales

# Función para clasificar estaciones
def clasificar_estacion(mes):
    if mes in [12, 1, 2]:
        return 'Verano'
    elif mes in [3, 4, 5]:
        return 'Otoño'
    elif mes in [6, 7, 8]:
        return 'Invierno'
    else:
        return 'Primavera'

# Crear columnas auxiliares
df_aux = df_eventos_extremos_NueveCinco.copy()
df_aux['Mes'] = df_aux.index.month
df_aux['Año'] = df_aux.index.year
df_aux['Estacion'] = df_aux['Mes'].apply(clasificar_estacion)

# Ajustar el año para diciembre (que cuenta para verano del año siguiente)
df_aux.loc[df_aux['Mes'] == 12, 'Año'] += 1

# Eliminar columnas auxiliares para el conteo
conteo_mask = df_aux.drop(columns=['Mes', 'Año', 'Estacion']).notna()

# Agrupar y contar por estación y año
conteo_estacional = conteo_mask.groupby([df_aux['Año'], df_aux['Estacion']]).sum()

# Mostrar resultados
conteo_estacional

def graficar_anomalias_vs_conteo_estacional(anomalias_estacionales, conteo_eventos_estacional, estacion_nombre):
    """
    Grafica anomalías estacionales (%) y conteo de eventos extremos, con barras coloreadas por estación
    y orden cronológico correcto (Verano, Otoño, Invierno, Primavera según ocurren).
    """

    # Combinar anomalías y conteo
    anom = anomalias_estacionales[[estacion_nombre]].copy()
    eventos = conteo_eventos_estacional[[estacion_nombre]].copy()

    df_plot = anom.join(eventos, lsuffix='_anom', rsuffix='_eventos').reset_index()

    # Crear columna con fecha representativa para ordenar
    estacion_meses = {
        'Verano': 1,      # Representar como Enero
        'Otoño': 4,       # Abril
        'Invierno': 7,    # Julio
        'Primavera': 10   # Octubre
    }

    df_plot['Mes_referencia'] = df_plot['Estacion'].map(estacion_meses)
    df_plot['Fecha_estacional'] = pd.to_datetime(dict(year=df_plot['Año'], month=df_plot['Mes_referencia'], day=1))

    # Ordenar cronológicamente
    df_plot = df_plot.sort_values('Fecha_estacional')

    # Etiquetas simplificadas
    df_plot['Etiqueta'] = df_plot['Fecha_estacional'].dt.year.astype(str)

    # Asignar colores por estación
    colores = {
        'Verano': '#FFD700',     # Dorado
        'Otoño': '#F4A460',      # Marrón claro
        'Invierno': '#87CEFA',   # Azul claro
        'Primavera': '#DA70D6'   # Violeta
    }

    # Crear gráfico
    fig, ax = plt.subplots(figsize=(16, 6))

    for i, row in df_plot.iterrows():
        color = colores[row['Estacion']]
        ax.bar(row['Fecha_estacional'], row[f'{estacion_nombre}_anom'], color=color, width=80)

        # Etiquetas de conteo de eventos
        ax.text(row['Fecha_estacional'], row[f'{estacion_nombre}_anom'] + 3,
                str(int(row[f'{estacion_nombre}_eventos'])),
                ha='center', va='bottom', fontsize=12, color='red')

    ax.axhline(0, color='gray', linestyle='--')
    ax.set_ylabel('Anomalía de Precipitación (%)')
    ax.set_title(f'Anomalía estacional vs Conteo de eventos extremos - {estacion_nombre}')

    # Formato del eje X (solo años)
    años = df_plot['Fecha_estacional'].dt.year.unique()
    posiciones = [df_plot[df_plot['Fecha_estacional'].dt.year == a]['Fecha_estacional'].values[0] for a in años]
    ax.set_xticks(posiciones)
    ax.set_xticklabels(años, rotation=90)

    # Leyenda
    from matplotlib.patches import Patch
    leyenda = [Patch(color=colores[e], label=e) for e in colores]
    ax.legend(handles=leyenda, title='Estación del año')

    ax.grid(axis='y', linestyle=':', alpha=0.3)
    fig.tight_layout()
    plt.show()

graficar_anomalias_vs_conteo_estacional(anomalias_estacionales, conteo_estacional, 'Mercedes_mm')

graficar_anomalias_vs_conteo_estacional(anomalias_estacionales, conteo_estacional, 'Artigas_mm')

graficar_anomalias_vs_conteo_estacional(anomalias_estacionales, conteo_estacional, 'Prado_mm')

graficar_anomalias_vs_conteo_estacional(anomalias_estacionales, conteo_estacional, 'Rivera_mm')

"""##Anomalías precipitacion anual

###ANOMALÍAS + CONTEO
"""

# Asegurar índice de fechas
data_anual.index = pd.to_datetime(data_anual.index)

# Agrupar por año calendario y sumar si no está ya en formato anual
data_anual_agg = data_anual.resample('Y').sum()

# Calcular la media anual por estación (promedio climatológico)
media_anual = data_anual_agg.mean()

# Calcular anomalías (%)
anomalias_anuales = ((data_anual_agg - media_anual) / media_anual) * 100

# Cambiar índice a solo el año (opcional)
anomalias_anuales.index = anomalias_anuales.index.year

# Mostrar resultado
print(anomalias_anuales.head())

def graficar_anomalias_con_etiqueta_eventos_anuales(df_anomalias_anuales, df_eventos_diarios, estacion, mostrar_tabla=False, guardar=False, nombre_archivo=None):
    """
    Grafica anomalías anuales de precipitación (%) y agrega etiquetas con la cantidad de eventos extremos por año.

    Parámetros:
    - df_anomalias_anuales: DataFrame con anomalías anuales (%)
    - df_eventos_diarios: DataFrame diario con eventos extremos (NaN donde no hay evento)
    - estacion: string, nombre de la estación
    - mostrar_tabla: bool, imprime comparación tabla si True
    - guardar: bool, guarda como PNG si True
    - nombre_archivo: nombre base del archivo si se guarda
    """
    import matplotlib.pyplot as plt
    import pandas as pd

    try:
        anom = df_anomalias_anuales[estacion]
        eventos_diarios = df_eventos_diarios[estacion]
    except KeyError:
        print(f"⚠️ Estación '{estacion}' no encontrada.")
        return

    # Índices a datetime
    anom.index = pd.to_datetime(anom.index, format='%Y')
    eventos_diarios.index = pd.to_datetime(eventos_diarios.index)

    # Conteo de eventos extremos por año
    conteo_eventos = eventos_diarios.dropna().resample('Y').count()
    conteo_eventos.index = conteo_eventos.index.year
    anom.index = anom.index.year

    # Alinear
    conteo_eventos = conteo_eventos.reindex(anom.index)
    anom.index = pd.to_datetime(anom.index, format='%Y')
    conteo_eventos.index = pd.to_datetime(conteo_eventos.index, format='%Y')

    if mostrar_tabla:
        df_comb = pd.concat([anom, conteo_eventos], axis=1)
        df_comb.columns = ['Anomalía (%)', 'Eventos extremos (n°)']
        print(df_comb.head(15))

    # Gráfico
    fig, ax = plt.subplots(figsize=(12, 6))

    # Barras de anomalía
    ax.bar(anom.index, anom.values, width=200, color='skyblue')
    ax.axhline(0, color='gray', linestyle='--')
    ax.set_ylabel('Anomalía de precipitación (%)', color='steelblue')
    ax.tick_params(axis='y', labelcolor='steelblue')

    # Etiquetas con el número de eventos
    for fecha, valor in conteo_eventos.items():
        if pd.notna(valor):
            altura_barra = anom.loc[fecha]
            ax.text(fecha, altura_barra + 3, f'{int(valor)}', ha='center', va='bottom', fontsize=9, color='crimson', weight='bold')

    # Estética
    plt.title(f"Anomalía anual y cantidad de eventos extremos - {estacion}")
    plt.grid(True, axis='x', linestyle=':', alpha=0.4)
    fig.tight_layout()

    # Guardado
    if guardar:
        if not nombre_archivo:
            nombre_archivo = f"anomalias_eventos_etiqueta_{estacion}"
        plt.savefig(f"{nombre_archivo}.png", dpi=300)
        print(f"✅ Gráfico guardado como {nombre_archivo}.png")

    plt.show()

graficar_anomalias_con_etiqueta_eventos_anuales(anomalias_anuales, df_eventos_extremos_NueveCinco, 'Mercedes_mm', mostrar_tabla=True)

graficar_anomalias_con_etiqueta_eventos_anuales(anomalias_anuales, df_eventos_extremos_NueveCinco, 'Artigas_mm', mostrar_tabla=True)

graficar_anomalias_con_etiqueta_eventos_anuales(anomalias_anuales, df_eventos_extremos_NueveCinco, 'Rivera_mm', mostrar_tabla=True)

graficar_anomalias_con_etiqueta_eventos_anuales(anomalias_anuales, df_eventos_extremos_NueveCinco, 'Prado_mm', mostrar_tabla=True)

def graficar_anomalias_y_eventos_diarios_anuales(df_anomalias_anuales, df_eventos_diarios, estacion, mostrar_tabla=False, guardar=False, nombre_archivo=None):
    """
    Grafica anomalías anuales de precipitación (%) con eventos extremos diarios (en mm) como puntos individuales.

    Parámetros:
    - df_anomalias_anuales: DataFrame anual con anomalías (%)
    - df_eventos_diarios: DataFrame diario con eventos extremos (NaN donde no hay evento)
    - estacion: nombre de la estación
    - mostrar_tabla: muestra tabla de eventos extremos (diarios)
    - guardar: guarda el gráfico si es True
    - nombre_archivo: nombre base si se guarda
    """
    try:
        anom = df_anomalias_anuales[estacion]
        eventos = df_eventos_diarios[estacion]
    except KeyError:
        print(f"⚠️ Estación '{estacion}' no encontrada.")
        return

    # Convertir índice a datetime
    anom.index = pd.to_datetime(anom.index, format='%Y')
    eventos.index = pd.to_datetime(eventos.index)

    # Eventos extremos válidos (diarios)
    eventos_validos = eventos.dropna()

    # Extraer años para ubicar puntos sobre las barras
    eventos_anuales = eventos_validos.copy()
    eventos_anuales.index = eventos_anuales.index.year
    eventos_anuales = eventos_anuales.groupby(eventos_anuales.index).apply(lambda x: list(x.values))

    if mostrar_tabla:
        print(eventos_validos.head(15))

    # Crear figura
    fig, ax1 = plt.subplots(figsize=(14, 6))

    # Barras de anomalía
    ax1.bar(anom.index, anom.values, width=200, color='skyblue', alpha=0.8)
    ax1.axhline(0, color='gray', linestyle='--')
    ax1.set_ylabel('Anomalía de precipitación (%)', color='steelblue')
    ax1.tick_params(axis='y', labelcolor='steelblue')

    # Eje secundario: eventos extremos diarios (como puntos)
    ax2 = ax1.twinx()
    ax2.set_ylabel('Precipitación diaria en eventos extremos (mm)', color='crimson')
    ax2.tick_params(axis='y', labelcolor='crimson')

    # Dibujar puntos por año (con leve jitter para evitar superposición)
    for año, lista_mm in eventos_anuales.items():
        fecha = pd.to_datetime(f"{año}-07-01")  # centro del año
        for i, mm in enumerate(lista_mm):
            jitter = (i - len(lista_mm)/2) * 10  # para dispersar los puntos
            ax2.scatter(fecha + pd.Timedelta(days=jitter), mm, color='crimson', s=40, alpha=0.9)

    # Título y formato
    plt.title(f"Anomalía anual y eventos extremos diarios - {estacion}")
    plt.grid(True, axis='x', linestyle=':', alpha=0.4)
    fig.tight_layout()

    # Guardado
    if guardar:
        if not nombre_archivo:
            nombre_archivo = f"anomalias_eventos_diarios_ANUAL_{estacion}"
        plt.savefig(f"{nombre_archivo}.png", dpi=300)
        print(f"✅ Gráfico guardado como {nombre_archivo}.png")

    plt.show()

graficar_anomalias_y_eventos_diarios_anuales(anomalias_anuales, df_eventos_extremos_NueveCinco, 'Prado_mm', mostrar_tabla=True)

graficar_anomalias_y_eventos_diarios_anuales(anomalias_anuales, df_eventos_extremos_NueveCinco, 'Mercedes_mm', mostrar_tabla=True)

graficar_anomalias_y_eventos_diarios_anuales(anomalias_anuales, df_eventos_extremos_NueveCinco, 'Artigas_mm', mostrar_tabla=True)

def graficar_anomalias_y_evento_max_anual(df_anomalias_anuales, df_eventos_diarios, estacion, mostrar_tabla=False, guardar=False, nombre_archivo=None):
    """
    Grafica anomalías anuales de precipitación (%) y el evento extremo diario más intenso de cada año (en mm).

    Parámetros:
    - df_anomalias_anuales: DataFrame con anomalías anuales (%)
    - df_eventos_diarios: DataFrame diario con eventos extremos (NaN donde no hay evento)
    - estacion: nombre de la estación
    - mostrar_tabla: si True, imprime tabla de valores combinados
    - guardar: si True, guarda el gráfico como PNG
    - nombre_archivo: string base para el archivo
    """
    import matplotlib.pyplot as plt
    import pandas as pd

    try:
        anom = df_anomalias_anuales[estacion]
        eventos = df_eventos_diarios[estacion]
    except KeyError:
        print(f"⚠️ Estación '{estacion}' no encontrada.")
        return

    # Asegurar índices como datetime
    anom.index = pd.to_datetime(anom.index, format='%Y')
    eventos.index = pd.to_datetime(eventos.index)

    # Extraer máximo diario de eventos por año
    eventos_maximos = eventos.dropna().resample('Y').max()
    eventos_maximos.index = eventos_maximos.index.year
    anom.index = anom.index.year

    # Alinear índices
    eventos_maximos = eventos_maximos.reindex(anom.index)
    anom.index = pd.to_datetime(anom.index, format='%Y')
    eventos_maximos.index = pd.to_datetime(eventos_maximos.index, format='%Y')

    if mostrar_tabla:
        df_comb = pd.concat([anom, eventos_maximos], axis=1)
        df_comb.columns = ['Anomalía (%)', 'Máximo evento extremo (mm)']
        print(df_comb.head(15))

    # Gráfico
    fig, ax1 = plt.subplots(figsize=(12, 6))

    # Barras de anomalía anual
    ax1.bar(anom.index, anom.values, width=200, color='skyblue', alpha=0.8)
    ax1.axhline(0, color='gray', linestyle='--')
    ax1.set_ylabel('Anomalía de precipitación (%)', color='steelblue')
    ax1.tick_params(axis='y', labelcolor='steelblue')

    # Eje secundario: punto del evento máximo
    ax2 = ax1.twinx()
    ax2.set_ylabel('Evento extremo más intenso (mm)', color='crimson')
    ax2.tick_params(axis='y', labelcolor='crimson')

    # Graficar un solo punto por año (máximo evento)
    ax2.plot(eventos_maximos.index, eventos_maximos.values, 'o', color='crimson', markersize=6, label='Evento máximo')

    # Título y estética
    plt.title(f"Anomalía anual y evento extremo más intenso - {estacion}")
    plt.grid(True, axis='x', linestyle=':', alpha=0.4)
    fig.tight_layout()

    # Guardar si se pide
    if guardar:
        if not nombre_archivo:
            nombre_archivo = f"anomalias_evento_max_anual_{estacion}"
        plt.savefig(f"{nombre_archivo}.png", dpi=300)
        print(f"✅ Gráfico guardado como {nombre_archivo}.png")

    plt.show()

graficar_anomalias_y_evento_max_anual(anomalias_anuales, df_eventos_extremos_NueveCinco, 'Prado_mm', mostrar_tabla=True)

graficar_anomalias_y_evento_max_anual(anomalias_anuales, df_eventos_extremos_NueveCinco, 'Mercedes_mm', mostrar_tabla=True)

"""# INDICES MACROCLIMÁTICOS"""

ruta_oni = "/content/drive/MyDrive/datasCo/inumet/ONI_Trimestre - Hoja 1.csv"

# Cargar el archivo
df_oni_trim = pd.read_csv(ruta_oni)

# Visualizar para verificar
print(df_oni_trim.head())
print(df_oni_trim.columns)

"""###Mensual"""

# Mapeo de trimestre a mes representativo
mapa_trimestres = {
    'DJF': '01', 'JFM': '02', 'FMA': '03', 'MAM': '04',
    'AMJ': '05', 'MJJ': '06', 'JJA': '07', 'JAS': '08',
    'ASO': '09', 'SON': '10', 'OND': '11', 'NDJ': '12'
}

# Asignar mes correspondiente al trimestre
df_oni_trim['Mes'] = df_oni_trim['Trimestre'].map(mapa_trimestres)

# Crear columna de fecha
df_oni_trim['Fecha'] = pd.to_datetime(df_oni_trim['Año'].astype(str) + '-' + df_oni_trim['Mes'] + '-01')

# Poner 'Fecha' como índice
df_oni_trim.set_index('Fecha', inplace=True)

# Filtrar solo la columna ONI si querés trabajar con ella
oni_df = df_oni_trim[['ONI']].copy()

# Verificamos
print(oni_df.head())

print(df_anomalias.index.min(), df_anomalias.index.max())
print(oni_df.index.min(), oni_df.index.max())
print(df_anomalias.index.freq, oni_df.index.freq)

# Asegurar que ambos índices estén al primer día del mes
df_anomalias.index = df_anomalias.index.to_period('M').to_timestamp()
oni_df.index = oni_df.index.to_period('M').to_timestamp()

def graficar_anomalias_vs_ONI_con_fases(df_anomalias, df_oni, estacion, escala='mensual'):
    """
    Grafica la anomalía de precipitación junto al índice ONI, sombreando los períodos Niño/Niña
    y mostrando todos los elementos en la leyenda.
    """
    anom = df_anomalias[estacion].copy()
    oni = df_oni.copy()

    if escala == 'anual':
        anom = anom.resample('Y').mean()
        oni = oni.resample('Y').mean()

    # Alinear
    datos = pd.concat([anom, oni['ONI']], axis=1).dropna()
    datos.columns = ['Anomalía (%)', 'ONI']

    # Detectar fases Niño/Niña
    oni_series = datos['ONI']
    eventos = []
    umbral = 0.5
    min_duracion = 5

    estado_actual = None
    inicio = None

    for i in range(len(oni_series)):
        valor = oni_series.iloc[i]
        fecha = oni_series.index[i]

        if valor >= umbral:
            if estado_actual != 'nino':
                estado_actual = 'nino'
                inicio = fecha
            elif i == len(oni_series) - 1:
                eventos.append(('nino', inicio, fecha))
        elif valor <= -umbral:
            if estado_actual != 'nina':
                estado_actual = 'nina'
                inicio = fecha
            elif i == len(oni_series) - 1:
                eventos.append(('nina', inicio, fecha))
        else:
            if estado_actual is not None:
                fin = fecha
                if len(oni_series[inicio:fin]) >= min_duracion:
                    eventos.append((estado_actual, inicio, fin))
                estado_actual = None

    # Gráfico
    fig, ax1 = plt.subplots(figsize=(12, 5))

    linea_anom, = ax1.plot(datos.index, datos['Anomalía (%)'], color='steelblue', label='Anomalía de precipitación (%)')
    ax1.axhline(0, color='blue', linestyle='--', linewidth=1)  # Línea horizontal en 0%
    ax1.set_ylabel('Anomalía de precipitación (%)', color='steelblue')
    ax1.tick_params(axis='y', labelcolor='steelblue')

    ax2 = ax1.twinx()
    linea_oni, = ax2.plot(datos.index, datos['ONI'], color='crimson', label='Índice ONI')
    ax2.set_ylabel('Índice ONI', color='crimson')
    ax2.tick_params(axis='y', labelcolor='crimson')

    # Sombreado Niño/Niña
    for tipo, inicio, fin in eventos:
        color = 'red' if tipo == 'nino' else 'blue'
        ax1.axvspan(inicio, fin, color=color, alpha=0.15)

    # Leyenda completa
    parches = [
        mpatches.Patch(color='red', alpha=0.15, label='El Niño (ONI ≥ 0.5)'),
        mpatches.Patch(color='blue', alpha=0.15, label='La Niña (ONI ≤ -0.5)')
    ]
    handles = [linea_anom, linea_oni] + parches
    labels = [h.get_label() for h in handles]

    ax1.legend(handles, labels, loc='upper left')

    plt.title(f"Anomalía de precipitación vs ONI - {estacion} ({escala})")
    fig.tight_layout()
    plt.grid(True, axis='x', linestyle=':', alpha=0.4)
    plt.savefig(f'anomalias_vs_ONI_{estacion}.svg', dpi=300, bbox_inches='tight')
    plt.show()

"""### Mensual anomalias vs ONI"""

graficar_anomalias_vs_ONI_con_fases(df_anomalias, oni_df, 'Mercedes_mm', escala='mensual')

graficar_anomalias_vs_ONI_con_fases(df_anomalias, oni_df, 'Artigas_mm', escala='mensual')

graficar_anomalias_vs_ONI_con_fases(df_anomalias, oni_df, 'Prado_mm', escala='mensual')

graficar_anomalias_vs_ONI_con_fases(df_anomalias, oni_df, 'PuntaEste_mm', escala='mensual')

graficar_anomalias_vs_ONI_con_fases(df_anomalias, oni_df, 'TreintaTres_mm', escala='mensual')

graficar_anomalias_vs_ONI_con_fases(df_anomalias, oni_df, 'Melo_mm', escala='mensual')

def graficar_anomalias_vs_conteo_estacional_con_ONI_mensual(anomalias_estacionales, conteo_eventos_estacional, estacion_nombre, oni_df):
    """
    Grafica anomalía estacional y conteo de eventos extremos junto con las fases Niño/Niña basadas en ONI mensual.
    """
    import matplotlib.patches as mpatches

    # === Preparar datos principales ===
    anom = anomalias_estacionales[[estacion_nombre]].copy()
    eventos = conteo_eventos_estacional[[estacion_nombre]].copy()
    df_plot = anom.join(eventos, lsuffix='_anom', rsuffix='_eventos').reset_index()

    estacion_meses = {'Verano': [12, 1, 2], 'Otoño': [3, 4, 5], 'Invierno': [6, 7, 8], 'Primavera': [9, 10, 11]}
    mes_referencia = {'Verano': 1, 'Otoño': 4, 'Invierno': 7, 'Primavera': 10}
    df_plot['Mes_referencia'] = df_plot['Estacion'].map(mes_referencia)
    df_plot['Fecha_estacional'] = pd.to_datetime(dict(year=df_plot['Año'], month=df_plot['Mes_referencia'], day=1))
    df_plot = df_plot.sort_values('Fecha_estacional')

    # === Clasificar ONI mensual como Niño, Niña o neutro ===
    oni_df = oni_df.copy()
    oni_df.index = pd.to_datetime(oni_df.index)
    oni_df['Fase'] = 'Neutro'
    oni_df.loc[oni_df['ONI'] >= 0.5, 'Fase'] = 'Niño'
    oni_df.loc[oni_df['ONI'] <= -0.5, 'Fase'] = 'Niña'

    # === Detectar periodos Niño/Niña (mínimo 5 meses consecutivos) ===
    eventos = []
    actual = None
    inicio = None

    for fecha, fase in oni_df['Fase'].items():
        if fase in ['Niño', 'Niña']:
            if actual != fase:
                actual = fase
                inicio = fecha
        else:
            if actual is not None:
                fin = fecha
                periodo = oni_df.loc[inicio:fin]
                if len(periodo[periodo['Fase'] == actual]) >= 5:
                    eventos.append((actual, inicio, fin))
                actual = None

    # === Graficar ===
    import matplotlib.pyplot as plt

    fig, ax = plt.subplots(figsize=(16, 6))

    # Sombreado de fases
    for tipo, inicio, fin in eventos:
        color = 'red' if tipo == 'Niño' else 'blue'
        ax.axvspan(inicio, fin, color=color, alpha=0.15)

    # Barras de anomalía por estación
    colores_estaciones = {
        'Verano': '#FFD700',     # Dorado
        'Otoño': '#F4A460',      # Marrón claro
        'Invierno': '#87CEFA',   # Azul claro
        'Primavera': 'green'   # Violeta
    }

    for _, row in df_plot.iterrows():
        color = colores_estaciones[row['Estacion']]
        ax.bar(row['Fecha_estacional'], row[f'{estacion_nombre}_anom'], color=color, width=80)
        ax.text(row['Fecha_estacional'], row[f'{estacion_nombre}_anom'] + 3,
                str(int(row[f'{estacion_nombre}_eventos'])),
                ha='center', va='bottom', fontsize=10, color='black')

    ax.axhline(0, color='gray', linestyle='--')
    ax.set_ylabel('Anomalía de Precipitación (%)')
    ax.set_title(f'Anomalía estacional vs Conteo de eventos extremos - {estacion_nombre} (con fases ENOS)')

    # Eje X con años
    años = df_plot['Fecha_estacional'].dt.year.unique()
    posiciones = [df_plot[df_plot['Fecha_estacional'].dt.year == a]['Fecha_estacional'].values[0] for a in años]
    ax.set_xticks(posiciones)
    ax.set_xticklabels(años, rotation=90)

    # Leyenda
    from matplotlib.patches import Patch
    leyenda_estaciones = [Patch(color=colores_estaciones[e], label=e) for e in colores_estaciones]
    leyenda_fases = [
        Patch(color='red', alpha=0.15, label='El Niño (ONI ≥ 0.5)'),
        Patch(color='blue', alpha=0.15, label='La Niña (ONI ≤ -0.5)')
    ]
    ax.legend(handles=leyenda_estaciones + leyenda_fases, title='Estación y fases ENOS')
    ax.grid(axis='y', linestyle=':', alpha=0.3)
    fig.tight_layout()
    plt.savefig(f'anomalias_estacional_eventos_{estacion_nombre}.png', dpi=300, bbox_inches='tight')
    print(f"✅ Gráfico guardado como anomalias_estacional_eventos_{estacion_nombre}.png")
    plt.show()

graficar_anomalias_vs_conteo_estacional_con_ONI_mensual(
    anomalias_estacionales, conteo_estacional, 'Mercedes_mm', oni_df
)

graficar_anomalias_vs_conteo_estacional_con_ONI_mensual(
    anomalias_estacionales, conteo_estacional, 'Artigas_mm', oni_df
)

graficar_anomalias_vs_conteo_estacional_con_ONI_mensual(
    anomalias_estacionales, conteo_estacional, 'Prado_mm', oni_df
)

graficar_anomalias_vs_conteo_estacional_con_ONI_mensual(
    anomalias_estacionales, conteo_estacional, 'Carrasco_mm', oni_df
)

graficar_anomalias_vs_conteo_estacional_con_ONI_mensual(
    anomalias_estacionales, conteo_estacional, 'TreintaTres_mm', oni_df
)

"""### Anual anomalia vs ONI"""

#Arreglar el índice de anomalias_anual: convertir años (int) a datetime al 31 de diciembre de cada año
if anomalias_anuales.index.dtype != 'datetime64[ns]':
    anomalias_anuales.index = pd.to_datetime(anomalias_anuales.index.astype(str) + "-12-31")

# ✅ Verificación rápida (opcional)
print(anomalias_anuales.index[:5])
print(oni_df.index[:5])

graficar_anomalias_vs_ONI_con_fases(anomalias_anuales, oni_df, 'PuntaEste_mm', escala='anual')

graficar_anomalias_vs_ONI_con_fases(anomalias_anuales, oni_df, 'Mercedes_mm', escala='anual')

"""### Conteo de eventos vs ONI"""

def graficar_ONI_vs_eventos(oni_df, df_eventos_extremos, estacion, variable='conteo'):
    """
    Gráfico ONI vs eventos extremos con cruces + etiquetas, mejor visualización.

    Parámetros:
    - oni_df: DataFrame con columna 'ONI', índice datetime mensual
    - df_eventos_extremos: DataFrame diario con eventos extremos (NaN donde no hay)
    - estacion: columna de estación (str)
    - variable: 'conteo' o 'mm'
    """
    import matplotlib.pyplot as plt

    eventos = df_eventos_extremos[[estacion]].copy()
    eventos.index = pd.to_datetime(eventos.index)

    if variable == 'conteo':
        eventos_mensuales = eventos.dropna().resample('M').count()
        ylabel = 'Cantidad de eventos extremos'
    elif variable == 'mm':
        eventos_mensuales = eventos.dropna().resample('M').sum()
        ylabel = 'Precipitación total en eventos extremos (mm)'
    else:
        raise ValueError("El parámetro 'variable' debe ser 'conteo' o 'mm'")

    oni_df.index = pd.to_datetime(oni_df.index)
    oni_mensual = oni_df.resample('M').mean()

    datos = pd.concat([eventos_mensuales, oni_mensual['ONI']], axis=1).dropna()
    datos.columns = ['Eventos', 'ONI']

    if datos.empty:
        print("⚠️ No hay datos combinados entre eventos y ONI para graficar.")
        return

    # Gráfico
    fig, ax1 = plt.subplots(figsize=(14, 6))

    # Cruces para eventos
    ax1.scatter(datos.index, datos['Eventos'], color='darkorange', marker='x', s=70, label=ylabel, zorder=3)

    # Etiquetas numéricas arriba de cada cruz
    for fecha, valor in datos['Eventos'].items():
        if valor > 0:
            ax1.text(fecha, valor + 0.15, str(int(valor)), color='darkred',
                     fontsize=8, ha='center', va='bottom', zorder=4)

    ax1.set_ylabel(ylabel, color='darkorange')
    ax1.tick_params(axis='y', labelcolor='darkorange')

    # Línea ONI
    ax2 = ax1.twinx()
    ax2.plot(datos.index, datos['ONI'], color='navy', linewidth=2, label='ONI', zorder=2)
    ax2.set_ylabel('Índice ONI', color='navy')
    ax2.tick_params(axis='y', labelcolor='navy')

    plt.title(f"ONI vs {ylabel.lower()} - {estacion}")
    fig.tight_layout()
    plt.grid(True, axis='x', linestyle=':', alpha=0.4)

    # Leyendas combinadas
    h1, l1 = ax1.get_legend_handles_labels()
    h2, l2 = ax2.get_legend_handles_labels()
    ax1.legend(h1 + h2, l1 + l2, loc='upper left')

    plt.show()

graficar_ONI_vs_eventos(oni_df, df_eventos_extremos_NueveCinco, 'Mercedes_mm', variable='conteo')

graficar_ONI_vs_eventos(oni_df, df_eventos_extremos_NueveCinco, 'Mercedes_mm', variable='mm')

"""###PRUEBA"""

df_oni_trim['Etiqueta'] = df_oni_trim['Trimestre'] + ' ' + df_oni_trim['Año'].astype(str)
df_oni_trim.set_index('Etiqueta', inplace=True)
df_oni_trim = df_oni_trim[['ONI']]  # quedate solo con ONI si querés

df_oni_trim

print(df_oni_trim.index[:10])

for i in df_oni_trim.index[:5]:
    print(repr(i))

# Ver el índice como string
index_str = df_oni_trim.index.astype(str).str.strip()

# Ver los primeros 5 para inspeccionar
print("Ejemplos de índice:")
for i in index_str[:5]:
    print(repr(i))

# Aplicar la extracción paso a paso
import re

trimestres = []
anios = []

for etiqueta in index_str:
    match = re.match(r'^([A-Z]{3})\s+(\d{4})$', etiqueta)
    if match:
        trimestres.append(match.group(1))
        anios.append(int(match.group(2)))
    else:
        print("No matchea:", repr(etiqueta))

# Crear nuevo DataFrame si todo salió bien
if len(trimestres) == len(df_oni_trim):
    df_oni_trim['Trimestre'] = trimestres
    df_oni_trim['Año'] = anios
    print("✔️ Trimestres y años extraídos correctamente.")
    print(df_oni_trim.head())
else:
    print("❌ Algo salió mal, cantidades no coinciden.")

def clasificar_fases_ENOS_por_trimestre(df_oni_trim):
    """
    Clasifica cada trimestre móvil como 'Niño', 'Niña' o 'Neutral',
    siguiendo el criterio NOAA: 5 trimestres consecutivos con ONI ≥ 0.5 (Niño)
    o ≤ -0.5 (Niña).

    Parámetro:
    - df_oni_trim: DataFrame con columnas 'ONI', 'Trimestre', 'Año', y el índice en formato 'DJF 1991'

    Devuelve:
    - diccionario { 'DJF 1991': 'Niño' / 'Niña' / 'Neutral', ... }
    """

    umbral = 0.5
    min_consecutivos = 5

    fases = []
    etiquetas = df_oni_trim.index.astype(str)
    valores = df_oni_trim['ONI'].values

    secuencia = []
    for v in valores:
        if v >= umbral:
            secuencia.append('nino')
        elif v <= -umbral:
            secuencia.append('nina')
        else:
            secuencia.append('neutral')

    # Buscar eventos
    eventos = []
    actual = None
    conteo = 0
    for i, fase in enumerate(secuencia):
        if fase == actual:
            conteo += 1
        else:
            if conteo >= min_consecutivos and actual != 'neutral':
                eventos.append((actual, i - conteo, i - 1))
            actual = fase
            conteo = 1
    if conteo >= min_consecutivos and actual != 'neutral':
        eventos.append((actual, len(secuencia) - conteo, len(secuencia) - 1))

    # Construir diccionario de fase por trimestre
    fase_por_trimestre = {etiquetas[i]: 'Neutral' for i in range(len(etiquetas))}
    for tipo, i_ini, i_fin in eventos:
        for i in range(i_ini, i_fin + 1):
            fase_por_trimestre[etiquetas[i]] = 'Niño' if tipo == 'nino' else 'Niña'

    return fase_por_trimestre

fase_por_trimestre = clasificar_fases_ENOS_por_trimestre(df_oni_trim)
fase_por_trimestre

# 1. Crear DataFrame a partir del diccionario de fases por trimestre móvil
df_fase_trim = pd.DataFrame.from_dict(fase_por_trimestre, orient='index', columns=['Fase_ENOS'])
df_fase_trim.index.name = 'Trimestre'

# 2. Mapeo de meses a trimestres móviles tipo NOAA
mes_a_trimestre = {
    1: 'DJF',  2: 'JFM',  3: 'FMA',
    4: 'MAM',  5: 'AMJ',  6: 'MJJ',
    7: 'JJA',  8: 'JAS',  9: 'ASO',
    10: 'SON', 11: 'OND', 12: 'NDJ'
}

# 3. Crear columna con trimestre móvil correspondiente a cada mes
df_pp = data_imputado[['Mercedes_mm']].copy()  # Cambiá por la estación que quieras
df_pp['Mes'] = df_pp.index.month
df_pp['Año'] = df_pp.index.year
df_pp['Trimestre'] = df_pp['Mes'].map(mes_a_trimestre) + ' ' + df_pp['Año'].astype(str)

# 4. Mapear fase ENSO usando los trimestres móviles
df_pp['Fase_ENOS'] = df_pp['Trimestre'].map(df_fase_trim['Fase_ENOS'])

def graficar_precip_por_fase_ENOS(nombre_estacion, fase_por_trimestre, df_pp_base, guardar_pdf=False):
    """
    Grafica la precipitación media mensual por fase ENSO según trimestres móviles (NOAA).

    Parámetros:
    - nombre_estacion: str, nombre de la columna en df_pp_base (ej: 'Mercedes_mm')
    - fase_por_trimestre: dict, con claves tipo 'DJF 1991' y valores 'Niño', 'Niña' o 'Neutral'
    - df_pp_base: DataFrame con índice datetime y columnas de estaciones
    - guardar_pdf: bool, si True guarda el gráfico como PDF
    """

    # Crear DataFrame con las fases
    df_fase_trim = pd.DataFrame.from_dict(fase_por_trimestre, orient='index', columns=['Fase_ENOS'])
    df_fase_trim.index.name = 'Trimestre'

    # Mapeo de mes a trimestre móvil tipo NOAA
    mes_a_trimestre = {
        1: 'DJF',  2: 'JFM',  3: 'FMA',
        4: 'MAM',  5: 'AMJ',  6: 'MJJ',
        7: 'JJA',  8: 'JAS',  9: 'ASO',
        10: 'SON', 11: 'OND', 12: 'NDJ'
    }

    # Copia del df base
    df_pp = df_pp_base[[nombre_estacion]].copy()
    df_pp['Mes'] = df_pp.index.month
    df_pp['Año'] = df_pp.index.year
    df_pp['Trimestre'] = df_pp['Mes'].map(mes_a_trimestre) + ' ' + df_pp['Año'].astype(str)

    # Asignar fase ENSO
    df_pp['Fase_ENOS'] = df_pp['Trimestre'].map(df_fase_trim['Fase_ENOS'])

    # Etiquetas
    df_pp['Mes_nombre'] = df_pp.index.month_name()
    df_pp['Mes_num'] = df_pp.index.month

    # Agrupar por mes y fase
    media_mensual = df_pp.groupby(['Fase_ENOS', 'Mes_num'])[nombre_estacion].mean().unstack(0)

    # Promedio global
    promedio_global = df_pp.groupby('Mes_num')[nombre_estacion].mean()

    # Orden y nombres de meses
    meses_ordenados = list(range(1, 13))
    nombres_meses = ['Ene', 'Feb', 'Mar', 'Abr', 'May', 'Jun',
                 'Jul', 'Ago', 'Sep', 'Oct', 'Nov', 'Dic']
    media_mensual = media_mensual.loc[meses_ordenados]

    # Gráfico
    plt.figure(figsize=(10, 5))
    if 'Niño' in media_mensual:
        plt.plot(nombres_meses, media_mensual['Niño'], label='Niño', color='red', marker='s')
    if 'Niña' in media_mensual:
        plt.plot(nombres_meses, media_mensual['Niña'], label='Niña', color='skyblue', marker='x')
    if 'Neutral' in media_mensual:
        plt.plot(nombres_meses, media_mensual['Neutral'], label='Neutral', color='orange', marker='o', linestyle=':')

    plt.plot(nombres_meses, promedio_global, label='Promedio global', color='gray', linewidth=2)

    plt.title(f'Precipitación media mensual por fase ENOS - {nombre_estacion}')
    plt.ylabel('Precipitación media (mm)')
    plt.xlabel('Meses')
    plt.xticks(rotation=45)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()

    if guardar_pdf:
        nombre_archivo = f"precip_por_fase_ENOS_{nombre_estacion}.svg"
        plt.savefig(nombre_archivo)
        print(f"✅ Gráfico guardado como {nombre_archivo}")

    plt.show()

graficar_precip_por_fase_ENOS('Prado_mm', fase_por_trimestre, data_meses, guardar_pdf=True)

graficar_precip_por_fase_ENOS('Carrasco_mm', fase_por_trimestre, data_meses, guardar_pdf=True)

graficar_precip_por_fase_ENOS('Colonia_mm', fase_por_trimestre, data_meses, guardar_pdf=True)

graficar_precip_por_fase_ENOS('Mercedes_mm', fase_por_trimestre, data_meses, guardar_pdf=True)

graficar_precip_por_fase_ENOS('Paysandu_mm', fase_por_trimestre, data_meses, guardar_pdf=True)

graficar_precip_por_fase_ENOS('TreintaTres_mm', fase_por_trimestre, data_meses, guardar_pdf=True)

graficar_precip_por_fase_ENOS('Durazno_mm', fase_por_trimestre, data_meses, guardar_pdf=True)

graficar_precip_por_fase_ENOS('Young_mm', fase_por_trimestre, data_meses, guardar_pdf=True)

graficar_precip_por_fase_ENOS('Artigas_mm', fase_por_trimestre, data_meses, guardar_pdf=True)

graficar_precip_por_fase_ENOS('Rivera_mm', fase_por_trimestre, data_meses, guardar_pdf=True)

"""### SOI"""

# Ruta al archivo en tu Google Drive
ruta_soi = '/content/drive/MyDrive/datasCo/inumet/soi.long.csv'  # Ajustala si es otra

# Cargar con separador ;
soi_df = pd.read_csv(ruta_soi, sep=';')

# Convertir a datetime y setear como índice
soi_df['Fecha'] = pd.to_datetime(soi_df['Fecha'], dayfirst=True)
soi_df.set_index('Fecha', inplace=True)

# Verificamos
print(soi_df.head())

def graficar_anomalias_vs_SOI_con_fases(df_anomalias, df_soi, estacion, escala='mensual'):
    """
    Grafica la anomalía de precipitación junto al índice SOI, sombreando los períodos Niño/Niña
    usando z-score (SOI estandarizado) y criterio ≥5 meses consecutivos.
    """
    # Copias de trabajo
    anom = df_anomalias[estacion].copy()
    soi = df_soi.copy()

    # Escalamiento anual si se indica
    if escala == 'anual':
        anom = anom.resample('Y').mean()
        soi = soi.resample('Y').mean()

    # Combinar y alinear
    datos = pd.concat([anom, soi['SOI']], axis=1).dropna()
    datos.columns = ['Anomalía (%)', 'SOI']

    # --- Detectar fases ENSO con criterio de persistencia ---
    umbral = 1  # SOI ≥ +1 (La Niña), ≤ -1 (El Niño)
    min_meses = 5

    eventos = []
    estado_actual = None
    inicio = None
    duracion = 0

    for i in range(len(datos)):
        valor = datos['SOI'].iloc[i]
        fecha = datos.index[i]

        if valor >= umbral:
            if estado_actual == 'nina':
                duracion += 1
            else:
                estado_actual = 'nina'
                inicio = fecha
                duracion = 1
        elif valor <= -umbral:
            if estado_actual == 'nino':
                duracion += 1
            else:
                estado_actual = 'nino'
                inicio = fecha
                duracion = 1
        else:
            if estado_actual in ['nina', 'nino'] and duracion >= min_meses:
                eventos.append((estado_actual, inicio, fecha))
            estado_actual = None
            duracion = 0

    # Cierre del último evento si persiste hasta el final
    if estado_actual in ['nina', 'nino'] and duracion >= min_meses:
        eventos.append((estado_actual, inicio, datos.index[-1]))

    # --- Gráfico ---
    fig, ax1 = plt.subplots(figsize=(12, 5))

    # Anomalía
    linea_anom, = ax1.plot(datos.index, datos['Anomalía (%)'], color='steelblue', label='Anomalía de precipitación (%)')
    ax1.axhline(0, color='gray', linestyle='--', linewidth=1)
    ax1.set_ylabel('Anomalía de precipitación (%)', color='steelblue')
    ax1.tick_params(axis='y', labelcolor='steelblue')

    # SOI
    ax2 = ax1.twinx()
    linea_soi, = ax2.plot(datos.index, datos['SOI'], color='green', label='Índice SOI (estandarizado)')
    ax2.set_ylabel('Índice SOI (z-score)', color='green')
    ax2.tick_params(axis='y', labelcolor='green')

    # Sombreado de fases
    for tipo, inicio, fin in eventos:
        color = 'red' if tipo == 'nino' else 'blue'
        ax1.axvspan(inicio, fin, color=color, alpha=0.15)

    # Leyenda
    parches = [
        mpatches.Patch(color='red', alpha=0.15, label='El Niño (SOI ≤ –1, ≥5 meses)'),
        mpatches.Patch(color='blue', alpha=0.15, label='La Niña (SOI ≥ +1, ≥5 meses)')
    ]
    handles = [linea_anom, linea_soi] + parches
    ax1.legend(handles, [h.get_label() for h in handles], loc='upper left')

    plt.title(f"Anomalía de precipitación vs SOI - {estacion} ({escala})")
    fig.tight_layout()
    plt.grid(True, axis='x', linestyle=':', alpha=0.4)
    plt.savefig(f'anomalias_vs_SOI_{estacion}.pdf', dpi=300, bbox_inches='tight')
    plt.show()

graficar_anomalias_vs_SOI_con_fases(df_anomalias, soi_df, 'Mercedes_mm', escala='mensual')